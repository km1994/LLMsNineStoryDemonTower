# ã€LLMs å…¥é—¨å®æˆ˜ â€”â€” ä¸ƒ ã€‘Vicuna æ¨¡å‹å­¦ä¹ ä¸å®æˆ˜

- github åœ°å€: https://github.com/tatsu-lab/stanford_alpaca
- è¯•ç”¨åœ°å€ï¼šhttps://alpaca-ai-custom6.ngrok.io/

## ã€LLMs å…¥é—¨å®æˆ˜ç³»åˆ—ã€‘

### ç¬¬ä¸€å±‚ ChatGLM-6B

1. [ã€ChatGLM-6Bå…¥é—¨-ä¸€ã€‘æ¸…åå¤§å­¦å¼€æºä¸­æ–‡ç‰ˆChatGLM-6Bæ¨¡å‹å­¦ä¹ ä¸å®æˆ˜](ChatGLM-6B/induction.md)
   1. ä»‹ç»ï¼šChatGLM-6B ç¯å¢ƒé…ç½® å’Œ éƒ¨ç½²
2. [ã€ChatGLM-6Bå…¥é—¨-äºŒã€‘æ¸…åå¤§å­¦å¼€æºä¸­æ–‡ç‰ˆChatGLM-6Bæ¨¡å‹å¾®è°ƒå®æˆ˜](ChatGLM-6B/ptuning.md)
   1. ChatGLM-6B P-Tuning V2 å¾®è°ƒï¼šFine-tuning the prefix encoder of the model.
3. [ã€ChatGLM-6Bå…¥é—¨-ä¸‰ã€‘ChatGLM ç‰¹å®šä»»åŠ¡å¾®è°ƒå®æˆ˜](https://articles.zsxq.com/id_3b42ukjdkwpt.html)
4. [ã€ChatGLM-6Bå…¥é—¨-å››ã€‘ChatGLM + LoRA è¿›è¡Œfinetune](https://articles.zsxq.com/id_e2389qm0w0sx.html)
   1. ä»‹ç»ï¼šChatGLM-6B LoRA å¾®è°ƒï¼šFine-tuning the low-rank adapters of the model.
5. [ChatGLM-6B å°ç¼–å¡«å‘è®°](https://articles.zsxq.com/id_fw7vn0mhdsnq.html)
   1. ä»‹ç»ï¼šChatGLM-6B åœ¨ éƒ¨ç½²å’Œå¾®è°ƒ è¿‡ç¨‹ä¸­ ä¼šé‡åˆ°å¾ˆå¤šå‘ï¼Œå°ç¼–æ‰å‘äº†å¾ˆå¤šæ¬¡ï¼Œä¸ºé˜²æ­¢ åäººå’Œå°ç¼–ä¸€æ ·ç»§ç»­æ‰å‘ï¼Œå°ç¼–ç´¢æ€§æŠŠé‡åˆ°çš„å‘éƒ½å¡«äº†ã€‚
6. [ã€LLMså­¦ä¹ ã€‘å…³äºå¤§æ¨¡å‹å®è·µçš„ä¸€äº›æ€»ç»“](https://articles.zsxq.com/id_il58nxrs9jxr.html)
7. [ã€LLMs å…¥é—¨å®æˆ˜ â€”â€” åä¸€ ã€‘åŸºäº ğŸ¤—PEFT çš„é«˜æ•ˆ ğŸ¤–ChatGLM-6B å¾®è°ƒ](https://articles.zsxq.com/id_7rz5jtfguuc5.html)
   1. å¾®è°ƒæ–¹å¼ï¼š
      1. ChatGLM-6B Freeze å¾®è°ƒï¼šFine-tuning the MLPs in the last n blocks of the model.
      2. ChatGLM-6B P-Tuning V2 å¾®è°ƒï¼šFine-tuning the prefix encoder of the model.
      3. ChatGLM-6B LoRA å¾®è°ƒï¼šFine-tuning the low-rank adapters of the model.
8. [ã€LLMs å…¥é—¨å®æˆ˜ â€”â€” åäºŒ ã€‘åŸºäº æœ¬åœ°çŸ¥è¯†åº“ çš„é«˜æ•ˆ ğŸ¤–langchain-ChatGLM ](https://articles.zsxq.com/id_54vjwns5t6in.html)
   1. ä»‹ç»ï¼šlangchain-ChatGLMæ˜¯ä¸€ä¸ªåŸºäºæœ¬åœ°çŸ¥è¯†çš„é—®ç­”æœºå™¨äººï¼Œä½¿ç”¨è€…å¯ä»¥è‡ªç”±é…ç½®æœ¬åœ°çŸ¥è¯†ï¼Œç”¨æˆ·é—®é¢˜çš„ç­”æ¡ˆä¹Ÿæ˜¯åŸºäºæœ¬åœ°çŸ¥è¯†ç”Ÿæˆçš„ã€‚

### ç¬¬äºŒå±‚ Stanford Alpaca 7B 

- [ã€LLMs å…¥é—¨å®æˆ˜ â€”â€” äº” ã€‘Stanford Alpaca 7B æ¨¡å‹å­¦ä¹ ä¸å®æˆ˜](https://articles.zsxq.com/id_xnt3fvp2wxz0.html)
  - ä»‹ç»ï¼šæœ¬æ•™ç¨‹æä¾›äº†å¯¹LLaMAæ¨¡å‹è¿›è¡Œå¾®è°ƒçš„å»‰ä»·äº²æ°‘ LLMs å­¦ä¹ å’Œå¾®è°ƒ æ–¹å¼ï¼Œä¸»è¦ä»‹ç»å¯¹äº Stanford Alpaca 7B æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸Š çš„ å¾®è°ƒå®éªŒï¼Œæ‰€ç”¨çš„æ•°æ®ä¸ºOpenAIæä¾›çš„GPTæ¨¡å‹APIç”Ÿæˆè´¨é‡è¾ƒé«˜çš„æŒ‡ä»¤æ•°æ®ï¼ˆä»…52kï¼‰ã€‚

### ç¬¬ä¸‰å±‚ Chinese-LLaMA-Alpaca 

- [ã€LLMs å…¥é—¨å®æˆ˜ â€”â€” å…­ ã€‘Chinese-LLaMA-Alpaca æ¨¡å‹å­¦ä¹ ä¸å®æˆ˜](https://articles.zsxq.com/id_dqvusswrdg6c.html)
  - ä»‹ç»ï¼šæœ¬æ•™ç¨‹ä¸»è¦ä»‹ç»äº† Chinese-ChatLLaMA,æä¾›ä¸­æ–‡å¯¹è¯æ¨¡å‹ ChatLLama ã€ä¸­æ–‡åŸºç¡€æ¨¡å‹ LLaMA-zh åŠå…¶è®­ç»ƒæ•°æ®ã€‚ æ¨¡å‹åŸºäº TencentPretrain å¤šæ¨¡æ€é¢„è®­ç»ƒæ¡†æ¶æ„å»º

### ç¬¬å››å±‚ å°ç¾Šé©¼ Vicuna

- [ã€LLMs å…¥é—¨å®æˆ˜ â€”â€” ä¸ƒ ã€‘å°ç¾Šé©¼ Vicunaæ¨¡å‹å­¦ä¹ ä¸å®æˆ˜](https://articles.zsxq.com/id_q9mx24q9fdab.html)
  - ä»‹ç»ï¼šUCä¼¯å…‹åˆ©å­¦è€…è”æ‰‹CMUã€æ–¯å¦ç¦ç­‰ï¼Œå†æ¬¡æ¨å‡ºä¸€ä¸ªå…¨æ–°æ¨¡å‹70äº¿/130äº¿å‚æ•°çš„Vicunaï¼Œä¿—ç§°ã€Œå°ç¾Šé©¼ã€ï¼ˆéª†é©¬ï¼‰ã€‚å°ç¾Šé©¼å·ç§°èƒ½è¾¾åˆ°GPT-4çš„90%æ€§èƒ½

### ç¬¬äº”å±‚ MiniGPT-4 

- [ã€LLMs å…¥é—¨å®æˆ˜ â€”â€” å…« ã€‘MiniGPT-4 æ¨¡å‹å­¦ä¹ ä¸å®æˆ˜](https://articles.zsxq.com/id_ff0w6czthq25.html)
  - ä»‹ç»ï¼š MiniGPT-4ï¼Œæ˜¯æ¥è‡ªé˜¿åœæœæ‹‰å›½ç‹ç§‘æŠ€å¤§å­¦çš„å‡ ä½åšå£«åšçš„ï¼Œå®ƒèƒ½æä¾›ç±»ä¼¼ GPT-4 çš„å›¾åƒç†è§£ä¸å¯¹è¯èƒ½åŠ›

### ç¬¬å…­å±‚ GPT4ALL

- [ã€LLMs å…¥é—¨å®æˆ˜ â€”â€” å…« ã€‘GPT4ALL æ¨¡å‹å­¦ä¹ ä¸å®æˆ˜](https://articles.zsxq.com/id_ff0w6czthq25.html)
  - ä»‹ç»ï¼šä¸€ä¸ª å¯ä»¥åœ¨è‡ªå·±ç¬”è®°æœ¬ä¸Šé¢è·‘èµ·æ¥çš„  Nomic AI çš„åŠ©æ‰‹å¼èŠå¤©æœºå™¨äººï¼Œæˆä¸ºè´«æ°‘å®¶å­©å­çš„ ç¦éŸ³ï¼

### ç¬¬ä¸ƒå±‚ AutoGPT

- [AutoGPT ä½¿ç”¨å’Œéƒ¨ç½²](https://articles.zsxq.com/id_pli0z9916126.html)
  - ä»‹ç»ï¼šAuto-GPTæ˜¯ä¸€ä¸ªåŸºäºChatGPTçš„å·¥å…·ï¼Œä»–èƒ½å¸®ä½ è‡ªåŠ¨å®Œæˆå„ç§ä»»åŠ¡ï¼Œæ¯”å¦‚å†™ä»£ç ã€å†™æŠ¥å‘Šã€åšè°ƒç ”ç­‰ç­‰ã€‚ä½¿ç”¨å®ƒæ—¶ï¼Œä½ åªéœ€è¦å‘Šè¯‰ä»–è¦æ‰®æ¼”çš„è§’è‰²å’Œè¦å®ç°çš„ç›®æ ‡ï¼Œç„¶åä»–å°±ä¼šåˆ©ç”¨ChatGPTå’Œè°·æ­Œæœç´¢ç­‰å·¥å…·ï¼Œä¸æ–­â€œæ€è€ƒâ€å¦‚ä½•æ¥è¿‘ç›®æ ‡å¹¶æ‰§è¡Œï¼Œä½ ç”šè‡³å¯ä»¥çœ‹åˆ°ä»–çš„æ€è€ƒè¿‡ç¨‹ã€‚

### ç¬¬å…«å±‚ MOSS

- [ã€LLMs å…¥é—¨å®æˆ˜ â€”â€” åä¸‰ ã€‘MOSS æ¨¡å‹å­¦ä¹ ä¸å®æˆ˜](https://articles.zsxq.com/id_4vwpxod23zrc.html)
  - ä»‹ç»ï¼šMOSSæ˜¯ä¸€ä¸ªæ”¯æŒä¸­è‹±åŒè¯­å’Œå¤šç§æ’ä»¶çš„å¼€æºå¯¹è¯è¯­è¨€æ¨¡å‹ï¼Œmoss-moonç³»åˆ—æ¨¡å‹å…·æœ‰160äº¿å‚æ•°ï¼Œåœ¨FP16ç²¾åº¦ä¸‹å¯åœ¨å•å¼ A100/A800æˆ–ä¸¤å¼ 3090æ˜¾å¡è¿è¡Œï¼Œåœ¨INT4/8ç²¾åº¦ä¸‹å¯åœ¨å•å¼ 3090æ˜¾å¡è¿è¡Œã€‚MOSSåŸºåº§è¯­è¨€æ¨¡å‹åœ¨çº¦ä¸ƒåƒäº¿ä¸­è‹±æ–‡ä»¥åŠä»£ç å•è¯ä¸Šé¢„è®­ç»ƒå¾—åˆ°ï¼Œåç»­ç»è¿‡å¯¹è¯æŒ‡ä»¤å¾®è°ƒã€æ’ä»¶å¢å¼ºå­¦ä¹ å’Œäººç±»åå¥½è®­ç»ƒå…·å¤‡å¤šè½®å¯¹è¯èƒ½åŠ›åŠä½¿ç”¨å¤šç§æ’ä»¶çš„èƒ½åŠ›ã€‚
  - å±€é™æ€§ï¼šç”±äºæ¨¡å‹å‚æ•°é‡è¾ƒå°å’Œè‡ªå›å½’ç”ŸæˆèŒƒå¼ï¼ŒMOSSä»ç„¶å¯èƒ½ç”ŸæˆåŒ…å«äº‹å®æ€§é”™è¯¯çš„è¯¯å¯¼æ€§å›å¤æˆ–åŒ…å«åè§/æ­§è§†çš„æœ‰å®³å†…å®¹ï¼Œè¯·è°¨æ…é‰´åˆ«å’Œä½¿ç”¨MOSSç”Ÿæˆçš„å†…å®¹ï¼Œè¯·å‹¿å°†MOSSç”Ÿæˆçš„æœ‰å®³å†…å®¹ä¼ æ’­è‡³äº’è”ç½‘ã€‚è‹¥äº§ç”Ÿä¸è‰¯åæœï¼Œç”±ä¼ æ’­è€…è‡ªè´Ÿã€‚


## ä¸€ã€å‰è¨€

UCä¼¯å…‹åˆ©å­¦è€…è”æ‰‹CMUã€æ–¯å¦ç¦ç­‰ï¼Œå†æ¬¡æ¨å‡ºä¸€ä¸ªå…¨æ–°æ¨¡å‹70äº¿/130äº¿å‚æ•°çš„Vicunaï¼Œä¿—ç§°ã€Œå°ç¾Šé©¼ã€ï¼ˆéª†é©¬ï¼‰ã€‚å°ç¾Šé©¼å·ç§°èƒ½è¾¾åˆ°GPT-4çš„90%æ€§èƒ½

## äºŒã€ç¯å¢ƒæ­å»º

### 2.1 æ„å»ºç¯å¢ƒ

```s
    $ conda create -n py310_chat python=3.10       # åˆ›å»ºæ–°ç¯å¢ƒ
    $ source activate py310_chat                   # æ¿€æ´»ç¯å¢ƒ
```

### 2.2 å®‰è£… FastChat

#### 2.2.1 åˆ©ç”¨ pip å®‰è£…

```s
    $ pip install fschat
```

#### 2.2.2 ä» github ä¸‹è½½ repository  å®‰è£…

1. clone repositoryï¼Œç„¶å åŠ å…¥ FastChat folder 

```s
    $ git clone https://github.com/lm-sys/FastChat.git
    $ cd FastChat
```

2. å®‰è£… åŒ…

```s
    $ pip install --upgrade pip  # enable PEP 660 support
    $ pip install -e .
```

## ä¸‰ã€Vicuna Weights ç”Ÿæˆ

### 3.1 å®˜æ–¹æä¾› çš„ Vicuna Weights ç”Ÿæˆæ–¹å¼

æˆ‘ä»¬å°† Vicuna Weights ä½œä¸º delta weights å‘å¸ƒï¼Œä»¥ç¬¦åˆLLaMAæ¨¡å‹è®¸å¯è¯ã€‚æ‚¨å¯ä»¥å°†æˆ‘ä»¬çš„deltaæ·»åŠ åˆ° åŸå§‹ LLaMA Weights ä¸­ï¼Œä»¥è·å¾— Vicuna Weights ã€‚è¯´æ˜ï¼š

1. æŒ‰ç…§ [æ­¤å¤„](https://huggingface.co/docs/transformers/main/model_doc/llama) çš„è¯´æ˜ï¼Œä»¥ huggingface format è·å–åŸå§‹ LLaMA Weights;
2. ä½¿ç”¨ä»¥ä¸‹è„šæœ¬é€šè¿‡åº”ç”¨æˆ‘ä»¬çš„deltaæ¥è·å¾— Vicuna weights ã€‚ä»–ä»¬ä¼šè‡ªåŠ¨ä»æˆ‘ä»¬çš„  Hugging Face account ä¸‹è½½ Vicuna Weights ã€‚

> æ³¨ï¼šæƒé‡v1.1 ä»…ä¸ transformers>=4.28.0 å’Œ fschat>=0.2.0 å…¼å®¹ã€‚è¯·ç›¸åº”åœ°æ›´æ–°æ‚¨çš„ æœ¬åœ°package ã€‚å¦‚æœæ‚¨æŒ‰ç…§ä¸Šé¢çš„å‘½ä»¤è¿›è¡Œæ–°çš„å®‰è£…ï¼Œé‚£ä¹ˆæ‚¨åº”è¯¥å¾—åˆ°æ‰€æœ‰æ­£ç¡®çš„ç‰ˆæœ¬ã€‚

### 3.2 æœ¬é¡¹ç›®æ‰€ä½¿ç”¨çš„çš„ Vicuna Weights ç”Ÿæˆæ–¹å¼

> å‚è€ƒï¼š[How to Prepare Vicuna Weight](https://github.com/Vision-CAIR/MiniGPT-4/blob/main/PrepareVicuna.md)

#### 3.2.1 ä¸‹è½½ Vicuna Weight

å½“å‰ç‰ˆæœ¬çš„MiniGPT-4æ˜¯å»ºç«‹åœ¨v0ç‰ˆæœ¬çš„ Vicuna-13B ä¹‹ä¸Šçš„ã€‚è¯·å‚è€ƒæˆ‘ä»¬çš„è¯´æ˜æ¥å‡†å¤‡ Vicuna weightsã€‚æœ€ç»ˆçš„æƒé‡å°†åœ¨ç»“æ„ç±»ä¼¼äºä»¥ä¸‹çš„å•ä¸ªæ–‡ä»¶å¤¹ä¸­:

> æ³¨ï¼šVicunaæ˜¯ä¸€ä¸ªå¼€æºçš„åŸºäºllamaçš„LLMï¼Œå…¶æ€§èƒ½æ¥è¿‘ChatGPTã€‚æˆ‘ä»¬ç›®å‰ä½¿ç”¨çš„æ˜¯v0ç‰ˆæœ¬çš„Vicuna-13Bã€‚

```s
    $ git lfs install
    $ git clone https://huggingface.co/lmsys/vicuna-13b-delta-v1.1  # more powerful, need at least 24G gpu memory
    $ # or
    $ git clone https://huggingface.co/lmsys/vicuna-7b-delta-v1.1  # smaller, need 12G gpu memory
```

> è¯·æ³¨æ„ï¼Œè¿™ä¸æ˜¯ç›´æ¥çš„ working weight ï¼Œè€Œæ˜¯LLAMA-13Bçš„ working weight ä¸ original weight çš„å·®å€¼ã€‚(ç”±äºLLAMAçš„è§„åˆ™ï¼Œæˆ‘ä»¬æ— æ³•åˆ†é…LLAMAçš„ weight ã€‚)

#### 3.2.2 ä¸‹è½½ åŸå§‹LLAMA-7Bæˆ–LLAMA-13Bæƒé‡

ç„¶åï¼Œæ‚¨éœ€è¦æŒ‰ç…§HuggingFaceæä¾›çš„[åŸå§‹æƒé‡](https://huggingface.co/docs/transformers/main/model_doc/llama) æˆ– ä»äº’è”ç½‘ä¸Šè·å– [HuggingFaceæ ¼å¼çš„åŸå§‹LLAMA-7Bæˆ–LLAMA-13B æƒé‡](https://huggingface.co/decapoda-research/llama-7b-hf)ã€‚

> æ³¨ï¼šè¿™é‡Œ ç›´æ¥ ä» HuggingFace ä¸‹è½½ å·²è½¬åŒ–ä¸º HuggingFaceæ ¼å¼çš„åŸå§‹LLAMA-7Bæˆ–LLAMA-13B æƒé‡

```s
    $ git lfs install
    $ git clone https://huggingface.co/decapoda-research/llama-13b-hf  # more powerful, need at least 24G gpu memory
    $ # or
    $ git clone https://huggingface.co/decapoda-research/llama-7b-hf  # smaller, need 12G gpu memory
```

#### 3.2.3 æ„å»ºçœŸæ­£çš„ working weight  

å½“è¿™ä¸¤ä¸ª weight å¤‡å¥½åï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨Vicunaå›¢é˜Ÿçš„å·¥å…·æ¥åˆ›å»ºçœŸæ­£çš„ working weight  ã€‚

æ‰§è¡Œå¦‚ä¸‹å‘½ä»¤åˆ›å»ºæœ€ç»ˆ working weight

```s
    $ python -m fastchat.model.apply_delta --base /path/to/llama-13bOR7b-hf/  --target /path/to/save/working/vicuna/weight/  --delta /path/to/vicuna-13bOR7b-delta-v1.1/ --low-cpu-mem
    >>>
    The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. 
    The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. 
    The class this function is called from is 'LlamaTokenizer'.
    Split files for the base model to /tmp/tmptu2g17_d
    100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [01:47<00:00,  3.26s/it]
    Split files for the delta model to /tmp/tmpol8jc2oy
    100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [01:03<00:00, 31.92s/it]
    Applying the delta
    33it [02:09,  3.91s/it]
    Saving the target model to vicuna/weight/

```

> æ³¨ï¼šä½CPUå†…å­˜éœ€åŠ å…¥--low-cpu-memï¼Œå¯ä»¥æŠŠå¤§çš„æƒé‡æ–‡ä»¶åˆ†å‰²æˆå¤šä¸ªå°ä»½ï¼Œå¹¶ä½¿ç”¨ç£ç›˜ä½œä¸ºä¸´æ—¶å­˜å‚¨ã€‚å¯ä»¥ä½¿å³°å€¼å†…å­˜ä¿æŒåœ¨16GBä»¥ä¸‹ã€‚ä¸ç„¶æ— æ³•è½½å…¥vicunaå¢é‡æ–‡ä»¶ï¼ŒCPUå†…å­˜å æ»¡ï¼Œç¨‹åºç›´æ¥è¢«killï¼Œ

>  output
```s
config.json           pytorch_model-16.bin  pytorch_model-23.bin  pytorch_model-30.bin  pytorch_model-8.bin
pytorch_model-0.bin   pytorch_model-17.bin  pytorch_model-24.bin  pytorch_model-31.bin  pytorch_model-9.bin
pytorch_model-10.bin  pytorch_model-18.bin  pytorch_model-25.bin  pytorch_model-32.bin  pytorch_model.bin.index.json
pytorch_model-11.bin  pytorch_model-19.bin  pytorch_model-26.bin  pytorch_model-3.bin   special_tokens_map.json
pytorch_model-12.bin  pytorch_model-1.bin   pytorch_model-27.bin  pytorch_model-4.bin   tokenizer_config.json
pytorch_model-13.bin  pytorch_model-20.bin  pytorch_model-28.bin  pytorch_model-5.bin   tokenizer.model
pytorch_model-14.bin  pytorch_model-21.bin  pytorch_model-29.bin  pytorch_model-6.bin
pytorch_model-15.bin  pytorch_model-22.bin  pytorch_model-2.bin   pytorch_model-7.bin

```

### 3.0 å¡«å‘æ‰‹å†Œ

#### 3.0.1 ValueError: Tokenizer class LLaMATokenizer does not exist or is not currently imported.

> å‚è€ƒï¼š
> 1. [MiniGPT-4 æœ¬åœ°éƒ¨ç½² RTX 3090](https://zhuanlan.zhihu.com/p/624417097)
> 2. [LLaMATokenizer does not exist or is not currently imported- LLaMA 4-bit ](https://github.com/oobabooga/text-generation-webui/issues/233)

1. æ‰“å¼€fastchat.model.apply_delta.py
2. ä½¿ç”¨æ–‡æœ¬æ›¿æ¢ï¼Œå°†æ‰€æœ‰çš„
   1. AutoTokenizer æ›¿æ¢ä¸º LlamaTokenizer
   2. AutoModelForCausalLM æ›¿æ¢ä¸º LlamaForCausalLM
   3. ä¿å­˜
3. é‡æ–°è¿è¡Œä¸Šé¢çš„å‘½ä»¤å³å¯ã€‚

#### 3.0.2 å¦‚æœä½ çš„CPUå†…å­˜ä¸è¶³ï¼Œæ‚¨ä¹Ÿå¯ä»¥å°è¯•é€šè¿‡è¿™äº›æ–¹æ³•æ¥å‡å°‘æƒé‡è½¬æ¢å¯¹ CPU å†…å­˜çš„è¦æ±‚

- æ–¹æ¡ˆä¸€ï¼šå°† --low-cpu-mem è¿½åŠ åˆ°ä¸Šé¢çš„å‘½ä»¤ä¸­ï¼Œè¿™ä¼šå°†å¤§æƒé‡æ–‡ä»¶æ‹†åˆ†ä¸ºè¾ƒå°çš„æ–‡ä»¶ï¼Œå¹¶å°†ç£ç›˜ç”¨ä½œä¸´æ—¶å­˜å‚¨ã€‚ è¿™å¯ä»¥å°†å³°å€¼å†…å­˜ä¿æŒåœ¨ 16GB ä»¥ä¸‹ï¼›

```s
    $ python -m fastchat.model.apply_delta --base /mnt/kaimo/data/chat/llama-7b-hf/  --target /mnt/kaimo/data/chat/vicuna/weight/  --delta /mnt/kaimo/data/chat/vicuna-7b-delta-v0/ --low-cpu-mem
```

- æ–¹æ¡ˆäºŒï¼šåˆ›å»ºä¸€ä¸ªå¤§çš„äº¤æ¢æ–‡ä»¶å¹¶ä¾é æ“ä½œç³»ç»Ÿè‡ªåŠ¨çš„å°†ç£ç›˜å½“ä½œè™šæ‹Ÿå†…å­˜ã€‚

#### 3.0.3 tensorå°ºåº¦ä¸ä¸€è‡´

bugï¼štensorå°ºåº¦ä¸ä¸€è‡´

RuntimeError: The size of tensor a (32000) must match the size of tensor b (32001) at non-singleton dimension 0

å½“ä½¿ç”¨v0ç‰ˆæœ¬æ—¶ï¼Œç”Ÿæˆvicunaæƒé‡å‡ºé”™ï¼ˆbugï¼štensorå°ºåº¦ä¸ä¸€è‡´ï¼‰ï¼Œè€Œæ¢ä¸ºv1.1ç‰ˆæœ¬å³å¯è§£å†³ã€‚

## å››ã€ä½¿ç”¨å‘½ä»¤è¡Œæ¥å£è¿›è¡Œæ¨ç†

### 4.1 ä»‹ç»

> æ³¨ï¼šï¼ˆå®éªŒåŠŸèƒ½ï¼šæ‚¨å¯ä»¥æŒ‡å®š--style richï¼Œä»¥ä¾¿ä¸ºæŸäº›éASCIIå†…å®¹æä¾›å¯Œæ ¼æ–‡æœ¬è¾“å‡ºå’Œæ›´å¥½çš„æ–‡æœ¬æµè´¨é‡ã€‚è¿™åœ¨æŸäº›ç»ˆç«¯ä¸Šå¯èƒ½æ— æ³•æ­£å¸¸å·¥ä½œã€‚ï¼‰

```python
init_kwargs {'torch_dtype': torch.float16}
Loading checkpoint shards: 100%| | 33/33 [00:41<00:00,  1.24s/it]
USER: Implement a Python function to compute fibonacci numbers
ASSISTANT: Here is an implementation of a function `fibonacci` that computes the nth Fibonacci number:

def fibonacci(n):
    """
    Compute the nth Fibonacci number using the recurrence relation
    F(n) = F(n-1) + F(n-2)
    """
    if n < 0 or n > 1:
        raise ValueError("n must be a positive integer")
    if n == 0:
        return 0
    if n == 1:
        return 1
    result = fibonacci(n-1) + fibonacci(n-2)
    return result

```

### 4.2 Single GPU

ä¸‹é¢çš„å‘½ä»¤è¦æ±‚Vicuna-13Bå¤§çº¦æœ‰28GBçš„GPUå†…å­˜ï¼ŒVicuna-7Bå¤§çº¦æœ‰14GBçš„GPUå­˜å‚¨å™¨ã€‚å¦‚æœå†…å­˜ä¸è¶³ï¼Œè¯·å‚é˜…ä¸‹é¢çš„â€œå†…å­˜ä¸è¶³â€éƒ¨åˆ†ã€‚

```shell
    $ python -m fastchat.serve.cli --model-path /path/to/vicuna/weights 
```

- å‚æ•°ä»‹ç»

```s
usage: cli.py [-h] 
    [--model-path MODEL_PATH]                     Vicuna Weights è·¯å¾„
    [--device {cpu,cuda,mps}]                     é€‰æ‹© ä½¿ç”¨ cpu or cuda è¿è¡Œ
    [--gpus GPUS]                                 é€‰æ‹© ä½¿ç”¨ gpu å‹å·
    [--num-gpus NUM_GPUS]                         é€‰æ‹© gpu æ•°é‡
    [--max-gpu-memory MAX_GPU_MEMORY] 
    [--load-8bit]                                 8bit é‡åŒ–ï¼Œç”¨äºé™ä½æ˜¾å­˜
    [--conv-template CONV_TEMPLATE]
    [--temperature TEMPERATURE] 
    [--max-new-tokens MAX_NEW_TOKENS] 
    [--style {simple,rich}] 
    [--debug]
```

### 4.3 Multiple GPUs

You can use model parallelism to aggregate GPU memory from multiple GPUs on the same machine.

```shell
    python -m fastchat.serve.cli --model-path /path/to/vicuna/weights --num-gpus 2
```

### 4.4 CPU Only

è¿™åªåœ¨CPUä¸Šè¿è¡Œï¼Œä¸éœ€è¦GPUã€‚Vicuna-13Béœ€è¦å¤§çº¦60GBçš„CPUå†…å­˜ï¼ŒVicuna-7Béœ€è¦å¤§çº¦30GBçš„CPUå­˜å‚¨å™¨ã€‚

```shell
    $ python -m fastchat.serve.cli --model-path /path/to/vicuna/weights --device cpu
```

### 4.0 å¡«å‘æ‰‹å†Œ

#### 4.0.1 No Enough Memory or Other Platforms

å¦‚æœå†…å­˜ä¸è¶³ï¼Œå¯ä»¥é€šè¿‡åœ¨ä¸Šé¢çš„å‘½ä»¤ä¸­æ·»åŠ  --load-8bit æ¥å¯ç”¨ 8ä½å‹ç¼©ã€‚è¿™å¯ä»¥å°†å†…å­˜ä½¿ç”¨é‡å‡å°‘çº¦ä¸€åŠï¼ŒåŒæ—¶ç•¥å¾®é™ä½æ¨¡å‹è´¨é‡ã€‚å®ƒä¸CPUã€GPUå’ŒMetalåç«¯å…¼å®¹ã€‚å…·æœ‰8ä½å‹ç¼©çš„Vicuna-13Bå¯ä»¥åœ¨å•ä¸ªNVIDIA 3090/4080/V100ï¼ˆ16GBï¼‰GPUä¸Šè¿è¡Œã€‚

```s
    $ python -m fastchat.serve.cli --model-path /path/to/vicuna/weights --load-8bit
```

## äº”ã€ä½¿ç”¨Web GUIæœåŠ¡

### 5.1 ä»‹ç»

![](img/20230426093616.jpg)

è¦ä½¿ç”¨web UIæä¾›æœåŠ¡ï¼Œæ‚¨éœ€è¦ä¸‰ä¸ªä¸»è¦ç»„ä»¶ï¼šä¸ç”¨æˆ·æ¥å£çš„webæœåŠ¡å™¨ã€æ‰˜ç®¡ä¸€ä¸ªæˆ–å¤šä¸ªæ¨¡å‹çš„æ¨¡å‹å·¥ä½œè€…ï¼Œä»¥åŠåè°ƒwebæœåŠ¡å™¨å’Œæ¨¡å‹å·¥ä½œè€…çš„æ§åˆ¶å™¨ã€‚ä»¥ä¸‹æ˜¯æ‚¨çš„ç»ˆç«¯ä¸­è¦éµå¾ªçš„å‘½ä»¤ï¼š

1. Launch the controller

```s
    $ python -m fastchat.serve.controller
```

æ­¤æ§åˆ¶å™¨ç®¡ç†åˆ†å¸ƒå¼å·¥ä½œç¨‹åºã€‚

2. Launch the model worker

```s
    $ python -m fastchat.serve.model_worker --model-path /path/to/vicuna/weights
```

ç­‰å¾…æµç¨‹å®ŒæˆåŠ è½½æ¨¡å‹ï¼Œç„¶åçœ‹åˆ°â€œUvicorn running on ...â€ã€‚

æ‚¨å¯ä»¥å¯åŠ¨å¤šä¸ªæ¨¡å‹å·¥ä½œç¨‹åºæ¥åŒæ—¶ä¸ºå¤šä¸ªæ¨¡å‹æä¾›æœåŠ¡ã€‚æ¨¡å‹å·¥äººå°†è‡ªåŠ¨è¿æ¥åˆ°æ§åˆ¶å™¨ã€‚

è¦ç¡®ä¿æ¨¡å‹å·¥ä½œè€…æ­£ç¡®è¿æ¥åˆ°æ§åˆ¶å™¨ï¼Œè¯·ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å‘é€æµ‹è¯•æ¶ˆæ¯ï¼š

```s
    $ python3 -m fastchat.serve.test_message --model-name vicuna-13b
```

3. Launch the Gradio web server

```s
    $ python -m fastchat.serve.gradio_web_server
```

è¿™æ˜¯ç”¨æˆ·å°†ä¸ä¹‹äº¤äº’çš„ç”¨æˆ·ç•Œé¢ã€‚

é€šè¿‡ä»¥ä¸‹æ­¥éª¤ï¼Œæ‚¨å°†èƒ½å¤Ÿä½¿ç”¨web UIä¸ºæ‚¨çš„æ¨¡å‹æä¾›æœåŠ¡ã€‚æ‚¨ç°åœ¨å¯ä»¥æ‰“å¼€æµè§ˆå™¨å¹¶ä¸æ¨¡ç‰¹èŠå¤©äº†ã€‚



## å‚è€ƒ

1. [FastChat](https://github.com/lm-sys/FastChat)
2. [Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality](https://vicuna.lmsys.org/)
3. [Vicuna-13Bæ¨¡å‹å¯åœ¨çº¿è¯•ç©ï¼Œå‚æ•°å·²å¼€æºå¯ä¸‹è½½](https://zhuanlan.zhihu.com/p/619257262)
4. [GPT-4 â€œè®¤ä¸ºâ€ æˆ‘ä»¬çš„å¼€æºç‰ˆå¯¹è¯æ¨¡å‹è¾¾åˆ°äº†ChatGPT 90%çš„æ€§èƒ½ â€”â€” Vicuna å¼€å‘æ·±åº¦ç»éªŒåˆ†äº«](https://zhuanlan.zhihu.com/p/618389519)
5. [facebookresearch/llama](https://github.com/facebookresearch/llama)
6. [stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca)
7. [alpaca-lora](https://github.com/tloen/alpaca-lora)
8. [Alpaca: A Strong, Replicable Instruction-Following Model](https://crfm.stanford.edu/2023/03/13/alpaca.html)
9.  [llama-7b-hf/tree/main](https://huggingface.co/decapoda-research/llama-7b-hf/tree/main)
10. [LLMç³»åˆ— | 02: Vicunaç®€ä»‹åŠæ¨¡å‹éƒ¨ç½²å®æµ‹](https://mp.weixin.qq.com/s/aYISevR_qJTNPZKK_VkmZw)
