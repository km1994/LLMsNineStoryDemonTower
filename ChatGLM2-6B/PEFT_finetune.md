# ã€LLMs å…¥é—¨å®æˆ˜ã€‘åŸºäº ğŸ¤—PEFT çš„é«˜æ•ˆ ğŸ¤–ChatGLM2-6B å¾®è°ƒ

> å‚è€ƒï¼š[hiyouga/ChatGLM-Efficient-Tuning](https://github.com/hiyouga/ChatGLM-Efficient-Tuning)

## ä¸€ã€å‰è¨€

æœ¬æ•™ç¨‹ä¸»è¦ä»‹ç»å¯¹äº ChatGLM2-6B æ¨¡å‹åŸºäº [PEFT](https://github.com/huggingface/peft) çš„ç‰¹å®šä»»åŠ¡å¾®è°ƒå®éªŒã€‚

### 1.1 ç¡¬ä»¶éœ€æ±‚

![](img/20230504085247.png)
> æ³¨ï¼šr ä¸ºLoRA ç»´æ•°å¤§å°ï¼Œp ä¸ºå‰ç¼€è¯è¡¨å¤§å°ï¼Œl ä¸ºå¾®è°ƒå±‚æ•°ï¼Œex/s ä¸ºæ¯ç§’è®­ç»ƒçš„æ ·æœ¬æ•°ã€‚gradient_accumulation_steps å‚æ•°è®¾ç½®ä¸º 1ã€‚ä¸Šè¿°ç»“æœå‡æ¥è‡ªäºå•ä¸ª Tesla V100 GPUï¼Œä»…ä¾›å‚è€ƒã€‚

### 1.2 å¾®è°ƒæ–¹æ³•

ç›®å‰æˆ‘ä»¬å®ç°äº†é’ˆå¯¹ä»¥ä¸‹é«˜æ•ˆå¾®è°ƒæ–¹æ³•çš„æ”¯æŒï¼š

- [LoRA](https://arxiv.org/abs/2106.09685)ï¼šä»…å¾®è°ƒä½ç§©é€‚åº”å™¨ã€‚
- [P-Tuning V2](https://github.com/THUDM/P-tuning-v2)ï¼šä»…å¾®è°ƒå‰ç¼€ç¼–ç å™¨ã€‚
- [Freeze](https://arxiv.org/abs/2012.14913) ï¼šä»…å¾®è°ƒåå‡ å±‚çš„å…¨è¿æ¥å±‚ã€‚

### 1.3 è½¯ä»¶ä¾èµ–

- Python 3.8+, PyTorch 2.0.0
- ğŸ¤—Transformers, Datasets, Accelerate, TRL, PEFTï¼ˆæœ€ä½éœ€è¦ 0.3.0.dev0ï¼‰
- protobuf, cpm_kernels, sentencepiece
- jieba, rouge_chinese, nltkï¼ˆç”¨äºè¯„ä¼°ï¼‰
- gradio, mdtex2htmlï¼ˆç”¨äºç½‘é¡µç«¯äº¤äº’ï¼‰

## äºŒã€ç¯å¢ƒæ­å»º

### 2.1 æ„å»ºç¯å¢ƒ

```s
    $ conda create -n py310_chat python=3.10       # åˆ›å»ºæ–°ç¯å¢ƒ
    $ source activate py310_chat                   # æ¿€æ´»ç¯å¢ƒ
```

### 2.2 ä¸‹è½½ä»£ç 

```s
    $ git clone https://github.com/hiyouga/ChatGLM-Efficient-Tuning.git
    $ cd ChatGLM-Efficient-Tuning
```

### 2.3 å®‰è£…ä¾èµ–

```s
    $ pip install -r requirements.txt
```

## ä¸‰ã€èµ„æºå‡†å¤‡

### 3.1 æ•°æ®æ¥æºä»‹ç»

éƒ¨åˆ†é¢„ç½®æ•°æ®é›†ç®€ä»‹ï¼š

| æ•°æ®é›†åç§° | è§„æ¨¡ | æè¿° |
| --- | --- | --- |
| [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca) | 52k | æ–¯å¦ç¦å¤§å­¦å¼€æºçš„ Alpaca æ•°æ®é›†ï¼Œè®­ç»ƒäº† Alpaca è¿™ç±»æ—©æœŸåŸºäº LLaMA çš„æ¨¡å‹ |
| [Stanford Alpaca (Chinese)](https://github.com/ymcui/Chinese-LLaMA-Alpaca) | 51k | ä½¿ç”¨ ChatGPT ç¿»è¯‘çš„ Alpaca æ•°æ®é›† |
| [GPT-4 Generated Data](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM) | 100k+ | åŸºäº GPT-4 çš„ self-instruction æ•°æ®é›† |
| [BELLE 2M](https://huggingface.co/datasets/BelleGroup/train_2M_CN) | 2m | åŒ…å«çº¦ 200 ä¸‡æ¡ç”± [BELLE](https://github.com/LianjiaTech/BELLE) é¡¹ç›®ç”Ÿæˆçš„ä¸­æ–‡æŒ‡ä»¤æ•°æ® |
| [BELLE 1M](https://huggingface.co/datasets/BelleGroup/train_1M_CN) | 1m | åŒ…å«çº¦ 100 ä¸‡æ¡ç”± [BELLE](https://github.com/LianjiaTech/BELLE) é¡¹ç›®ç”Ÿæˆçš„ä¸­æ–‡æŒ‡ä»¤æ•°æ® |
| [BELLE 0.5M](https://huggingface.co/datasets/BelleGroup/train_0.5M_CN) | 500k  | åŒ…å«çº¦ 50 ä¸‡æ¡ç”± [BELLE](https://github.com/LianjiaTech/BELLE) é¡¹ç›®ç”Ÿæˆçš„ä¸­æ–‡æŒ‡ä»¤æ•°æ® |
| [BELLE Dialogue 0.4M](https://huggingface.co/datasets/BelleGroup/generated_chat_0.4M) | 400k | åŒ…å«çº¦ 40 ä¸‡æ¡ç”± [BELLE](https://github.com/LianjiaTech/BELLE) é¡¹ç›®ç”Ÿæˆçš„ä¸ªæ€§åŒ–è§’è‰²å¯¹è¯æ•°æ®ï¼ŒåŒ…å«è§’è‰²ä»‹ç» |
| [BELLE School Math 0.25M](https://huggingface.co/datasets/BelleGroup/school_math_0.25M) | 250k  | åŒ…å«çº¦ 25 ä¸‡æ¡ç”± [BELLE](https://github.com/LianjiaTech/BELLE) é¡¹ç›®ç”Ÿæˆçš„ä¸­æ–‡æ•°å­¦é¢˜æ•°æ®ï¼ŒåŒ…å«è§£é¢˜è¿‡ç¨‹ |
| [BELLE Multiturn Chat 0.8M](https://huggingface.co/datasets/BelleGroup/multiturn_chat_0.8M) | 800k | åŒ…å«çº¦ 80 ä¸‡æ¡ç”± [BELLE](https://github.com/LianjiaTech/BELLE) é¡¹ç›®ç”Ÿæˆçš„ç”¨æˆ·ä¸åŠ©æ‰‹çš„å¤šè½®å¯¹è¯ |
| [Guanaco Dataset](https://huggingface.co/datasets/JosephusCheung/GuanacoDataset) | 100k+ | åŒ…å«æ—¥æ–‡ã€ç®€ç¹ä½“ä¸­æ–‡ã€è‹±æ–‡ç­‰å¤šç±»æ•°æ®ï¼Œæ•°æ®é›†åŸç”¨äº Guanaco æ¨¡å‹è®­ç»ƒ |
| [Firefly 1.1M](https://huggingface.co/datasets/YeungNLP/firefly-train-1.1M) | 1.1M  | ä¸­æ–‡å¯¹è¯å¤§æ¨¡å‹ fireflyï¼ˆæµè¤ï¼‰çš„ä¸­æ–‡æ•°æ®é›†ï¼ŒåŒ…å«å¤šä¸ª NLP ä»»åŠ¡ |
| [CodeAlpaca 20k](https://huggingface.co/datasets/sahil2801/CodeAlpaca-20k) | 20k | è‹±æ–‡ä»£ç ç”Ÿæˆä»»åŠ¡æ•°æ®é›† |
| [Alpaca CoT](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT) | 6M | ç”¨äºå¾®è°ƒçš„æŒ‡ä»¤æ•°æ®é›†é›†åˆ |
| [Web QA](https://huggingface.co/datasets/suolyer/webqa) | 36k | ç™¾åº¦çŸ¥é“æ±‡é›†çš„ä¸­æ–‡é—®ç­”æ•°æ®é›† |
| [UltraChat](https://github.com/thunlp/UltraChat) | 1.57M | æ¸…å NLP å‘å¸ƒçš„å¤§è§„æ¨¡å¤šè½®å¯¹è¯æ•°æ®é›† |

> æ³¨ï¼šBELLE æ•°æ®é›†æ˜¯ç”± ChatGPT äº§ç”Ÿçš„æ•°æ®é›†ï¼Œä¸ä¿è¯æ•°æ®å‡†ç¡®æ€§ï¼Œæ‰€æœ‰ç±» GPT æ¨¡å‹äº§ç”Ÿçš„ self-instruction æ•°æ®é›†å‡ä¸èƒ½ä¿è¯å…¶å‡†ç¡®æ€§ã€‚

éƒ¨åˆ†æ•°æ®é›†çš„ä½¿ç”¨éœ€è¦ç¡®è®¤ï¼Œæˆ‘ä»¬æ¨èä½¿ç”¨ä¸‹è¿°å‘½ä»¤ç™»å½•æ‚¨çš„ HuggingFace è´¦æˆ·ã€‚

```s
    pip install --upgrade huggingface_hub
    huggingface-cli login
```

### 3.2 æ¨¡å‹ä¸‹è½½ã€å»ºè®®ä¸‹è½½åˆ°æœ¬åœ°ã€‘

ä» [Hugging Face Hub](https://huggingface.co/THUDM/chatglm2-6b/tree/main) ä¸‹è½½æ¨¡å‹å®ç°å’Œå‚æ•° åˆ°æœ¬åœ°ï¼ŒåæœŸä½¿ç”¨ åªéœ€è¦ ä»æœ¬åœ°ä¸‹è½½å³å¯ã€‚

```s
    $ git lfs install
    $ git clone https://huggingface.co/THUDM/chatglm2-6b
```

## å››ã€ChatGLM2-6B æ¨¡å‹å¾®è°ƒ

### 4.1 ChatGLM2-6B æ¨¡å‹å¾®è°ƒ æ•°æ®é›†ä»‹ç»

#### 4.1.1 alpaca_gpt4_zh å¾®è°ƒæ•°æ®é›†ä»‹ç»

```s
[
  {
    "instruction": "ä¿æŒå¥åº·çš„ä¸‰ä¸ªæç¤ºã€‚",
    "input": "",
    "output": "ä»¥ä¸‹æ˜¯ä¿æŒå¥åº·çš„ä¸‰ä¸ªæç¤ºï¼š\n\n1. ä¿æŒèº«ä½“æ´»åŠ¨ã€‚æ¯å¤©åšé€‚å½“çš„èº«ä½“è¿åŠ¨ï¼Œå¦‚æ•£æ­¥ã€è·‘æ­¥æˆ–æ¸¸æ³³ï¼Œèƒ½ä¿ƒè¿›å¿ƒè¡€ç®¡å¥åº·ï¼Œå¢å¼ºè‚Œè‚‰åŠ›é‡ï¼Œå¹¶æœ‰åŠ©äºå‡å°‘ä½“é‡ã€‚\n\n2. å‡è¡¡é¥®é£Ÿã€‚æ¯å¤©é£Ÿç”¨æ–°é²œçš„è”¬èœã€æ°´æœã€å…¨è°·ç‰©å’Œè„‚è‚ªå«é‡ä½çš„è›‹ç™½è´¨é£Ÿç‰©ï¼Œé¿å…é«˜ç³–ã€é«˜è„‚è‚ªå’ŒåŠ å·¥é£Ÿå“ï¼Œä»¥ä¿æŒå¥åº·çš„é¥®é£Ÿä¹ æƒ¯ã€‚\n\n3. ç¡çœ å……è¶³ã€‚ç¡çœ å¯¹äººä½“å¥åº·è‡³å…³é‡è¦ï¼Œæˆå¹´äººæ¯å¤©åº”ä¿è¯ 7-8 å°æ—¶çš„ç¡çœ ã€‚è‰¯å¥½çš„ç¡çœ æœ‰åŠ©äºå‡è½»å‹åŠ›ï¼Œä¿ƒè¿›èº«ä½“æ¢å¤ï¼Œå¹¶æé«˜æ³¨æ„åŠ›å’Œè®°å¿†åŠ›ã€‚"
  },
  {
    "instruction": "ä¸‰åŸè‰²æ˜¯ä»€ä¹ˆï¼Ÿ",
    "input": "",
    "output": "ä¸‰åŸè‰²é€šå¸¸æŒ‡çš„æ˜¯çº¢è‰²ã€ç»¿è‰²å’Œè“è‰²ï¼ˆRGBï¼‰ã€‚å®ƒä»¬æ˜¯é€šè¿‡åŠ è‰²æ··åˆåŸç†åˆ›å»ºè‰²å½©çš„ä¸‰ç§åŸºç¡€é¢œè‰²ã€‚åœ¨ä»¥å‘å…‰ä¸ºåŸºç¡€çš„æ˜¾ç¤ºè®¾å¤‡ä¸­ï¼ˆå¦‚ç”µè§†ã€è®¡ç®—æœºæ˜¾ç¤ºå™¨ã€æ™ºèƒ½æ‰‹æœºå’Œå¹³æ¿ç”µè„‘æ˜¾ç¤ºå±ï¼‰, ä¸‰åŸè‰²å¯æ··åˆäº§ç”Ÿå¤§é‡è‰²å½©ã€‚å…¶ä¸­çº¢è‰²å’Œç»¿è‰²å¯ä»¥æ··åˆç”Ÿæˆé»„è‰²ï¼Œçº¢è‰²å’Œè“è‰²å¯ä»¥æ··åˆç”Ÿæˆå“çº¢è‰²ï¼Œè“è‰²å’Œç»¿è‰²å¯ä»¥æ··åˆç”Ÿæˆé’è‰²ã€‚å½“çº¢è‰²ã€ç»¿è‰²å’Œè“è‰²æŒ‰ç›¸ç­‰æ¯”ä¾‹æ··åˆæ—¶ï¼Œå¯ä»¥äº§ç”Ÿç™½è‰²æˆ–ç°è‰²ã€‚\n\næ­¤å¤–ï¼Œåœ¨å°åˆ·å’Œç»˜ç”»ä¸­ï¼Œä¸‰åŸè‰²æŒ‡çš„æ˜¯ä»¥é¢œæ–™ä¸ºåŸºç¡€çš„çº¢ã€é»„å’Œè“é¢œè‰²ï¼ˆRYBï¼‰ã€‚è¿™ä¸‰ç§é¢œè‰²ç”¨ä»¥é€šè¿‡å‡è‰²æ··åˆåŸç†æ¥åˆ›å»ºè‰²å½©ã€‚ä¸è¿‡ï¼Œä¸‰åŸè‰²çš„å…·ä½“å®šä¹‰å¹¶ä¸å”¯ä¸€ï¼Œä¸åŒçš„é¢œè‰²ç³»ç»Ÿå¯èƒ½ä¼šé‡‡ç”¨ä¸åŒçš„ä¸‰åŸè‰²ã€‚"
  },
  ...
]
```

### 4.2 ChatGLM2-6B æ¨¡å‹å¾®è°ƒ

#### 4.2.1 ChatGLM2-6B æ¨¡å‹ å• GPU å¾®è°ƒè®­ç»ƒ

ChatGLM2-6B æ¨¡å‹çš„å¾®è°ƒã€‚éœ€è¦ä½¿ç”¨--use_v2 å‚æ•°æ¥è¿›è¡Œè®­ç»ƒã€‚

è¿è¡Œä»¥ä¸‹æŒ‡ä»¤è¿›è¡Œå¾®è°ƒï¼š

- freeze æ–¹å¼ å¾®è°ƒ

```s
    $ CUDA_VISIBLE_DEVICES=0 python ../src/train_sft.py \
    --do_train \
    --dataset alpaca_gpt4_zh \
    --dataset_dir ../data \
    --finetuning_type freeze \
    --output_dir path_to_sft_checkpoint \
    --overwrite_cache \
    --per_device_train_batch_size 1 \
    --gradient_accumulation_steps 1 \
    --lr_scheduler_type cosine \
    --logging_steps 10 \
    --save_steps 1000 \
    --learning_rate 5e-5 \
    --num_train_epochs 3.0 \
    --plot_loss \
    --fp16 \
    --use_v2        
```

- p_tuning æ–¹å¼ å¾®è°ƒ

```s
    $ CUDA_VISIBLE_DEVICES=0 python ../src/train_sft.py \
    --do_train \
    --dataset alpaca_gpt4_zh \
    --dataset_dir ../data \
    --finetuning_type p_tuning \
    --output_dir path_to_sft_checkpoint \
    --overwrite_cache \
    --per_device_train_batch_size 1 \
    --gradient_accumulation_steps 1 \
    --lr_scheduler_type cosine \
    --logging_steps 10 \
    --save_steps 1000 \
    --learning_rate 5e-5 \
    --num_train_epochs 3.0 \
    --plot_loss \
    --use_v2         
```

> output
```s
    ...
    100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 146454/146454 [7:51:57<00:00,  4.03it/s][INFO|trainer.py:2053] 2023-07-01 09:22:53,773 >> 

  Training completed. Do not forget to share your model on huggingface.co/models =)


  {'train_runtime': 28324.7089, 'train_samples_per_second': 5.171, 'train_steps_per_second': 5.171, 'train_loss': 0.177752665720254, 'epoch': 3.0}
  100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 146454/146454 [7:51:57<00:00,  5.17it/s]
  ***** train metrics *****
    epoch                    =        3.0
    train_loss               =     0.1778
    train_runtime            = 7:52:04.70
    train_samples_per_second =      5.171
    train_steps_per_second   =      5.171
  07/01/2023 09:22:53 - INFO - utils.peft_trainer - Saving model checkpoint to path_to_sft_checkpoint
  [INFO|configuration_utils.py:458] 2023-07-01 09:22:53,897 >> Configuration saved in path_to_sft_checkpoint/config.json
  [INFO|configuration_utils.py:364] 2023-07-01 09:22:53,897 >> Configuration saved in path_to_sft_checkpoint/generation_config.json
  [INFO|modeling_utils.py:1853] 2023-07-01 09:22:53,898 >> Model weights saved in path_to_sft_checkpoint/pytorch_model.bin
  [INFO|tokenization_utils_base.py:2194] 2023-07-01 09:22:53,898 >> tokenizer config file saved in path_to_sft_checkpoint/tokenizer_config.json
  [INFO|tokenization_utils_base.py:2201] 2023-07-01 09:22:53,898 >> Special tokens file saved in path_to_sft_checkpoint/special_tokens_map.json
  Figure saved: path_to_sft_checkpoint/training_loss.png
  07/01/2023 09:43:24 - WARNING - utils.other - No metric eval_loss to plot.

  wandb: Waiting for W&B process to finish... (success).
  wandb:                                                                                
  wandb: 
  wandb: Run history:
  wandb:                    train/epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
  wandb:              train/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
  wandb:            train/learning_rate â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–…â–…â–…â–…â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–
  wandb:                     train/loss â–ˆâ–†â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
  wandb:               train/total_flos â–
  wandb:               train/train_loss â–
  wandb:            train/train_runtime â–
  wandb: train/train_samples_per_second â–
  wandb:   train/train_steps_per_second â–
  wandb: 
  wandb: Run summary:
  wandb:                    train/epoch 3.0
  wandb:              train/global_step 146454
  wandb:            train/learning_rate 0.0
  wandb:                     train/loss 0.0086
  wandb:               train/total_flos 8.419979127255368e+17
  wandb:               train/train_loss 0.17775
  wandb:            train/train_runtime 28324.7089
  wandb: train/train_samples_per_second 5.171
  wandb:   train/train_steps_per_second 5.171
  wandb: 
  wandb: Synced wise-durian-50: https://wandb.ai/13025232601/huggingface/runs/202g1fjg
  wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
  wandb: Find logs at: ./wandb/run-20230701_013050-202g1fjg/logs


```

- lora æ–¹å¼ å¾®è°ƒ

```s
    $ CUDA_VISIBLE_DEVICES=0 python ../src/train_sft.py \
    --do_train \
    --dataset alpaca_gpt4_zh \
    --dataset_dir ../data \
    --finetuning_type lora \
    --output_dir path_to_sft_checkpoint \
    --overwrite_cache \
    --per_device_train_batch_size 1 \
    --gradient_accumulation_steps 1 \
    --lr_scheduler_type cosine \
    --logging_steps 10 \
    --save_steps 1000 \
    --learning_rate 5e-5 \
    --num_train_epochs 3.0 \
    --plot_loss \
    --fp16 \
    --use_v2        
```

> output
```s
...
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 146454/146454 [8:39:27<00:00,  3.44it/s]{'train_runtime': 31174.0434, 'train_samples_per_second': 4.698, 'train_steps_per_second': 4.698, 'train_loss': 1.6946858559184583, 'epoch': 3.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 146454/146454 [8:39:27<00:00,  4.70it/s]
***** train metrics *****
  epoch                    =        3.0
  train_loss               =     1.6947
  train_runtime            = 8:39:34.04
  train_samples_per_second =      4.698
  train_steps_per_second   =      4.698
07/01/2023 00:22:01 - INFO - utils.peft_trainer - Saving model checkpoint to path_to_sft_checkpoint
Figure saved: path_to_sft_checkpoint/training_loss.png
07/01/2023 01:11:13 - WARNING - utils.other - No metric eval_loss to plot.

wandb: Waiting for W&B process to finish... (success).
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                    train/epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:              train/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:            train/learning_rate â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–…â–…â–…â–…â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–
wandb:                     train/loss â–…â–…â–„â–„â–†â–ƒâ–ƒâ–ƒâ–†â–„â–…â–†â–ƒâ–ƒâ–â–‚â–â–‚â–‚â–…â–‚â–…â–‡â–‚â–…â–ƒâ–ˆâ–…â–†â–ƒâ–‡â–‚â–‚â–ƒâ–…â–„â–…â–‚â–ƒâ–†
wandb:               train/total_flos â–
wandb:               train/train_loss â–
wandb:            train/train_runtime â–
wandb: train/train_samples_per_second â–
wandb:   train/train_steps_per_second â–
wandb: 
wandb: Run summary:
wandb:                    train/epoch 3.0
wandb:              train/global_step 146454
wandb:            train/learning_rate 0.0
wandb:                     train/loss 1.4363
wandb:               train/total_flos 8.422725609575055e+17
wandb:               train/train_loss 1.69469
wandb:            train/train_runtime 31174.0434
wandb: train/train_samples_per_second 4.698
wandb:   train/train_steps_per_second 4.698
```

#### 4.2.2 ChatGLM2-6B æ¨¡å‹ å¤š GPU åˆ†å¸ƒå¼å¾®è°ƒ

1. é…ç½® åˆ†å¸ƒå¼ç¯å¢ƒ

```s
    $ accelerate config # é¦–å…ˆé…ç½®åˆ†å¸ƒå¼ç¯å¢ƒ
```

> æ³¨ï¼šæ³¨æ„ï¼šè‹¥æ‚¨ä½¿ç”¨ LoRA æ–¹æ³•è¿›è¡Œå¾®è°ƒï¼Œè¯·æŒ‡å®šä»¥ä¸‹å‚æ•° --ddp_find_unused_parameters False æ¥é¿å…æŠ¥é”™ã€‚

2. è¿è¡Œä»¥ä¸‹æŒ‡ä»¤è¿›è¡Œå¾®è°ƒï¼š

> lora æ–¹å¼ å¾®è°ƒ
```s
    $ accelerate launch src/train_sft.py # å‚æ•°åŒä¸Š
```

## äº”ã€ChatGLM2-6B è¯„ä¼°é¢„æµ‹

### 5.1 ChatGLM2-6B æŒ‡æ ‡è¯„ä¼°ï¼ˆBLEUåˆ†æ•°å’Œæ±‰è¯­ROUGEåˆ†æ•°ï¼‰

```s
CUDA_VISIBLE_DEVICES=0 python src/finetune.py \
    --do_eval \
    --dataset alpaca_gpt4_zh \
    --checkpoint_dir path_to_checkpoint \
    --output_dir path_to_eval_result \
    --per_device_eval_batch_size 8 \
    --max_samples 50 \
    --predict_with_generate
```

> output
```s
===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
...
mon - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=-1,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=path_to_eval_result/runs/May05_00-58-16_tgnet,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=path_to_eval_result,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=8,
predict_with_generate=True,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard', 'wandb'],
resume_from_checkpoint=None,
run_name=path_to_eval_result,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
sortish_sampler=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
...
[INFO|configuration_utils.py:720] 2023-05-05 00:58:18,251 >> Model config ChatGLMConfig {
  "_name_or_path": "THUDM/ChatGLM2-6B",
  "architectures": [
    "ChatGLMModel"
  ],
  "auto_map": {
    "AutoConfig": "configuration_chatglm.ChatGLMConfig",
    "AutoModel": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForSeq2SeqLM": "modeling_chatglm.ChatGLMForConditionalGeneration"
  },
  "bos_token_id": 130004,
  "eos_token_id": 130005,
  "gmask_token_id": 130001,
  "hidden_size": 4096,
  "inner_hidden_size": 16384,
  "layernorm_epsilon": 1e-05,
  "mask_token_id": 130000,
  "max_sequence_length": 2048,
  "model_type": "chatglm",
  "num_attention_heads": 32,
  "num_layers": 28,
  "pad_token_id": 3,
  "position_encoding_2d": true,
  "pre_seq_len": null,
  "prefix_projection": false,
  "quantization_bit": 0,
  "torch_dtype": "float16",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 130528
}
[INFO|configuration_utils.py:575] 2023-05-05 00:58:18,291 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 130004,
  "eos_token_id": 130005,
  "pad_token_id": 3,
  "transformers_version": "4.28.1"
}

Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:10<00:00,  1.30s/it]
...
[INFO|modeling_utils.py:2839] 2023-05-05 00:58:29,280 >> Generation config file not found, using a generation config created from the model config.
05/05/2023 00:59:18 - INFO - utils.common - Quantized model to 4 bit.
05/05/2023 00:59:18 - INFO - utils.common - Fine-tuning method: P-Tuning V2
trainable params: 3670016 || all params: 3359416320 || trainable%: 0.1092
05/05/2023 00:59:18 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /.cache/huggingface/datasets/json/default-5c75ee3f92a08afd/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e/cache-a21964d2ca8fe3cd.arrow
input_ids:
[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 112991, 80990, 66334, 63823, 130001, 130004]
inputs:
ä¿æŒå¥åº·çš„ä¸‰ä¸ªæç¤ºã€‚
label_ids:
[82235, 112991, 80990, 66334, 12, 4, 4, 9, 7, 5, 64442, 64310, 63987, 63823, 64354, 63912, 70666, 64310, 64287, 6, 63906, 71738, 63824, 70153, 63853, 68483, 6, 83231, 83242, 64176, 6, 65337, 66448, 65006, 6, 63885, 67623, 64651, 67266, 63823, 4, 4, 10, 7, 5, 71356, 65821, 63823, 64354, 65979, 73362, 66296, 63824, 66220, 63824, 64080, 89181, 63826, 100913, 64284, 94211, 65091, 6, 65073, 63905, 65044, 63824, 105241, 63826, 65521, 65060, 6, 63847, 112991, 108006, 63823, 4, 4, 13, 7, 5, 66625, 69769, 63823, 66625, 118143, 76038, 6, 73929, 64354, 64064, 64849, 5, 25, 11, 23, 5, 88081, 66625, 63823, 66584, 66625, 67623, 67455, 64700, 6, 64721, 64310, 65181, 6, 63885, 64299, 73066, 63826, 75991, 63823, 130001, 130004]
labels:
ä»¥ä¸‹æ˜¯ä¿æŒå¥åº·çš„ä¸‰ä¸ªæç¤º:

1. ä¿æŒèº«ä½“æ´»åŠ¨ã€‚æ¯å¤©åšé€‚å½“çš„èº«ä½“è¿åŠ¨,å¦‚æ•£æ­¥ã€è·‘æ­¥æˆ–æ¸¸æ³³,èƒ½ä¿ƒè¿›å¿ƒè¡€ç®¡å¥åº·,å¢å¼ºè‚Œè‚‰åŠ›é‡,å¹¶æœ‰åŠ©äºå‡å°‘ä½“é‡ã€‚

2. å‡è¡¡é¥®é£Ÿã€‚æ¯å¤©é£Ÿç”¨æ–°é²œçš„è”¬èœã€æ°´æœã€å…¨è°·ç‰©å’Œè„‚è‚ªå«é‡ä½çš„è›‹ç™½è´¨é£Ÿç‰©,é¿å…é«˜ç³–ã€é«˜è„‚è‚ªå’ŒåŠ å·¥é£Ÿå“,ä»¥ä¿æŒå¥åº·çš„é¥®é£Ÿä¹ æƒ¯ã€‚

3. ç¡çœ å……è¶³ã€‚ç¡çœ å¯¹äººä½“å¥åº·è‡³å…³é‡è¦,æˆå¹´äººæ¯å¤©åº”ä¿è¯ 7-8 å°æ—¶çš„ç¡çœ ã€‚è‰¯å¥½çš„ç¡çœ æœ‰åŠ©äºå‡è½»å‹åŠ›,ä¿ƒè¿›èº«ä½“æ¢å¤,å¹¶æé«˜æ³¨æ„åŠ›å’Œè®°å¿†åŠ›ã€‚
[INFO|trainer.py:3129] 2023-05-05 00:59:19,444 >> ***** Running Evaluation *****
[INFO|trainer.py:3131] 2023-05-05 00:59:19,444 >>   Num examples = 50
[INFO|trainer.py:3134] 2023-05-05 00:59:19,444 >>   Batch size = 1
[INFO|configuration_utils.py:575] 2023-05-05 00:59:19,449 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 130004,
  "eos_token_id": 130005,
  "pad_token_id": 3,
  "transformers_version": "4.28.1"
}

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 ...
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [07:41<00:00,  9.24s/it]
***** eval metrics *****
  eval_bleu-4             =    13.0515
  eval_rouge-1            =    33.0999
  eval_rouge-2            =    13.6305
  eval_rouge-l            =    24.3066
  eval_runtime            = 0:07:43.40
  eval_samples_per_second =      0.108
  eval_steps_per_second   =      0.108

wandb: Waiting for W&B process to finish... (success).
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:             eval/bleu-4 â–
wandb:            eval/rouge-1 â–
wandb:            eval/rouge-2 â–
wandb:            eval/rouge-l â–
wandb:            eval/runtime â–
wandb: eval/samples_per_second â–
wandb:   eval/steps_per_second â–
wandb:       train/global_step â–
wandb: 
wandb: Run summary:
wandb:             eval/bleu-4 13.05145
wandb:            eval/rouge-1 33.09988
wandb:            eval/rouge-2 13.63049
wandb:            eval/rouge-l 24.30655
wandb:            eval/runtime 463.4072
wandb: eval/samples_per_second 0.108
wandb:   eval/steps_per_second 0.108
wandb:       train/global_step 0
wandb: 
wandb: Synced imperial-council-44: https://wandb.ai/13025232601/huggingface/runs/37i3uo74
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230505_010703-37i3uo74/logs
```

### 5.2 ChatGLM2-6B æ¨¡å‹é¢„æµ‹

```s
  CUDA_VISIBLE_DEVICES=0 python src/finetune.py \
    --do_predict \
    --dataset alpaca_gpt4_zh \
    --checkpoint_dir path_to_checkpoint \
    --output_dir path_to_predict_result \
    --per_device_eval_batch_size 8 \
    --max_samples 50 \
    --predict_with_generate
```

> output
```s
...
input_ids:
[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 112991, 80990, 66334, 63823, 130001, 130004]
inputs:
ä¿æŒå¥åº·çš„ä¸‰ä¸ªæç¤ºã€‚
label_ids:
[82235, 112991, 80990, 66334, 12, 4, 4, 9, 7, 5, 64442, 64310, 63987, 63823, 64354, 63912, 70666, 64310, 64287, 6, 63906, 71738, 63824, 70153, 63853, 68483, 6, 83231, 83242, 64176, 6, 65337, 66448, 65006, 6, 63885, 67623, 64651, 67266, 63823, 4, 4, 10, 7, 5, 71356, 65821, 63823, 64354, 65979, 73362, 66296, 63824, 66220, 63824, 64080, 89181, 63826, 100913, 64284, 94211, 65091, 6, 65073, 63905, 65044, 63824, 105241, 63826, 65521, 65060, 6, 63847, 112991, 108006, 63823, 4, 4, 13, 7, 5, 66625, 69769, 63823, 66625, 118143, 76038, 6, 73929, 64354, 64064, 64849, 5, 25, 11, 23, 5, 88081, 66625, 63823, 66584, 66625, 67623, 67455, 64700, 6, 64721, 64310, 65181, 6, 63885, 64299, 73066, 63826, 75991, 63823, 130001, 130004]
labels:
ä»¥ä¸‹æ˜¯ä¿æŒå¥åº·çš„ä¸‰ä¸ªæç¤º:

1. ä¿æŒèº«ä½“æ´»åŠ¨ã€‚æ¯å¤©åšé€‚å½“çš„èº«ä½“è¿åŠ¨,å¦‚æ•£æ­¥ã€è·‘æ­¥æˆ–æ¸¸æ³³,èƒ½ä¿ƒè¿›å¿ƒè¡€ç®¡å¥åº·,å¢å¼ºè‚Œè‚‰åŠ›é‡,å¹¶æœ‰åŠ©äºå‡å°‘ä½“é‡ã€‚

2. å‡è¡¡é¥®é£Ÿã€‚æ¯å¤©é£Ÿç”¨æ–°é²œçš„è”¬èœã€æ°´æœã€å…¨è°·ç‰©å’Œè„‚è‚ªå«é‡ä½çš„è›‹ç™½è´¨é£Ÿç‰©,é¿å…é«˜ç³–ã€é«˜è„‚è‚ªå’ŒåŠ å·¥é£Ÿå“,ä»¥ä¿æŒå¥åº·çš„é¥®é£Ÿä¹ æƒ¯ã€‚

3. ç¡çœ å……è¶³ã€‚ç¡çœ å¯¹äººä½“å¥åº·è‡³å…³é‡è¦,æˆå¹´äººæ¯å¤©åº”ä¿è¯ 7-8 å°æ—¶çš„ç¡çœ ã€‚è‰¯å¥½çš„ç¡çœ æœ‰åŠ©äºå‡è½»å‹åŠ›,ä¿ƒè¿›èº«ä½“æ¢å¤,å¹¶æé«˜æ³¨æ„åŠ›å’Œè®°å¿†åŠ›ã€‚
[INFO|trainer.py:3129] 2023-05-05 04:42:02,531 >> ***** Running Prediction *****
[INFO|trainer.py:3131] 2023-05-05 04:42:02,531 >>   Num examples = 50
[INFO|trainer.py:3134] 2023-05-05 04:42:02,531 >>   Batch size = 8
[INFO|configuration_utils.py:575] 2023-05-05 04:42:02,539 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 130004,
  "eos_token_id": 130005,
  "pad_token_id": 3,
  "transformers_version": "4.28.1"
}

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [02:07<00:00, 19.63s/it]Building prefix dict from the default dictionary ...
05/05/2023 04:44:34 - DEBUG - jieba - Building prefix dict from the default dictionary ...
Loading model from cache /tmp/jieba.cache
05/05/2023 04:44:34 - DEBUG - jieba - Loading model from cache /tmp/jieba.cache
Loading model cost 0.221 seconds.
05/05/2023 04:44:34 - DEBUG - jieba - Loading model cost 0.221 seconds.
Prefix dict has been built successfully.
05/05/2023 04:44:34 - DEBUG - jieba - Prefix dict has been built successfully.
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [02:07<00:00, 18.26s/it]
***** predict metrics *****
  predict_bleu-4             =    11.0719
  predict_rouge-1            =    31.7603
  predict_rouge-2            =    12.2967
  predict_rouge-l            =    23.4435
  predict_runtime            = 0:02:32.01
  predict_samples_per_second =      0.329
  predict_steps_per_second   =      0.046
05/05/2023 04:44:34 - INFO - utils.seq2seq - Saving prediction results to path_to_predict_result/generated_predictions.txt

```

### 5.3 ChatGLM2-6B æ¨¡å‹æ¨ç†

```s
  CUDA_VISIBLE_DEVICES=0 python src/infer.py  --checkpoint_dir path_to_checkpoint --quantization_bit=4
```

> å¯¹è¯ä¸€
```s
User: æ”¿åºœå¯ä»¥é‡‡å–å“ªäº›ç­–ç•¥æ¥å‡å°‘ç©ºæ°”æ±¡æŸ“ï¼Ÿ

ChatGLM2-6B: æ”¿åºœå¯ä»¥é‡‡å–ä»¥ä¸‹ç­–ç•¥æ¥å‡å°‘ç©ºæ°”æ±¡æŸ“ï¼š

1. æ¨å¹¿æ¸…æ´èƒ½æºï¼šæ”¿åºœå¯ä»¥é€šè¿‡æ¨å¹¿å¤ªé˜³èƒ½ã€é£èƒ½ã€æ°´èƒ½ç­‰æ¸…æ´èƒ½æºæ¥å‡å°‘ç©ºæ°”æ±¡æŸ“ã€‚æ”¿åºœè¿˜å¯ä»¥é¼“åŠ±äººä»¬ä½¿ç”¨ç”µåŠ¨æ±½è½¦ã€æ··åˆåŠ¨åŠ›æ±½è½¦ã€èŠ‚èƒ½ç¯å…·ç­‰ç¯ä¿äº§å“ã€‚
2. æ”¹å–„äº¤é€šæ–¹å¼ï¼šæ”¿åºœå¯ä»¥æ”¹å–„äº¤é€šæ–¹å¼ï¼Œå¦‚å‡å°‘æ±½è½¦å°¾æ°”æ’æ”¾ï¼Œæé«˜å…¬å…±äº¤é€šçš„ä½¿ç”¨æ•ˆç‡ï¼Œæ”¹å–„é“è·¯çš„é€šè¡Œæ¡ä»¶ç­‰ã€‚
3. æ§åˆ¶å·¥ä¸šæ±¡æŸ“ï¼šæ”¿åºœå¯ä»¥é€šè¿‡æ”¹å–„å·¥ä¸šæ±¡æŸ“æ²»ç†ã€åŠ å¼ºç¯å¢ƒç›‘æµ‹æ¥å‡å°‘å·¥ä¸šæ±¡æŸ“ã€‚
4. åŠ å¼ºç¯å¢ƒæ•™è‚²ï¼šæ”¿åºœå¯ä»¥é€šè¿‡åŠ å¼ºç¯å¢ƒæ•™è‚²ï¼Œæé«˜å…¬ä¼—çš„ç¯å¢ƒæ„è¯†å’Œç¯ä¿æ„è¯†ï¼Œé¼“åŠ±äººä»¬é‡‡å–ç¯ä¿è¡ŒåŠ¨ã€‚
5. å®æ–½ç¯å¢ƒæ³•è§„ï¼šæ”¿åºœå¯ä»¥é€šè¿‡åˆ¶å®šå’Œæ‰§è¡Œç¯å¢ƒæ³•è§„ï¼ŒåŠ å¼ºç¯å¢ƒç›‘ç®¡ï¼Œæ‰“å‡»ç¯å¢ƒè¿æ³•è¡Œä¸ºï¼Œä¿è¯ç¯å¢ƒçš„å¯æŒç»­å‘å±•ã€‚

æ”¿åºœå¯ä»¥é€šè¿‡ä»¥ä¸Šå¤šç§ç­–ç•¥æ¥å‡å°‘ç©ºæ°”æ±¡æŸ“ï¼Œä¸ºäººä»¬åˆ›é€ æ›´åŠ å¥åº·ã€èˆ’é€‚çš„ç¯å¢ƒã€‚
```

> å¯¹è¯äºŒ
```s
User: è‡ªä¸»æœºå™¨äººæ˜¯è®¡ç®—æœºæ§åˆ¶çš„æœºå™¨ï¼Œè¢«ç¼–ç¨‹æ‰§è¡Œç‰¹å®šä»»åŠ¡è€Œä¸éœ€è¦ä»»ä½•äººç±»è¾“å…¥ã€‚è‡ªä¸»æœºå™¨äººåœ¨å„ä¸ªè¡Œä¸šä¸­è¢«è¶Šæ¥è¶Šå¹¿æ³›åœ°åº”ç”¨ï¼Œä»åˆ¶é€ ä¸šåˆ°åŒ»ç–—ä¿å¥å†åˆ°å®‰å…¨ã€‚

ChatGLM2-6B: è‡ªä¸»æœºå™¨äººæ˜¯æŒ‡è®¡ç®—æœºæ§åˆ¶çš„æœºå™¨ï¼Œè¢«ç¼–ç¨‹æ‰§è¡Œç‰¹å®šä»»åŠ¡è€Œä¸éœ€è¦ä»»ä½•äººç±»è¾“å…¥ã€‚è¿™ç§æŠ€æœ¯åœ¨å„ä¸ªè¡Œä¸šä¸­éƒ½å¾—åˆ°äº†è¶Šæ¥è¶Šå¹¿æ³›çš„åº”ç”¨ï¼Œä»åˆ¶é€ ä¸šåˆ°åŒ»ç–—ä¿å¥å†åˆ°å®‰å…¨ã€‚

åœ¨åˆ¶é€ ä¸šä¸­ï¼Œè‡ªä¸»æœºå™¨äººå¯ä»¥æ‰§è¡Œé‡å¤æ€§ä»»åŠ¡ï¼Œå¦‚è£…é…çº¿ã€åŒ…è£…ã€ç‰©æµç­‰ã€‚å®ƒä»¬å¯ä»¥æé«˜ç”Ÿäº§æ•ˆç‡å’Œè´¨é‡ï¼Œå‡å°‘äººå·¥æ“ä½œï¼Œæé«˜å®‰å…¨æ€§å’Œå¯é æ€§ã€‚
åœ¨åŒ»ç–—ä¿å¥é¢†åŸŸï¼Œè‡ªä¸»æœºå™¨äººå¯ä»¥æ‰§è¡Œæ‰‹æœ¯ã€æ²»ç–—ã€æŠ¤ç†ç­‰ä»»åŠ¡ã€‚å®ƒä»¬å¯ä»¥æé«˜åŒ»ç–—æ•ˆç‡å’Œå‡†ç¡®æ€§ï¼Œå‡å°‘æ‰‹æœ¯é£é™©å’Œé”™è¯¯ã€‚
åœ¨å®‰å…¨é¢†åŸŸï¼Œè‡ªä¸»æœºå™¨äººå¯ä»¥æ‰§è¡Œå·¡é€»ã€ç›‘æ§ã€æ•‘æ´ç­‰ä»»åŠ¡ã€‚å®ƒä»¬å¯ä»¥æé«˜å®‰å…¨æ€§å’Œå¯é æ€§ï¼Œå‡å°‘äººä¸ºé”™è¯¯å’Œå¤±è¯¯ã€‚
è‡ªä¸»æœºå™¨äººæŠ€æœ¯åœ¨å„ä¸ªè¡Œä¸šéƒ½æœ‰å¾ˆå¤šåº”ç”¨ï¼Œå¯ä»¥æé«˜ç”Ÿäº§æ•ˆç‡ã€è´¨é‡å’Œå®‰å…¨æ€§ã€‚éšç€æŠ€æœ¯çš„å‘å±•ï¼Œè‡ªä¸»æœºå™¨äººä¹Ÿä¼šå˜å¾—æ›´åŠ æ™ºèƒ½å’Œäººæ€§åŒ–ï¼Œæˆä¸ºäººç±»çš„ä¼™ä¼´ã€‚
```

### 5.4 ChatGLM2-6B æµè§ˆå™¨æµ‹è¯•

```s
  CUDA_VISIBLE_DEVICES=0 python src/web_demo.py \
    --checkpoint_dir path_to_checkpoint
```

## å…­ã€è¸©å‘ç¬”è®°

### 6.1 ç¬¬ä¸‰æ­¥å‡ºç°é”™è¯¯ï¼šRuntimeError: probability tensor contains either inf, nan or element < 0

- é—®é¢˜æè¿°

```s
  ...
  next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)
  RuntimeError: probability tensor contains either inf, nan or element < 0
```

- è§£å†³æ–¹æ³•: å°† model.generate ä¸­  è®¾ç½® do_sample=False 

### 6.2 å¾®è°ƒæ˜¾å­˜é—®é¢˜

- é—®é¢˜æè¿°ï¼šåœ¨ æ‰§è¡Œ å¾®è°ƒå‘½ä»¤æ—¶ï¼Œçˆ† æ˜¾å­˜ä¸è¶³
- è§£å†³æ–¹æ³•ï¼šåœ¨ train_sft.sh ä¸­ æ·»åŠ  å‚æ•° --quantization_bit=8ï¼Œè¯¥å‘½ä»¤çš„ä½œç”¨æ—¶ è¿›è¡Œ 8bit é‡åŒ–

### 6.3 ä½¿ç”¨ freeze è¿›è¡Œ é‡åŒ–å¾®è°ƒæ—¶å‡ºé”™

- é—®é¢˜æè¿°ï¼šä½¿ç”¨ freeze è¿›è¡Œ é‡åŒ–å¾®è°ƒæ—¶å‡ºé”™
- é—®é¢˜åŸå› ï¼šfreezeå¾®è°ƒè®­ç»ƒï¼Œä¸èƒ½è¿›è¡Œ é‡åŒ–æ“ä½œ
- è§£å†³æ–¹æ³•ï¼šåˆ é™¤ --quantization_bit=8 å³å¯

### 6.4 ä½¿ç”¨  P-Tuning v2 è¿›è¡Œ å¾®è°ƒæ—¶  AssertionError: Please disable fp16 training while using the P-Tuning v2 method.

- é—®é¢˜æè¿°ï¼šä½¿ç”¨ freeze è¿›è¡Œ é‡åŒ–å¾®è°ƒæ—¶å‡ºé”™

```s
  $ AssertionError: Please disable fp16 training while using the P-Tuning v2 method.
```

- é—®é¢˜åŸå› ï¼šP-Tuning v2 å¾®è°ƒ ä¸æ”¯æŒ fp16
- è§£å†³æ–¹æ³•ï¼šåˆ é™¤ --fp16 å³å¯

## å‚è€ƒ/æ„Ÿè°¢

1. [THUDM/ChatGLM2-6B](https://github.com/THUDM/ChatGLM2-6B)
2. [hiyouga/ChatGLM-Efficient-Tuning](https://github.com/hiyouga/ChatGLM-Efficient-Tuning)
