# ã€LLMs å…¥é—¨å®æˆ˜ â€”â€” äº” ã€‘Stanford Alpaca 7B æ¨¡å‹å­¦ä¹ ä¸å®æˆ˜

- github åœ°å€: https://github.com/tatsu-lab/stanford_alpaca
- è¯•ç”¨åœ°å€ï¼šhttps://alpaca-ai-custom6.ngrok.io/

## ã€LLMs å…¥é—¨å®æˆ˜ç³»åˆ—ã€‘

### ç¬¬ä¸€å±‚ ChatGLM-6B

1. [ã€ChatGLM-6Bå…¥é—¨-ä¸€ã€‘æ¸…åå¤§å­¦å¼€æºä¸­æ–‡ç‰ˆChatGLM-6Bæ¨¡å‹å­¦ä¹ ä¸å®æˆ˜](ChatGLM-6B/induction.md)
   1. ä»‹ç»ï¼šChatGLM-6B ç¯å¢ƒé…ç½® å’Œ éƒ¨ç½²
2. [ã€ChatGLM-6Bå…¥é—¨-äºŒã€‘æ¸…åå¤§å­¦å¼€æºä¸­æ–‡ç‰ˆChatGLM-6Bæ¨¡å‹å¾®è°ƒå®æˆ˜](ChatGLM-6B/ptuning.md)
   1. ChatGLM-6B P-Tuning V2 å¾®è°ƒï¼šFine-tuning the prefix encoder of the model.
3. [ã€ChatGLM-6Bå…¥é—¨-ä¸‰ã€‘ChatGLM ç‰¹å®šä»»åŠ¡å¾®è°ƒå®æˆ˜](https://articles.zsxq.com/id_3b42ukjdkwpt.html)
4. [ã€ChatGLM-6Bå…¥é—¨-å››ã€‘ChatGLM + LoRA è¿›è¡Œfinetune](https://articles.zsxq.com/id_e2389qm0w0sx.html)
   1. ä»‹ç»ï¼šChatGLM-6B LoRA å¾®è°ƒï¼šFine-tuning the low-rank adapters of the model.
5. [ChatGLM-6B å°ç¼–å¡«å‘è®°](https://articles.zsxq.com/id_fw7vn0mhdsnq.html)
   1. ä»‹ç»ï¼šChatGLM-6B åœ¨ éƒ¨ç½²å’Œå¾®è°ƒ è¿‡ç¨‹ä¸­ ä¼šé‡åˆ°å¾ˆå¤šå‘ï¼Œå°ç¼–æ‰å‘äº†å¾ˆå¤šæ¬¡ï¼Œä¸ºé˜²æ­¢ åäººå’Œå°ç¼–ä¸€æ ·ç»§ç»­æ‰å‘ï¼Œå°ç¼–ç´¢æ€§æŠŠé‡åˆ°çš„å‘éƒ½å¡«äº†ã€‚
6. [ã€LLMså­¦ä¹ ã€‘å…³äºå¤§æ¨¡å‹å®è·µçš„ä¸€äº›æ€»ç»“](https://articles.zsxq.com/id_il58nxrs9jxr.html)
7. [ã€LLMs å…¥é—¨å®æˆ˜ â€”â€” åä¸€ ã€‘åŸºäº ğŸ¤—PEFT çš„é«˜æ•ˆ ğŸ¤–ChatGLM-6B å¾®è°ƒ](https://articles.zsxq.com/id_7rz5jtfguuc5.html)
   1. å¾®è°ƒæ–¹å¼ï¼š
      1. ChatGLM-6B Freeze å¾®è°ƒï¼šFine-tuning the MLPs in the last n blocks of the model.
      2. ChatGLM-6B P-Tuning V2 å¾®è°ƒï¼šFine-tuning the prefix encoder of the model.
      3. ChatGLM-6B LoRA å¾®è°ƒï¼šFine-tuning the low-rank adapters of the model.
8. [ã€LLMs å…¥é—¨å®æˆ˜ â€”â€” åäºŒ ã€‘åŸºäº æœ¬åœ°çŸ¥è¯†åº“ çš„é«˜æ•ˆ ğŸ¤–langchain-ChatGLM ](https://articles.zsxq.com/id_54vjwns5t6in.html)
   1. ä»‹ç»ï¼šlangchain-ChatGLMæ˜¯ä¸€ä¸ªåŸºäºæœ¬åœ°çŸ¥è¯†çš„é—®ç­”æœºå™¨äººï¼Œä½¿ç”¨è€…å¯ä»¥è‡ªç”±é…ç½®æœ¬åœ°çŸ¥è¯†ï¼Œç”¨æˆ·é—®é¢˜çš„ç­”æ¡ˆä¹Ÿæ˜¯åŸºäºæœ¬åœ°çŸ¥è¯†ç”Ÿæˆçš„ã€‚

### ç¬¬äºŒå±‚ Stanford Alpaca 7B 

- [ã€LLMs å…¥é—¨å®æˆ˜ â€”â€” äº” ã€‘Stanford Alpaca 7B æ¨¡å‹å­¦ä¹ ä¸å®æˆ˜](https://articles.zsxq.com/id_xnt3fvp2wxz0.html)
  - ä»‹ç»ï¼šæœ¬æ•™ç¨‹æä¾›äº†å¯¹LLaMAæ¨¡å‹è¿›è¡Œå¾®è°ƒçš„å»‰ä»·äº²æ°‘ LLMs å­¦ä¹ å’Œå¾®è°ƒ æ–¹å¼ï¼Œä¸»è¦ä»‹ç»å¯¹äº Stanford Alpaca 7B æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸Š çš„ å¾®è°ƒå®éªŒï¼Œæ‰€ç”¨çš„æ•°æ®ä¸ºOpenAIæä¾›çš„GPTæ¨¡å‹APIç”Ÿæˆè´¨é‡è¾ƒé«˜çš„æŒ‡ä»¤æ•°æ®ï¼ˆä»…52kï¼‰ã€‚

### ç¬¬ä¸‰å±‚ Chinese-LLaMA-Alpaca 

- [ã€LLMs å…¥é—¨å®æˆ˜ â€”â€” å…­ ã€‘Chinese-LLaMA-Alpaca æ¨¡å‹å­¦ä¹ ä¸å®æˆ˜](https://articles.zsxq.com/id_dqvusswrdg6c.html)
  - ä»‹ç»ï¼šæœ¬æ•™ç¨‹ä¸»è¦ä»‹ç»äº† Chinese-ChatLLaMA,æä¾›ä¸­æ–‡å¯¹è¯æ¨¡å‹ ChatLLama ã€ä¸­æ–‡åŸºç¡€æ¨¡å‹ LLaMA-zh åŠå…¶è®­ç»ƒæ•°æ®ã€‚ æ¨¡å‹åŸºäº TencentPretrain å¤šæ¨¡æ€é¢„è®­ç»ƒæ¡†æ¶æ„å»º

### ç¬¬å››å±‚ å°ç¾Šé©¼ Vicuna

- [ã€LLMs å…¥é—¨å®æˆ˜ â€”â€” ä¸ƒ ã€‘å°ç¾Šé©¼ Vicunaæ¨¡å‹å­¦ä¹ ä¸å®æˆ˜](https://articles.zsxq.com/id_q9mx24q9fdab.html)
  - ä»‹ç»ï¼šUCä¼¯å…‹åˆ©å­¦è€…è”æ‰‹CMUã€æ–¯å¦ç¦ç­‰ï¼Œå†æ¬¡æ¨å‡ºä¸€ä¸ªå…¨æ–°æ¨¡å‹70äº¿/130äº¿å‚æ•°çš„Vicunaï¼Œä¿—ç§°ã€Œå°ç¾Šé©¼ã€ï¼ˆéª†é©¬ï¼‰ã€‚å°ç¾Šé©¼å·ç§°èƒ½è¾¾åˆ°GPT-4çš„90%æ€§èƒ½

### ç¬¬äº”å±‚ MiniGPT-4 

- [ã€LLMs å…¥é—¨å®æˆ˜ â€”â€” å…« ã€‘MiniGPT-4 æ¨¡å‹å­¦ä¹ ä¸å®æˆ˜](https://articles.zsxq.com/id_ff0w6czthq25.html)
  - ä»‹ç»ï¼š MiniGPT-4ï¼Œæ˜¯æ¥è‡ªé˜¿åœæœæ‹‰å›½ç‹ç§‘æŠ€å¤§å­¦çš„å‡ ä½åšå£«åšçš„ï¼Œå®ƒèƒ½æä¾›ç±»ä¼¼ GPT-4 çš„å›¾åƒç†è§£ä¸å¯¹è¯èƒ½åŠ›

### ç¬¬å…­å±‚ GPT4ALL

- [ã€LLMs å…¥é—¨å®æˆ˜ â€”â€” å…« ã€‘GPT4ALL æ¨¡å‹å­¦ä¹ ä¸å®æˆ˜](https://articles.zsxq.com/id_ff0w6czthq25.html)
  - ä»‹ç»ï¼šä¸€ä¸ª å¯ä»¥åœ¨è‡ªå·±ç¬”è®°æœ¬ä¸Šé¢è·‘èµ·æ¥çš„  Nomic AI çš„åŠ©æ‰‹å¼èŠå¤©æœºå™¨äººï¼Œæˆä¸ºè´«æ°‘å®¶å­©å­çš„ ç¦éŸ³ï¼

### ç¬¬ä¸ƒå±‚ AutoGPT

- [AutoGPT ä½¿ç”¨å’Œéƒ¨ç½²](https://articles.zsxq.com/id_pli0z9916126.html)
  - ä»‹ç»ï¼šAuto-GPTæ˜¯ä¸€ä¸ªåŸºäºChatGPTçš„å·¥å…·ï¼Œä»–èƒ½å¸®ä½ è‡ªåŠ¨å®Œæˆå„ç§ä»»åŠ¡ï¼Œæ¯”å¦‚å†™ä»£ç ã€å†™æŠ¥å‘Šã€åšè°ƒç ”ç­‰ç­‰ã€‚ä½¿ç”¨å®ƒæ—¶ï¼Œä½ åªéœ€è¦å‘Šè¯‰ä»–è¦æ‰®æ¼”çš„è§’è‰²å’Œè¦å®ç°çš„ç›®æ ‡ï¼Œç„¶åä»–å°±ä¼šåˆ©ç”¨ChatGPTå’Œè°·æ­Œæœç´¢ç­‰å·¥å…·ï¼Œä¸æ–­â€œæ€è€ƒâ€å¦‚ä½•æ¥è¿‘ç›®æ ‡å¹¶æ‰§è¡Œï¼Œä½ ç”šè‡³å¯ä»¥çœ‹åˆ°ä»–çš„æ€è€ƒè¿‡ç¨‹ã€‚

### ç¬¬å…«å±‚ MOSS

- [ã€LLMs å…¥é—¨å®æˆ˜ â€”â€” åä¸‰ ã€‘MOSS æ¨¡å‹å­¦ä¹ ä¸å®æˆ˜](https://articles.zsxq.com/id_4vwpxod23zrc.html)
  - ä»‹ç»ï¼šMOSSæ˜¯ä¸€ä¸ªæ”¯æŒä¸­è‹±åŒè¯­å’Œå¤šç§æ’ä»¶çš„å¼€æºå¯¹è¯è¯­è¨€æ¨¡å‹ï¼Œmoss-moonç³»åˆ—æ¨¡å‹å…·æœ‰160äº¿å‚æ•°ï¼Œåœ¨FP16ç²¾åº¦ä¸‹å¯åœ¨å•å¼ A100/A800æˆ–ä¸¤å¼ 3090æ˜¾å¡è¿è¡Œï¼Œåœ¨INT4/8ç²¾åº¦ä¸‹å¯åœ¨å•å¼ 3090æ˜¾å¡è¿è¡Œã€‚MOSSåŸºåº§è¯­è¨€æ¨¡å‹åœ¨çº¦ä¸ƒåƒäº¿ä¸­è‹±æ–‡ä»¥åŠä»£ç å•è¯ä¸Šé¢„è®­ç»ƒå¾—åˆ°ï¼Œåç»­ç»è¿‡å¯¹è¯æŒ‡ä»¤å¾®è°ƒã€æ’ä»¶å¢å¼ºå­¦ä¹ å’Œäººç±»åå¥½è®­ç»ƒå…·å¤‡å¤šè½®å¯¹è¯èƒ½åŠ›åŠä½¿ç”¨å¤šç§æ’ä»¶çš„èƒ½åŠ›ã€‚
  - å±€é™æ€§ï¼šç”±äºæ¨¡å‹å‚æ•°é‡è¾ƒå°å’Œè‡ªå›å½’ç”ŸæˆèŒƒå¼ï¼ŒMOSSä»ç„¶å¯èƒ½ç”ŸæˆåŒ…å«äº‹å®æ€§é”™è¯¯çš„è¯¯å¯¼æ€§å›å¤æˆ–åŒ…å«åè§/æ­§è§†çš„æœ‰å®³å†…å®¹ï¼Œè¯·è°¨æ…é‰´åˆ«å’Œä½¿ç”¨MOSSç”Ÿæˆçš„å†…å®¹ï¼Œè¯·å‹¿å°†MOSSç”Ÿæˆçš„æœ‰å®³å†…å®¹ä¼ æ’­è‡³äº’è”ç½‘ã€‚è‹¥äº§ç”Ÿä¸è‰¯åæœï¼Œç”±ä¼ æ’­è€…è‡ªè´Ÿã€‚


## ä¸€ã€å‰è¨€

ç›®å‰ï¼ŒLLMs æ»¡å¤©é£ï¼Œä½†æ˜¯æƒ³è¦ è®­ç»ƒé«˜è´¨é‡çš„æŒ‡ä»¤éµå¾ªæ¨¡å‹(instruction-following model)é¢ä¸´ä¸¤ä¸ªé‡è¦æŒ‘æˆ˜ï¼š

1. å¼ºå¤§çš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼›
2. é«˜è´¨é‡çš„æŒ‡ä»¤éµå¾ªæ•°æ®ï¼›

å¯¹äºç¬¬ä¸€ä¸ªæŒ‘æˆ˜ï¼Œæœ€è¿‘ï¼ŒMetaå¼€æºäº†ä»–ä»¬çš„LLaMAç³»åˆ—æ¨¡å‹ï¼ŒåŒ…å«äº†å‚æ•°é‡ä¸º7B/13B/33B/65Bçš„ä¸åŒæ¨¡å‹ï¼Œä½†æ˜¯Stanford ç§‘å­¦å®¶ä»¬å‘ç° åŸæ¨¡å‹çš„æ•ˆæœè¾ƒå·®ï¼ˆå¦‚ç”Ÿæˆçš„ç»“æœæ–‡ä¸å¯¹é¢˜ã€å¹¶ä¸”æ— æ³•è‡ªç„¶åœ°ç»“æŸç”Ÿæˆç­‰ï¼‰ã€‚

å¯¹äºç¬¬äºŒä¸ªæŒ‘æˆ˜ï¼ŒSelf-Instruct æå‡ºä¸€ç§åˆ©ç”¨ç°æœ‰çš„å¼ºå¤§è¯­è¨€æ¨¡å‹è‡ªåŠ¨ç”ŸæˆæŒ‡ä»¤æ•°æ®ã€‚å› æ­¤ï¼Œæ–¯å¦ç¦çš„ Alpaca æ¨¡å‹åŸºäº LLaMA-7Bæ¨¡å‹ç»“åˆself-instruct æ–¹å¼ç”Ÿæˆçš„52kæŒ‡ä»¤éµå¾ª(instruction-following)æ ·æœ¬æ•°æ®è¿›è¡Œæœ‰ç›‘ç£çš„æŒ‡ä»¤å¾®è°ƒï¼Œå°±èƒ½è¾¾åˆ°ç±»ä¼¼ GPT-3.5 çš„æ•ˆæœã€‚

æœ¬æ•™ç¨‹æä¾›äº†å¯¹LLaMAæ¨¡å‹è¿›è¡Œå¾®è°ƒçš„å»‰ä»·äº²æ°‘ LLMs å­¦ä¹ å’Œå¾®è°ƒ æ–¹å¼ï¼Œä¸»è¦ä»‹ç»å¯¹äº Stanford Alpaca 7B æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸Š çš„ å¾®è°ƒå®éªŒï¼Œæ‰€ç”¨çš„æ•°æ®ä¸ºOpenAIæä¾›çš„GPTæ¨¡å‹APIç”Ÿæˆè´¨é‡è¾ƒé«˜çš„æŒ‡ä»¤æ•°æ®ï¼ˆä»…52kï¼‰ã€‚

## äºŒã€æ•´ä½“æ–¹æ³•ä»‹ç»

1. åˆ©ç”¨OpenAIæä¾›çš„GPTæ¨¡å‹(text-davinci-003)APIç»“åˆself-instructæ–¹æ³•ç”Ÿæˆè´¨é‡è¾ƒé«˜çš„æŒ‡ä»¤æ•°æ®ï¼ˆä»…52kï¼‰ï¼Œä¾‹å¦‚ï¼š

```s
{
    "instruction": "Rewrite the following sentence in the third person",
    "input": "I am anxious",
    "output": "She is anxious."
}, {
    "instruction": "What are the three primary colors?",
    "input": "",
    "output": "The three primary colors are red, blue, and yellow."
}
```

å…·ä½“æ“ä½œæ–¹æ³•ï¼šä½¿ç”¨ self-instruct ç§å­é›†ä¸­çš„ 175 ä¸ªäººå·¥ç¼–å†™çš„æŒ‡ä»¤-è¾“å‡º(instruction-output)å¯¹ï¼Œç„¶åç”¨è¯¥ç§å­é›†ä½œä¸º in-context æ ·æœ¬ prompt text-davinci-003æ¨¡å‹æ¥ç”Ÿæˆæ›´å¤šæŒ‡ä»¤ã€‚Alpacaé€šè¿‡ç®€åŒ–ç”Ÿæˆ pipeline æ”¹è¿›äº† self-instruct æ–¹æ³•ï¼Œå¹¶æ˜¾è‘—é™ä½äº†æˆæœ¬ã€‚

> Alpacaå®˜æ–¹å£°ç§°åŸºäºopenaiçš„APIç”Ÿæˆ52kæŒ‡ä»¤æ•°æ®é›†çš„è´¹ç”¨<500ç¾å…ƒã€‚å…³äºself-instruct æ–¹æ³•çš„ç»†èŠ‚ç•™å¾…åç»­åœ¨æœ¬ç³»åˆ—ä¸­è¯¦ç»†è¯´æ˜ï¼Œæ•¬è¯·æœŸå¾…ç•™æ„ã€‚

2. åŸºäºè¿™äº›æŒ‡ä»¤æ•°æ®ä½¿ç”¨HuggingFace Transformersæ¡†æ¶ç²¾è°ƒLLaMA-7Bæ¨¡å‹ã€‚

![](img/å¾®ä¿¡æˆªå›¾_20230414124431.png)

> æ³¨ï¼šåœ¨è¿™ä¸ªè¿‡ç¨‹åˆ©ç”¨äº†FSDP(Fully Sharded Data Parallel)å’Œæ··åˆç²¾åº¦è®­ç»ƒç­‰æŠ€æœ¯ã€‚æˆæœ¬æ–¹é¢ï¼ŒAlpacaåœ¨8ä¸ª80GB A100 ä¸Šå¾®è°ƒä¸€ä¸ª 7B LLaMA æ¨¡å‹éœ€è¦3ä¸ªå°æ—¶ï¼Œè¿™å¯¹å¤§å¤šæ•°äº‘è®¡ç®—æä¾›å•†æ¥è¯´æˆæœ¬ä¸åˆ° 100 ç¾å…ƒã€‚æ•´ä½“ä»·æ ¼è¿˜ç®—æ¯”è¾ƒäº²æ°‘ï¼Œå¯ç›å¯ç”œã€‚

## ä¸‰ã€ç¯å¢ƒæ­å»º

### 3.1 åŸºç¡€ç¯å¢ƒé…ç½®è¦æ±‚

1. æ“ä½œç³»ç»Ÿï¼šLinux
2. CPUs: å•ä¸ªèŠ‚ç‚¹å…·æœ‰ 1TB å†…å­˜çš„ Intel CPUï¼Œç‰©ç†CPUä¸ªæ•°ä¸º64ï¼Œæ¯é¢—CPUæ ¸æ•°ä¸º16
3. GPUs: 8 å¡ A800 80GB GPUs
4. Python: 3.10 

### 3.2 æ„å»ºç¯å¢ƒ

```s
    $ conda create -n py310_chat python=3.10       # åˆ›å»ºæ–°ç¯å¢ƒ
    $ source activate py310_chat                   # æ¿€æ´»ç¯å¢ƒ
```

### 3.3 ç¦»çº¿å®‰è£… pytorch

ç¦»çº¿å®‰è£…PyTorchï¼Œç‚¹å‡»ä¸‹è½½å¯¹åº” [cudaç‰ˆæœ¬](https://download.pytorch.org/whl/torch_stable.html)çš„torchå’Œtorchvisionå³å¯ã€‚

```s
    $ pip  install -U torch==1.13.1
    $ pip install torchvision-0.14.1
```

### 3.4 å®‰è£… transformers

å®‰è£… transformers ï¼Œç›®å‰ï¼Œ[LLaMAç›¸å…³çš„å®ç°](https://github.com/huggingface/transformers/commit/0041be5b3d1b9a5e1443e1825d7d80f6dfadcdaa)å¹¶æ²¡æœ‰å‘å¸ƒå¯¹åº”çš„ç‰ˆæœ¬ï¼Œä½†æ˜¯å·²ç»åˆå¹¶åˆ°ä¸»åˆ†æ”¯äº†ï¼Œå› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦åˆ‡æ¢åˆ°å¯¹åº”çš„commitï¼Œä»æºä»£ç è¿›è¡Œç›¸åº”çš„å®‰è£…ã€‚

```s
    $ https://github.com/huggingface/transformers.git
    $ cd transformers
    $ git checkout 0041be5 
    $ pip3 install . -i https://mirrors.cloud.tencent.com/pypi/simple
```

### 3.5 å®‰è£… apex

```s
    $ git clone https://github.com/NVIDIA/apex.git
    $ cd apex
    $ git checkout 22.04-dev
    $ pip install -v --disable-pip-version-check --no-cache-dir --global-option="--cpp_ext" --global-option="--cuda_ext" ./
```

### 3.6 å®‰è£…å…¶ä»–ä¾èµ–åŒ…

ä½¿ç”¨ ä»¥ä¸‹å‘½ä»¤ å®‰è£…å…¶ä»–ä¾èµ–åŒ…

```s
    $ pip install -r requirements.txt
```

requirements.txtæ–‡ä»¶å…·ä½“çš„å†…å®¹å¦‚ä¸‹ã€‚

```s
    numpy
    rouge_score
    fire
    openai
    sentencepiece
    tokenizers==0.12.1
    wandb
    deepspeed==0.8.0
    accelerate
    tensorboardX
```

> æ³¨ï¼š**å¦‚æœåˆ°è¿™é‡Œä¸€è·¯ç•…é€šï¼Œé‚£å°±è¯´æ˜ ä½ å·²ç» å®Œæˆä¸€å¤§æ­¥å·¥ä½œäº†ï¼**

## å››ã€æ¨¡å‹æ ¼å¼è½¬æ¢

### 4.1 LLaMAåŸå§‹æƒé‡æ–‡ä»¶ ä¸‹è½½

æ ¹æ®è‡ªå·±çš„éœ€æ±‚ä¸‹è½½æ³„éœ²çš„LLaMAçš„7Bã€13Bç­‰ç­‰çš„æ¨¡å‹æƒé‡ï¼Œéœ€è¦å¡«formæ‰¾facebookç”³è¯·ï¼Œä½†å¾ˆéš¾å¾—åˆ°å›å¤ï¼Œæ¼”ç¤ºä½¿ç”¨çš„æ˜¯æœ€å°çš„7Bç‰ˆï¼Œ

1. ä¸‹è½½æ–¹å¼ä¸€ï¼š ä½¿ç”¨ aria2c ä¸‹è½½

```s
    For the 7B model...
    aria2c --select-file 21-23,25,26 'magnet:?xt=urn:btih:b8287ebfa04f879b048d4d4404108cf3e8014352&dn=LLaMA'
    https://huggingface.co/nyanko7/LLaMA-7B/tree/main

    For the 13B model...

    aria2c --select-file 1-4,25,26 'magnet:?xt=urn:btih:b8287ebfa04f879b048d4d4404108cf3e8014352&dn=LLaMA'
    For the 30B model...

    aria2c --select-file 5-10,25,26 'magnet:?xt=urn:btih:b8287ebfa04f879b048d4d4404108cf3e8014352&dn=LLaMA'
    For the 65B model...

    aria2c --select-file 11-20,25,26 'magnet:?xt=urn:btih:b8287ebfa04f879b048d4d4404108cf3e8014352&dn=LLaMA'

    And for everything...

    aria2c 'magnet:?xt=urn:btih:b8287ebfa04f879b048d4d4404108cf3e8014352&dn=LLaMA' 
```


2. ä¸‹è½½æ–¹å¼äºŒï¼šç™¾åº¦äº‘ç›˜ä¸‹è½½ï¼Œ[ç™¾åº¦ç½‘ç›˜çš„ä¸‹è½½é“¾æ¥](https://pan.baidu.com/s/19g792GUMtELGBMCfu0d2OQ?pwd=x2ck)

3. ä¸‹è½½æ–¹å¼ä¸‰ï¼šé€šè¿‡pyllamaä¸‹è½½

- step 1ï¼šå®‰è£…pyllama

```s
    $ pip install pyllama -U
```

- step 2ï¼šä¸‹è½½7Bçš„æ¨¡å‹

```s
    $ python -m llama.download --model_size 7B
```

- step 3ï¼šå½“ç„¶ä½ ä¹Ÿå¯ä»¥ä¸‹è½½æ›´å¤§çš„æ¨¡å‹ï¼Œæœ‰7B,13B,30B,65Bå…±è®¡4ç§ã€‚

4. ä¸‹è½½æ–¹å¼å››ï¼šé€šè¿‡ipfsä¸‹è½½

è¿™ä¸ªåº”è¯¥æ˜¯æœ€æ—©æ³„æ¼çš„LLaMAæ¨¡å‹ï¼Œåœ°å€ä¸º https://ipfs.io/ipfs/Qmb9y5GCkTG7ZzbBWMu2BXwMkzyCKcUjtEKPpgdZ7GEFKm/

- step 1ï¼šé¦–å…ˆå®‰è£…ipfså®¢æˆ·ç«¯ï¼Œæœ€å¥½ç”¨å¸¦ç•Œé¢çš„ã€‚https://docs.ipfs.tech/install/ipfs-desktop/
- step 2ï¼šç„¶å7Bæ¨¡å‹çš„indexä¸ºï¼šQmbvdJ7KgvZiyaqHw5QtQxRtUd7pCAdkWWbzuvyKusLGTw

### 4.2 LLaMAåŸå§‹æƒé‡æ–‡ä»¶æ ¼å¼è½¬æ¢

éœ€è¦å°†LLaMAåŸå§‹æƒé‡æ–‡ä»¶è½¬æ¢ä¸ºTransformersåº“å¯¹åº”çš„æ¨¡å‹æ–‡ä»¶æ ¼å¼ã€‚å¦‚æœä¸æƒ³è‡ªå·±è½¬ï¼Œä¹Ÿå¯ä»¥ç›´æ¥ä»Hugging Faceä¸‹è½½è½¬æ¢å¥½çš„æ¨¡å‹

```s
   $ cd transformers

    $ python src/transformers/models/llama/convert_llama_weights_to_hf.py \ 
    --input_dir data/llama-model \
    --model_size 7B \
    --output_dir data/hf-llama-model
```

è¿™ä¸ªç‰ˆæœ¬transformersè½¬æ¢å¾—åˆ°çš„ç»“æœæ˜¯åˆ†åˆ«å­˜äº2ä¸ªæ–‡ä»¶å¤¹ï¼šllama-7bå’Œtokenizer

å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨ï¼š

```s
    $ tokenizer = transformers.LlamaTokenizer.from_pretrained("data/llama-model/tokenizer/")

    $ model = transformers.LlamaForCausalLM.from_pretrained("data/llama-model/llama-7b/")
```

LLaMA åˆ†è¯å™¨ï¼ˆtokenizerï¼‰åŸºäº [sentencepieceåˆ†è¯å·¥å…·](https://github.com/google/sentencepiece)ã€‚ sentencepieceåœ¨è§£ç åºåˆ—æ—¶ï¼Œå¦‚æœç¬¬ä¸€ä¸ªtokenæ˜¯å•è¯ï¼ˆä¾‹å¦‚ï¼šBananaï¼‰å¼€å¤´ï¼Œåˆ™tokenizerä¸ä¼šåœ¨å­—ç¬¦ä¸²å‰æ·»åŠ å‰ç¼€ç©ºæ ¼ã€‚ è¦è®©tokenizerè¾“å‡ºå‰ç¼€ç©ºæ ¼ï¼Œè¯·åœ¨LlamaTokenizerå¯¹è±¡æˆ–tokenizeré…ç½®ä¸­è®¾ç½®decode_with_prefix_space=Trueã€‚

ä¸ºäº†æ–¹ä¾¿å°†tokenizerç›®å½•çš„æ–‡ä»¶æ‹·è´åˆ°llama-7bç›®å½•ä¸‹ã€‚å¦‚æœæ˜¯ç›´æ¥ç”¨æœ€æ–°ç‰ˆçš„transformersä¸­è½¬æ¢è„šæœ¬çš„è¯åœ¨hf-llama-modelä¼šå°†æ¨¡å‹å‚æ•°æ–‡ä»¶å’Œtokenizerç›¸å…³æ–‡ä»¶å¹³é“ºæ”¾ä¸€èµ·ã€‚

```s
    $ cp tokenizer/* llama-7b/
```

> æ³¨ï¼šæ³¨: å¦‚æœä¸æƒ³è½¬æ¢ä¹Ÿå¯ä»¥ç›´æ¥ä»Hugging Faceä¸‹è½½è½¬æ¢å¥½çš„ [æ¨¡å‹](https://huggingface.co/decapoda-research/llama-7b-hf)ã€‚

## äº”ã€æ•°æ®å‡†å¤‡

 [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca) ä¸­çš„alpaca_data.jsonæ–‡ä»¶å³æ˜¯ä»–ä»¬ç”¨äºè®­ç»ƒçš„æŒ‡ä»¤æ•°æ®é›†ï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œæ¨¡å‹ç²¾è°ƒã€‚ä½†æ˜¯åœ¨ [Alpaca-LoRA](https://github.com/tloen/alpaca-lora) ä¸­æåˆ°è¯¥æ•°æ®é›†å­˜åœ¨ä¸€äº›å™ªå£°ï¼Œå› æ­¤ï¼Œä»–ä»¬å¯¹è¯¥æ•°æ®é›†åšäº†æ¸…æ´—åå¾—åˆ°äº† [alpaca_data_cleaned_archive.json](https://github.com/tloen/alpaca-lora/blob/main/alpaca_data_cleaned_archive.json) æ–‡ä»¶ã€‚é‡‡ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè®­ç»ƒå¤§æ¦‚ç‡ä¼šå¾—åˆ°æ›´å¥½ç»“æœã€‚

## å…­ã€æ¨¡å‹ç²¾è°ƒ

è¿è¡Œå‘½ä»¤

```s
    torchrun --nproc_per_node=8 --master_port=25001 train.py \
        --model_name_or_path  pretrain/hf-llama-model/llama-7b \
        --data_path data/alpaca_data_cleaned.json \
        --bf16 True \
        --output_dir output/alpaca/sft_7b \
        --num_train_epochs 1 \
        --per_device_train_batch_size 4 \
        --per_device_eval_batch_size 4 \
        --gradient_accumulation_steps 8 \
        --evaluation_strategy "no" \
        --save_strategy "steps" \
        --save_steps 2000 \
        --save_total_limit 1 \
        --learning_rate 2e-5 \
        --weight_decay 0. \
        --warmup_ratio 0.03 \
        --lr_scheduler_type "cosine" \
        --logging_steps 1 \
        --report_to "tensorboard" \
        --fsdp "full_shard auto_wrap" \
        --fsdp_transformer_layer_cls_to_wrap 'LlamaDecoderLayer' \
        --tf32 True
```

> å¾®è°ƒä¼˜åŒ– å¯ä»¥çœ‹ è¸©å‘æ‰‹å†Œ->ï¼ˆ2ï¼‰æ˜¾å­˜å ç”¨é«˜å’Œè®­ç»ƒæ•ˆç‡æ…¢ é—®é¢˜ ç« èŠ‚

> å¾®è°ƒè¿‡ç¨‹

```s
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.
*****************************************
[2023-03-28 11:13:02,320] [INFO] [comm.py:657:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-03-28 11:13:20,236] [INFO] [partition_parameters.py:413:__exit__] finished initializing model with 6.74B parameters
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:41<00:00,  1.26s/it]
...
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:41<00:00,  1.26s/it]
Using pad_token, but it is not set yet.
...
Using pad_token, but it is not set yet.
WARNING:root:Loading data...
...
WARNING:root:Loading data...
WARNING:root:Formatting inputs...
...
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
..
WARNING:root:Tokenizing inputs... This may take some time...
Using /base/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
...
Using /base/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Emitting ninja build file /base/.cache/torch_extensions/py310_cu113/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Using /base/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
ninja: no work to do.
Loading extension module utils...
...
Loading extension module utils...
Time to load utils op: 0.10286140441894531 seconds
...
Time to load utils op: 0.20401406288146973 seconds
Parameter Offload: Total persistent parameters: 0 in 0 params
Using /base/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Using /base/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...Loading extension module utils...

Time to load utils op: 0.0004200935363769531 seconds
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Using /base/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Time to load utils op: 0.0003352165222167969 seconds
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0003571510314941406 seconds
Using /base/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Using /base/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...Using /base/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...

No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0006623268127441406 seconds
Time to load utils op: 0.0005290508270263672 seconds
Time to load utils op: 0.0006077289581298828 seconds
Using /base/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.001024484634399414 seconds
Using /base/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0003275871276855469 seconds
{'loss': 1.5163, 'learning_rate': 0.0, 'epoch': 0.01}
{'loss': 1.5216, 'learning_rate': 0.0, 'epoch': 0.02}
...
{'loss': 1.0547, 'learning_rate': 2.025571894372794e-06, 'epoch': 0.98}
{'loss': 1.0329, 'learning_rate': 1.8343633694278895e-06, 'epoch': 0.99}
{'loss': 1.0613, 'learning_rate': 1.6517194697072903e-06, 'epoch': 1.0}
{'train_runtime': 4605.8781, 'train_samples_per_second': 11.277, 'train_steps_per_second': 0.022, 'train_loss': 1.175760779050317, 'epoch': 1.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 101/101 [1:16:45<00:00, 45.60s/it]
...
```

> GPUæ˜¾å­˜å ç”¨æƒ…å†µ

```s
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.7     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A800 80G...  Off  | 00000000:34:00.0 Off |                    0 |
| N/A   47C    P0    75W / 300W |  66615MiB / 80994MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA A800 80G...  Off  | 00000000:35:00.0 Off |                    0 |
| N/A   46C    P0    70W / 300W |  31675MiB / 80994MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   2  NVIDIA A800 80G...  Off  | 00000000:36:00.0 Off |                    0 |
| N/A   49C    P0    72W / 300W |  35529MiB / 80994MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   3  NVIDIA A800 80G...  Off  | 00000000:37:00.0 Off |                    0 |
| N/A   50C    P0    76W / 300W |  54277MiB / 80994MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   4  NVIDIA A800 80G...  Off  | 00000000:9B:00.0 Off |                    0 |
| N/A   51C    P0    80W / 300W |  44229MiB / 80994MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   5  NVIDIA A800 80G...  Off  | 00000000:9C:00.0 Off |                    0 |
| N/A   49C    P0    72W / 300W |  59841MiB / 80994MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   6  NVIDIA A800 80G...  Off  | 00000000:9D:00.0 Off |                    0 |
| N/A   47C    P0    77W / 300W |  65217MiB / 80994MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   7  NVIDIA A800 80G...  Off  | 00000000:9E:00.0 Off |                    0 |
| N/A   43C    P0    68W / 300W |  30141MiB / 80994MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A     30523      C   ...py310/bin/python    63693MiB |
|    1   N/A  N/A     30235      C   ...py310/bin/python    31653MiB |
|    2   N/A  N/A     30136      C   ...py310/bin/python    35507MiB |
|    3   N/A  N/A     31537      C   ...py310/bin/python    53655MiB |
|    4   N/A  N/A     30540      C   ...py310/bin/python    44207MiB |
|    5   N/A  N/A     30542      C   ...py310/bin/python    59619MiB |
|    6   N/A  N/A     30522      C   ...py310/bin/python    65195MiB |
|    7   N/A  N/A     30743      C   ...py310/bin/python    30119MiB |
+-----------------------------------------------------------------------------+
```


## ä¸ƒã€æ¨¡å‹æ¨ç†

> å®ä¾‹ä¸€
```s
### Instruction:ä½ æ˜¯ä¸€åå°å­¦ç”Ÿï¼Œè°ˆè°ˆä½ çš„ç†æƒ³ã€‚
### Response:
æˆ‘çš„ç†æƒ³æ˜¯æƒ³æˆä¸ºä¸€åç§‘å­¦å®¶ã€‚ä»äº‹æœ‰å…³ç§‘å­¦ç ”ç©¶ï¼ŒåŠªåŠ›æ¨åŠ¨ç§‘å­¦ç ”ç©¶å‘å±•ï¼ŒåŒæ—¶è¿˜èƒ½ç»™äººä»¬å¸¦æ¥å®è´¨æ€§çš„æ”¹å˜å’Œå¸®åŠ©ã€‚æˆ‘å¸Œæœ›èƒ½å¤Ÿåœ¨æœ‰å…³çš„ç ”ç©¶é¢†åŸŸå–å¾—æ°å‡ºçš„æˆå°±ï¼Œåšå‡ºå¯¹äººç±»ç¤¾ä¼šçš„è´¡çŒ®ã€‚
```

> å®ä¾‹äºŒ
```s
### Instruction:ä½ æ˜¯ä¸€åå¤§å­¦ç”Ÿï¼Œè¯´ä¸€ä¸‹ä½ çš„ç†æƒ³ã€‚
### Response:
æˆ‘çš„ç†æƒ³æ˜¯æˆä¸ºä¸€åä¼˜ç§€çš„æ•™å¸ˆï¼Œèƒ½ç»™å­¦ç”Ÿä»¬æä¾›æ›´å¤šçš„æœºä¼šï¼Œå¸®åŠ©ä»–ä»¬æˆé•¿å’Œå‘å±•ï¼Œåšåˆ°ä»–ä»¬çš„çˆ±å¥½å’Œæ¢¦æƒ³ã€‚æˆ‘å¸Œæœ›èƒ½å¤Ÿè®©å­¦ç”Ÿä»¬å……åˆ†ç†è§£è¯¾ç¨‹çš„å†…å®¹ï¼Œæ·±å…¥æ¢ç´¢ï¼Œå……åˆ†å‘æŒ¥è‡ªå·±çš„æ½œèƒ½ï¼Œæœ€ç»ˆå–å¾—æˆåŠŸã€‚æˆ‘è¿˜å¸Œæœ›èƒ½å¤Ÿä¸å®¶é•¿å’Œå…¶ä»–ç›‘ç£äººæœ‰æ›´å¥½çš„æ²Ÿé€šï¼Œè®©å­¦ç”Ÿä»¬åœ¨å­¦ä¹ å’Œç”Ÿæ´»ä¸­æ‹¥æœ‰æ›´å¤šçš„ healthy competitionï¼Œæœ€ç»ˆè¾¾åˆ°å­¦ä¹ çš„ç›®çš„ã€‚
```

## å…«ã€æ€»ç»“

ç”±äº LLaMA åœ¨é¢„è®­ç»ƒé˜¶æ®µæ‰€ç”¨çš„è¯­æ–™åŸºæœ¬éƒ½æ˜¯è‹±æ–‡è¯­æ–™ï¼Œæ‰€ä»¥ LLaMA åœ¨è‹±æ–‡ä¸Šå…·æœ‰å¼ºå¤§çš„é›¶æ ·æœ¬å­¦ä¹ å’Œè¿ç§»èƒ½åŠ›ï¼Œä½†æ˜¯åœ¨ ä¸­æ–‡èƒ½åŠ›ä¸Š ç›¸æ¯”äº æ¸…åå¤§å­¦ çš„ CHatGLM åå¾ˆå¼±ã€‚

## è¸©å‘æ‰‹å†Œ

### ï¼ˆ1ï¼‰ Cuda ç‰ˆæœ¬å¤ªä½é—®é¢˜

Stanford Alpaca 7B éœ€è¦çš„ Cudaç‰ˆæœ¬ ä¸º 11.6åŠä»¥ä¸Šï¼Œä¸”PyTorchç‰ˆæœ¬å‡çº§ä¸º1.13.1åŠä»¥ä¸Š

### ï¼ˆ2ï¼‰æ˜¾å­˜å ç”¨é«˜å’Œè®­ç»ƒæ•ˆç‡æ…¢ é—®é¢˜

- åŠ¨æœºï¼šå› ä¸º Stanford Alpaca 7B å¾®è°ƒ æ˜¾å­˜å ç”¨é«˜å’Œè®­ç»ƒæ•ˆç‡æ…¢
- è§£å†³æ–¹æ³•ï¼šä½¿ç”¨DeepSpeedæ¡†æ¶æ¥å‡å°‘æ˜¾å­˜å ç”¨å’Œæé«˜è®­ç»ƒæ•ˆç‡
- å…·ä½“æ“ä½œ

1. clone stanford_alpaca é¡¹ç›®

```s
git clone https://github.com/tatsu-lab/stanford_alpaca.git
cd stanford_alpaca
```

2. ä¿®æ”¹train.pyæ–‡ä»¶

```s
    # æ³¨é‡Šæ‰åŸæœ‰ä»£ç 
    """
    model = transformers.AutoModelForCausalLM.from_pretrained(
        model_args.model_name_or_path,
        cache_dir=training_args.cache_dir,
    )

    tokenizer = transformers.AutoTokenizer.from_pretrained(
        model_args.model_name_or_path,
        cache_dir=training_args.cache_dir,
        model_max_length=training_args.model_max_length,
        padding_side="right",
        use_fast=False,
    )
    """
    # é€šè¿‡LlamaåŠ è½½tokenizerå’Œmodel
    model = transformers.LlamaForCausalLM.from_pretrained(
        model_args.model_name_or_path,
        cache_dir=training_args.cache_dir,
    )

    tokenizer = transformers.LlamaTokenizer.from_pretrained(
        model_args.model_name_or_path,
        cache_dir=training_args.cache_dir,
    )
    
    trainer.save_state()
    # safe_save_model_for_hf_trainer(trainer=trainer, output_dir=training_args.output_dir)
    trainer.save_model()
```

3. å¯åŠ¨å‘½ä»¤

```s
    torchrun --nproc_per_node=8 --master_port=11223 train.py \
    --model_name_or_path pretrain/hf-llama-model/llama-7b \
    --data_path data/alpaca_data_cleaned.json \
    --output_dir output/alpaca/sft_7b \
    --num_train_epochs 1 \
    --per_device_train_batch_size 4 \
    --per_device_eval_batch_size 1 \
    --gradient_accumulation_steps 4 \
    --evaluation_strategy "no" \
    --save_strategy "steps" \
    --save_steps 1000 \
    --save_total_limit 1 \
    --learning_rate 2e-5 \
    --weight_decay 0. \
    --warmup_ratio 0.03 \
    --lr_scheduler_type "cosine" \
    --logging_steps 1 \
    --report_to "tensorboard" \
    --gradient_checkpointing True \
    --fp16 True \
    --deepspeed ds_config.json
```

> å…¶ä¸­ï¼Œds_config.jsonæ–‡ä»¶å†…å®¹å¦‚ä¸‹æ‰€ç¤º

```s
{
    "zero_optimization": {
        "stage": 3,
        "contiguous_gradients": true,
        "stage3_max_live_parameters": 0,
        "stage3_max_reuse_distance": 0,
        "stage3_prefetch_bucket_size": 0,
        "stage3_param_persistence_threshold": 1e2,
        "reduce_bucket_size": 1e2,
        "sub_group_size": 1e8,
        "offload_optimizer": {
            "device": "cpu",
            "pin_memory": true
        },
        "offload_param": {
            "device": "cpu",
            "pin_memory": true
        },
        "stage3_gather_16bit_weights_on_model_save": true
    },
    "fp16": {
        "enabled": true,
        "auto_cast": false,
        "loss_scale": 0,
        "initial_scale_power": 32,
        "loss_scale_window": 1000,
        "hysteresis": 2,
        "min_loss_scale": 1
    },
    "train_batch_size": "auto",
    "train_micro_batch_size_per_gpu": "auto",
    "wall_clock_breakdown": false
}
```

### ï¼ˆ3ï¼‰è¿è¡Œå®Œæœ€åä¸€ä¸ªepochåå‡ºç°OOM

- é—®é¢˜æè¿°ï¼šè¿è¡Œå®Œæœ€åä¸€ä¸ªepochåå‡ºç°OOM

```s
    {'train_runtime': 5162.7837, 'train_samples_per_second': 10.072, 'train_steps_per_second': 0.157, 'train_loss': 1.0267484738615347, 'epoch': 1.0}
    100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 812/812 [1:26:02<00:00,  6.36s/it]

    /opt/python3.10.11/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: 
    UserWarning: Failed to clone() tensor with name _fsdp_wrapped_module._fpw_module.model.layers.28.mlp.down_proj.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from 
    state_dict() call if this parameter is managed by FSDP. 
    Please check clone implementation of _fsdp_wrapped_module._fpw_module.model.layers.28.mlp.down_proj.weight. 
    Error: CUDA out of memory. Tried to allocate 172.00 MiB (GPU 3; 39.59 GiB total capacity; 35.81 GiB already allocated; 
    79.19 MiB free; 37.59 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to 
    avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
```

- è§£å†³æ–¹æ³•ï¼šå‚è€ƒå®˜æ–¹Repoä¸Šissueï¼Œå°†safe_save_model_for_hf_traineræ”¹ä¸ºå¦‚ä¸‹ï¼š

```s
def safe_save_model_for_hf_trainer(trainer: transformers.Trainer, output_dir: str):
    """Collects the state dict and dump to disk."""
    # state_dict = trainer.model.state_dict()
    from torch.distributed.fsdp import (
        FullyShardedDataParallel as FSDP,
        MixedPrecision,
        BackwardPrefetch,
        ShardingStrategy,
        FullStateDictConfig,
        StateDictType,
    )
    model=trainer.model  
    save_policy = FullStateDictConfig(offload_to_cpu=True, rank0_only=True)
    with FSDP.state_dict_type(model, StateDictType.FULL_STATE_DICT, save_policy):
        cpu_state_dict = model.state_dict()
    if trainer.args.should_save:
        trainer._save(output_dir, state_dict=cpu_state_dict)  # noqa
```

## å‚è€ƒ

1. [facebookresearch/llama](https://github.com/facebookresearch/llama)
2. [stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca)
3. [alpaca-lora](https://github.com/tloen/alpaca-lora)
4. [Alpaca: A Strong, Replicable Instruction-Following Model](https://crfm.stanford.edu/2023/03/13/alpaca.html)
5.  [llama-7b-hf/tree/main](https://huggingface.co/decapoda-research/llama-7b-hf/tree/main)
6. [ä»0åˆ°1å¤ç°æ–¯å¦ç¦ç¾Šé©¼ï¼ˆStanford Alpaca 7Bï¼‰](https://zhuanlan.zhihu.com/p/618321077)
7. [LLMç³»åˆ— | 00ï¼šæ–¯å¦ç¦ Alpaca æ¨¡å‹ä»‹ç»åŠå…¶å¤ç°](https://mp.weixin.qq.com/s/JgnyifW5ZKeK_sW8ig-wXw)
8. [ChatGPTå¹³æ›¿æ¨¡å‹ï¼šLLaMAï¼ˆé™„ä¸‹è½½åœ°å€ï¼Œå¹³æ°‘ç©å®¶å’Œä¼¸æ‰‹å…šçš„ç¦éŸ³ï¼ï¼‰](https://zhuanlan.zhihu.com/p/614118339)
9. [æ— éœ€é«˜æ€§èƒ½GPUï¼Œåœ¨MacBookï¼ˆæˆ–linuxï¼‰ä¸Šè¿è¡Œå¯¹æ ‡GPT3çš„LLaMAæ¨¡å‹æ•™ç¨‹](https://www.bilibili.com/read/cv22383652?from=articleDetail)
