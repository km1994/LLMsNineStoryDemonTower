# ã€å…³äº ChatGLM + LoRA è¿›è¡Œfinetune ã€‘é‚£äº›ä½ ä¸çŸ¥é“çš„äº‹

## ã€LLMs å…¥é—¨å®æˆ˜ç³»åˆ—ã€‘

### ç¬¬ä¸€å±‚ ChatGLM-6B

1. [ã€ChatGLM-6Bå…¥é—¨-ä¸€ã€‘æ¸…åå¤§å­¦å¼€æºä¸­æ–‡ç‰ˆChatGLM-6Bæ¨¡å‹å­¦ä¹ ä¸å®æˆ˜](ChatGLM-6B/induction.md)
   1. ä»‹ç»ï¼šChatGLM-6B ç¯å¢ƒé…ç½® å’Œ éƒ¨ç½²
2. [ã€ChatGLM-6Bå…¥é—¨-äºŒã€‘æ¸…åå¤§å­¦å¼€æºä¸­æ–‡ç‰ˆChatGLM-6Bæ¨¡å‹å¾®è°ƒå®æˆ˜](ChatGLM-6B/ptuning.md)
   1. ChatGLM-6B P-Tuning V2 å¾®è°ƒï¼šFine-tuning the prefix encoder of the model.
3. [ã€ChatGLM-6Bå…¥é—¨-ä¸‰ã€‘ChatGLM ç‰¹å®šä»»åŠ¡å¾®è°ƒå®æˆ˜](https://articles.zsxq.com/id_3b42ukjdkwpt.html)
4. [ã€ChatGLM-6Bå…¥é—¨-å››ã€‘ChatGLM + LoRA è¿›è¡Œfinetune](https://articles.zsxq.com/id_e2389qm0w0sx.html)
   1. ä»‹ç»ï¼šChatGLM-6B LoRA å¾®è°ƒï¼šFine-tuning the low-rank adapters of the model.
5. [ChatGLM-6B å°ç¼–å¡«å‘è®°](https://articles.zsxq.com/id_fw7vn0mhdsnq.html)
   1. ä»‹ç»ï¼šChatGLM-6B åœ¨ éƒ¨ç½²å’Œå¾®è°ƒ è¿‡ç¨‹ä¸­ ä¼šé‡åˆ°å¾ˆå¤šå‘ï¼Œå°ç¼–æ‰å‘äº†å¾ˆå¤šæ¬¡ï¼Œä¸ºé˜²æ­¢ åäººå’Œå°ç¼–ä¸€æ ·ç»§ç»­æ‰å‘ï¼Œå°ç¼–ç´¢æ€§æŠŠé‡åˆ°çš„å‘éƒ½å¡«äº†ã€‚
6. [ã€LLMså­¦ä¹ ã€‘å…³äºå¤§æ¨¡å‹å®è·µçš„ä¸€äº›æ€»ç»“](https://articles.zsxq.com/id_il58nxrs9jxr.html)
7. [ã€LLMs å…¥é—¨å®æˆ˜ â€”â€” åä¸€ ã€‘åŸºäº ğŸ¤—PEFT çš„é«˜æ•ˆ ğŸ¤–ChatGLM-6B å¾®è°ƒ](https://articles.zsxq.com/id_7rz5jtfguuc5.html)
   1. å¾®è°ƒæ–¹å¼ï¼š
      1. ChatGLM-6B Freeze å¾®è°ƒï¼šFine-tuning the MLPs in the last n blocks of the model.
      2. ChatGLM-6B P-Tuning V2 å¾®è°ƒï¼šFine-tuning the prefix encoder of the model.
      3. ChatGLM-6B LoRA å¾®è°ƒï¼šFine-tuning the low-rank adapters of the model.
8. [ã€LLMs å…¥é—¨å®æˆ˜ â€”â€” åäºŒ ã€‘åŸºäº æœ¬åœ°çŸ¥è¯†åº“ çš„é«˜æ•ˆ ğŸ¤–langchain-ChatGLM ](https://articles.zsxq.com/id_54vjwns5t6in.html)
   1. ä»‹ç»ï¼šlangchain-ChatGLMæ˜¯ä¸€ä¸ªåŸºäºæœ¬åœ°çŸ¥è¯†çš„é—®ç­”æœºå™¨äººï¼Œä½¿ç”¨è€…å¯ä»¥è‡ªç”±é…ç½®æœ¬åœ°çŸ¥è¯†ï¼Œç”¨æˆ·é—®é¢˜çš„ç­”æ¡ˆä¹Ÿæ˜¯åŸºäºæœ¬åœ°çŸ¥è¯†ç”Ÿæˆçš„ã€‚

### ç¬¬äºŒå±‚ Stanford Alpaca 7B 

- [ã€LLMs å…¥é—¨å®æˆ˜ â€”â€” äº” ã€‘Stanford Alpaca 7B æ¨¡å‹å­¦ä¹ ä¸å®æˆ˜](https://articles.zsxq.com/id_xnt3fvp2wxz0.html)
  - ä»‹ç»ï¼šæœ¬æ•™ç¨‹æä¾›äº†å¯¹LLaMAæ¨¡å‹è¿›è¡Œå¾®è°ƒçš„å»‰ä»·äº²æ°‘ LLMs å­¦ä¹ å’Œå¾®è°ƒ æ–¹å¼ï¼Œä¸»è¦ä»‹ç»å¯¹äº Stanford Alpaca 7B æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸Š çš„ å¾®è°ƒå®éªŒï¼Œæ‰€ç”¨çš„æ•°æ®ä¸ºOpenAIæä¾›çš„GPTæ¨¡å‹APIç”Ÿæˆè´¨é‡è¾ƒé«˜çš„æŒ‡ä»¤æ•°æ®ï¼ˆä»…52kï¼‰ã€‚

### ç¬¬ä¸‰å±‚ Chinese-LLaMA-Alpaca 

- [ã€LLMs å…¥é—¨å®æˆ˜ â€”â€” å…­ ã€‘Chinese-LLaMA-Alpaca æ¨¡å‹å­¦ä¹ ä¸å®æˆ˜](https://articles.zsxq.com/id_dqvusswrdg6c.html)
  - ä»‹ç»ï¼šæœ¬æ•™ç¨‹ä¸»è¦ä»‹ç»äº† Chinese-ChatLLaMA,æä¾›ä¸­æ–‡å¯¹è¯æ¨¡å‹ ChatLLama ã€ä¸­æ–‡åŸºç¡€æ¨¡å‹ LLaMA-zh åŠå…¶è®­ç»ƒæ•°æ®ã€‚ æ¨¡å‹åŸºäº TencentPretrain å¤šæ¨¡æ€é¢„è®­ç»ƒæ¡†æ¶æ„å»º

### ç¬¬å››å±‚ å°ç¾Šé©¼ Vicuna

- [ã€LLMs å…¥é—¨å®æˆ˜ â€”â€” ä¸ƒ ã€‘å°ç¾Šé©¼ Vicunaæ¨¡å‹å­¦ä¹ ä¸å®æˆ˜](https://articles.zsxq.com/id_q9mx24q9fdab.html)
  - ä»‹ç»ï¼šUCä¼¯å…‹åˆ©å­¦è€…è”æ‰‹CMUã€æ–¯å¦ç¦ç­‰ï¼Œå†æ¬¡æ¨å‡ºä¸€ä¸ªå…¨æ–°æ¨¡å‹70äº¿/130äº¿å‚æ•°çš„Vicunaï¼Œä¿—ç§°ã€Œå°ç¾Šé©¼ã€ï¼ˆéª†é©¬ï¼‰ã€‚å°ç¾Šé©¼å·ç§°èƒ½è¾¾åˆ°GPT-4çš„90%æ€§èƒ½

### ç¬¬äº”å±‚ MiniGPT-4 

- [ã€LLMs å…¥é—¨å®æˆ˜ â€”â€” å…« ã€‘MiniGPT-4 æ¨¡å‹å­¦ä¹ ä¸å®æˆ˜](https://articles.zsxq.com/id_ff0w6czthq25.html)
  - ä»‹ç»ï¼š MiniGPT-4ï¼Œæ˜¯æ¥è‡ªé˜¿åœæœæ‹‰å›½ç‹ç§‘æŠ€å¤§å­¦çš„å‡ ä½åšå£«åšçš„ï¼Œå®ƒèƒ½æä¾›ç±»ä¼¼ GPT-4 çš„å›¾åƒç†è§£ä¸å¯¹è¯èƒ½åŠ›

### ç¬¬å…­å±‚ GPT4ALL

- [ã€LLMs å…¥é—¨å®æˆ˜ â€”â€” å…« ã€‘GPT4ALL æ¨¡å‹å­¦ä¹ ä¸å®æˆ˜](https://articles.zsxq.com/id_ff0w6czthq25.html)
  - ä»‹ç»ï¼šä¸€ä¸ª å¯ä»¥åœ¨è‡ªå·±ç¬”è®°æœ¬ä¸Šé¢è·‘èµ·æ¥çš„  Nomic AI çš„åŠ©æ‰‹å¼èŠå¤©æœºå™¨äººï¼Œæˆä¸ºè´«æ°‘å®¶å­©å­çš„ ç¦éŸ³ï¼

### ç¬¬ä¸ƒå±‚ AutoGPT

- [AutoGPT ä½¿ç”¨å’Œéƒ¨ç½²](https://articles.zsxq.com/id_pli0z9916126.html)
  - ä»‹ç»ï¼šAuto-GPTæ˜¯ä¸€ä¸ªåŸºäºChatGPTçš„å·¥å…·ï¼Œä»–èƒ½å¸®ä½ è‡ªåŠ¨å®Œæˆå„ç§ä»»åŠ¡ï¼Œæ¯”å¦‚å†™ä»£ç ã€å†™æŠ¥å‘Šã€åšè°ƒç ”ç­‰ç­‰ã€‚ä½¿ç”¨å®ƒæ—¶ï¼Œä½ åªéœ€è¦å‘Šè¯‰ä»–è¦æ‰®æ¼”çš„è§’è‰²å’Œè¦å®ç°çš„ç›®æ ‡ï¼Œç„¶åä»–å°±ä¼šåˆ©ç”¨ChatGPTå’Œè°·æ­Œæœç´¢ç­‰å·¥å…·ï¼Œä¸æ–­â€œæ€è€ƒâ€å¦‚ä½•æ¥è¿‘ç›®æ ‡å¹¶æ‰§è¡Œï¼Œä½ ç”šè‡³å¯ä»¥çœ‹åˆ°ä»–çš„æ€è€ƒè¿‡ç¨‹ã€‚

### ç¬¬å…«å±‚ MOSS

- [ã€LLMs å…¥é—¨å®æˆ˜ â€”â€” åä¸‰ ã€‘MOSS æ¨¡å‹å­¦ä¹ ä¸å®æˆ˜](https://articles.zsxq.com/id_4vwpxod23zrc.html)
  - ä»‹ç»ï¼šMOSSæ˜¯ä¸€ä¸ªæ”¯æŒä¸­è‹±åŒè¯­å’Œå¤šç§æ’ä»¶çš„å¼€æºå¯¹è¯è¯­è¨€æ¨¡å‹ï¼Œmoss-moonç³»åˆ—æ¨¡å‹å…·æœ‰160äº¿å‚æ•°ï¼Œåœ¨FP16ç²¾åº¦ä¸‹å¯åœ¨å•å¼ A100/A800æˆ–ä¸¤å¼ 3090æ˜¾å¡è¿è¡Œï¼Œåœ¨INT4/8ç²¾åº¦ä¸‹å¯åœ¨å•å¼ 3090æ˜¾å¡è¿è¡Œã€‚MOSSåŸºåº§è¯­è¨€æ¨¡å‹åœ¨çº¦ä¸ƒåƒäº¿ä¸­è‹±æ–‡ä»¥åŠä»£ç å•è¯ä¸Šé¢„è®­ç»ƒå¾—åˆ°ï¼Œåç»­ç»è¿‡å¯¹è¯æŒ‡ä»¤å¾®è°ƒã€æ’ä»¶å¢å¼ºå­¦ä¹ å’Œäººç±»åå¥½è®­ç»ƒå…·å¤‡å¤šè½®å¯¹è¯èƒ½åŠ›åŠä½¿ç”¨å¤šç§æ’ä»¶çš„èƒ½åŠ›ã€‚
  - å±€é™æ€§ï¼šç”±äºæ¨¡å‹å‚æ•°é‡è¾ƒå°å’Œè‡ªå›å½’ç”ŸæˆèŒƒå¼ï¼ŒMOSSä»ç„¶å¯èƒ½ç”ŸæˆåŒ…å«äº‹å®æ€§é”™è¯¯çš„è¯¯å¯¼æ€§å›å¤æˆ–åŒ…å«åè§/æ­§è§†çš„æœ‰å®³å†…å®¹ï¼Œè¯·è°¨æ…é‰´åˆ«å’Œä½¿ç”¨MOSSç”Ÿæˆçš„å†…å®¹ï¼Œè¯·å‹¿å°†MOSSç”Ÿæˆçš„æœ‰å®³å†…å®¹ä¼ æ’­è‡³äº’è”ç½‘ã€‚è‹¥äº§ç”Ÿä¸è‰¯åæœï¼Œç”±ä¼ æ’­è€…è‡ªè´Ÿã€‚


## ä¸€ã€å‰è¨€

æœ¬æ•™ç¨‹ä¸»è¦ä»‹ç»å¯¹äº [ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B) æ¨¡å‹åŸºäº [LoRA]() è¿›è¡Œfinetuneã€‚

ä»¥[alpaca](https://github.com/tatsu-lab/stanford_alpaca) ä¸ºä¾‹ã€‚

ç¡¬ä»¶éœ€æ±‚

- æ˜¾å¡: æ˜¾å­˜ >= 16G (æœ€å¥½24Gæˆ–è€…ä»¥ä¸Š)
- ç¯å¢ƒï¼š
  - python>=3.8
  - cuda>=11.6, cupti, cuDNN, TensorRTç­‰æ·±åº¦å­¦ä¹ ç¯å¢ƒ

## äºŒã€ç¯å¢ƒæ­å»º

### 2.1 æ„å»ºç¯å¢ƒ

```s
    $ conda create -n py310_chat python=3.10       # åˆ›å»ºæ–°ç¯å¢ƒ
    $ source activate py310_chat                   # æ¿€æ´»ç¯å¢ƒ
```

### 2.2 ä¸‹è½½ä»£ç 

```s
    $ git clone https://github.com/mymusise/ChatGLM-Tuning.git
    $ cd ChatGLM-Tuning
```

### 2.3 å®‰è£…ä¾èµ–

è¿è¡Œå¾®è°ƒéœ€è¦4.27.1ç‰ˆæœ¬çš„transformersã€‚é™¤ ChatGLM-6B çš„ä¾èµ–ä¹‹å¤–ï¼Œè¿˜éœ€è¦æŒ‰ç…§ä»¥ä¸‹ä¾èµ–

```s
    $ pip install -r requirements.txt
```

## ä¸‰ã€ä½¿ç”¨æ–¹æ³•

### 3.1 è®­ç»ƒæ•°æ®ä¸‹è½½

#### 3.1.1 æ•°æ®æ¥æº

1. [alpaca](https://github.com/tatsu-lab/stanford_alpaca)

#### 3.1.2 æ•°æ®ä»‹ç»

æœ¬ç« ä½¿ç”¨ [alpaca](https://github.com/tatsu-lab/stanford_alpaca)ä½œä¸ºæœ¬æ¬¡ç‰¹å®šä»»åŠ¡å¾®è°ƒå®éªŒæ•°æ®ã€‚

> æ ·ä¾‹
```s
[
    {
        "instruction": "Give three tips for staying healthy.",
        "input": "",
        "output": "1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \n2. Exercise regularly to keep your body active and strong. \n3. Get enough sleep and maintain a consistent sleep schedule."
    },...
]
```

- å­—æ®µ
  - instruction: æŒ‡ä»¤
  - input: è¾“å…¥ï¼ˆæœ¬æ•°æ®é›†å‡ä¸ºç©ºï¼‰
  - output: è¾“å‡º

#### 3.1.3 è½¬åŒ–alpacaæ•°æ®é›†ä¸ºjsonl

> è¿è¡Œä»£ç 
```s
    $ python cover_alpaca2jsonl.py  --data_path data/alpaca_data.json  --save_path data/alpaca_data.jsonl 
```

> ç”Ÿæˆæ•°æ® data/alpaca_data.jsonl 
```s
    {"text": "### Instruction:\nIdentify the odd one out.\n\n### Input:\nTwitter, Instagram, Telegram\n\n### Response:\nTelegram\nEND\n"}
    {"text": "### Instruction:\nExplain why the following fraction is equivalent to 1/4\n\n### Input:\n4/16\n\n### Response:\nThe fraction 4/16 is equivalent to 1/4 because both numerators and denominators are divisible by 4. Dividing both the top and bottom numbers by 4 yields the fraction 1/4.\nEND\n"}
    ...
```
> æ³¨ï¼štext ä¸­åŒ…å« Instructionã€Inputã€Response ä¸‰ä¸ªä¿¡æ¯
> æ‹¼æ¥æ ¼å¼ä¸º  ### Instruction:\nã€Instructionå†…å®¹ã€‘\n\n### Input:\nã€Inputå†…å®¹ã€‘\n\n### Response:\nã€Responseå†…å®¹ã€‘\nEND\n

### 3.2 tokenize_dataset ä¸‹è½½

```s
    $ python tokenize_dataset_rows.py  --jsonl_path data/alpaca_data.jsonl  --save_path data/alpaca     --max_seq_length 128
```

> --jsonl_path å¾®è°ƒçš„æ•°æ®è·¯å¾„, æ ¼å¼jsonl, å¯¹æ¯è¡Œçš„['context']å’Œ['target']å­—æ®µè¿›è¡Œencode
> --save_path è¾“å‡ºè·¯å¾„
> --max_seq_length æ ·æœ¬çš„æœ€å¤§é•¿åº¦

### 3.3 æ¨¡å‹ finetune

è¿è¡Œä»¥ä¸‹æŒ‡ä»¤è¿›è¡Œå¾®è°ƒï¼š

> lora æ–¹å¼ finetune
```s
    $ python finetune.py \
    --dataset_path data/alpaca \
    --lora_rank 8 \
    --per_device_train_batch_size 6 \
    --gradient_accumulation_steps 1 \
    --max_steps 52000 \
    --save_steps 1000 \
    --save_total_limit 2 \
    --learning_rate 1e-4 \
    --fp16 \
    --remove_unused_columns false \
    --logging_steps 50 \
    --output_dir output
    >>>
    TrainOutput(global_step=1500, training_loss=1.4622032979329427, metrics={'train_runtime': 474.9934, 'train_samples_per_second': 3.158, 'train_steps_per_second': 3.158, 'total_flos': 3781851053211648.0, 'train_loss': 1.4622032979329427, 'epoch': 3.0})
    ...
```

### 3.4 æ¨¡å‹æ¨ç†

è¿è¡Œä»¥ä¸‹æŒ‡ä»¤è¿›è¡Œæ¨ç†ï¼š

> infer.py æ–‡ä»¶
```s
    from modeling_chatglm import ChatGLMForConditionalGeneration
    import torch
    torch.set_default_tensor_type(torch.cuda.HalfTensor)
    model = ChatGLMForConditionalGeneration.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True, device_map='auto')
    from peft import PeftModel
    model = PeftModel.from_pretrained(model, "mymusise/chatGLM-6B-alpaca-lora")
    torch.set_default_tensor_type(torch.cuda.FloatTensor)
    from transformers import AutoTokenizer
    tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True)
    from cover_alpaca2jsonl import format_example

    # alpacaæ•°æ®é›†
    instructions = [
        {'instruction': 'Give three tips for staying healthy.',
        'input': '',
        'output': '1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \n2. Exercise regularly to keep your body active and strong. \n3. Get enough sleep and maintain a consistent sleep schedule.',
        },
        {'instruction': 'What are the three primary colors?',
        'input': '',
        'output': 'The three primary colors are red, blue, and yellow.',
        }
    ]

    with torch.no_grad():
        for idx, item in enumerate(instructions):
            feature = format_example(item)
            input_text = feature['context']
            ids = tokenizer.encode(input_text)
            input_ids = torch.LongTensor([ids])
            out = model.generate(
                input_ids=input_ids,
                max_length=150,
                do_sample=False,
                temperature=0
            )
            out_text = tokenizer.decode(out[0])
            answer = out_text.replace(input_text, "").replace("\nEND", "").strip()
            item['infer_answer'] = answer
            print(out_text)
            print(f"### {idx+1}.Answer:\n", item.get('output'), '\n\n')

```

> è¿è¡Œ infer.py è¿›è¡Œ æ¨ç†
```s
    $ python infer.py
    >>>
    Output exceeds the size limit. Open the full output data in a text editor
    Instruction: Give three tips for staying healthy.
    Answer: 1. Eat a balanced diet of fruits, vegetables, lean protein, and whole grains.
    2. Get regular exercise, such as walking, running, or swimming.
    3. Stay hydrated by drinking plenty of water.
    ### 1.Answer:
    1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. 
    2. Exercise regularly to keep your body active and strong. 
    3. Get enough sleep and maintain a consistent sleep schedule. 


    Instruction: What are the three primary colors?
    Answer: The three primary colors are red, blue, and yellow.
    ### 2.Answer:
    The three primary colors are red, blue, and yellow. 
```

## å‚è€ƒ/æ„Ÿè°¢

1. [THUDM/ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B)
2. [mymusise/ChatGLM-Tuning](https://github.com/mymusise/ChatGLM-Tuning) 
3. [ä»0åˆ°1åŸºäºChatGLM-6Bä½¿ç”¨LaRAè¿›è¡Œå‚æ•°é«˜æ•ˆå¾®è°ƒ](https://zhuanlan.zhihu.com/p/621793987)
