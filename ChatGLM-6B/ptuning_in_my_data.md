# ã€å…³äºŽ ChatGLM ç‰¹å®šä»»åŠ¡å¾®è°ƒã€‘é‚£äº›ä½ ä¸çŸ¥é“çš„äº‹

## ã€LLMs å…¥é—¨å®žæˆ˜ç³»åˆ—ã€‘

### ç¬¬ä¸€å±‚ ChatGLM-6B

1. [ã€ChatGLM-6Bå…¥é—¨-ä¸€ã€‘æ¸…åŽå¤§å­¦å¼€æºä¸­æ–‡ç‰ˆChatGLM-6Bæ¨¡åž‹å­¦ä¹ ä¸Žå®žæˆ˜](ChatGLM-6B/induction.md)
   1. ä»‹ç»ï¼šChatGLM-6B çŽ¯å¢ƒé…ç½® å’Œ éƒ¨ç½²
2. [ã€ChatGLM-6Bå…¥é—¨-äºŒã€‘æ¸…åŽå¤§å­¦å¼€æºä¸­æ–‡ç‰ˆChatGLM-6Bæ¨¡åž‹å¾®è°ƒå®žæˆ˜](ChatGLM-6B/ptuning.md)
   1. ChatGLM-6B P-Tuning V2 å¾®è°ƒï¼šFine-tuning the prefix encoder of the model.
3. [ã€ChatGLM-6Bå…¥é—¨-ä¸‰ã€‘ChatGLM ç‰¹å®šä»»åŠ¡å¾®è°ƒå®žæˆ˜](https://articles.zsxq.com/id_3b42ukjdkwpt.html)
4. [ã€ChatGLM-6Bå…¥é—¨-å››ã€‘ChatGLM + LoRA è¿›è¡Œfinetune](https://articles.zsxq.com/id_e2389qm0w0sx.html)
   1. ä»‹ç»ï¼šChatGLM-6B LoRA å¾®è°ƒï¼šFine-tuning the low-rank adapters of the model.
5. [ChatGLM-6B å°ç¼–å¡«å‘è®°](https://articles.zsxq.com/id_fw7vn0mhdsnq.html)
   1. ä»‹ç»ï¼šChatGLM-6B åœ¨ éƒ¨ç½²å’Œå¾®è°ƒ è¿‡ç¨‹ä¸­ ä¼šé‡åˆ°å¾ˆå¤šå‘ï¼Œå°ç¼–æŽ‰å‘äº†å¾ˆå¤šæ¬¡ï¼Œä¸ºé˜²æ­¢ åŽäººå’Œå°ç¼–ä¸€æ ·ç»§ç»­æŽ‰å‘ï¼Œå°ç¼–ç´¢æ€§æŠŠé‡åˆ°çš„å‘éƒ½å¡«äº†ã€‚
6. [ã€LLMså­¦ä¹ ã€‘å…³äºŽå¤§æ¨¡åž‹å®žè·µçš„ä¸€äº›æ€»ç»“](https://articles.zsxq.com/id_il58nxrs9jxr.html)
7. [ã€LLMs å…¥é—¨å®žæˆ˜ â€”â€” åä¸€ ã€‘åŸºäºŽ ðŸ¤—PEFT çš„é«˜æ•ˆ ðŸ¤–ChatGLM-6B å¾®è°ƒ](https://articles.zsxq.com/id_7rz5jtfguuc5.html)
   1. å¾®è°ƒæ–¹å¼ï¼š
      1. ChatGLM-6B Freeze å¾®è°ƒï¼šFine-tuning the MLPs in the last n blocks of the model.
      2. ChatGLM-6B P-Tuning V2 å¾®è°ƒï¼šFine-tuning the prefix encoder of the model.
      3. ChatGLM-6B LoRA å¾®è°ƒï¼šFine-tuning the low-rank adapters of the model.
8. [ã€LLMs å…¥é—¨å®žæˆ˜ â€”â€” åäºŒ ã€‘åŸºäºŽ æœ¬åœ°çŸ¥è¯†åº“ çš„é«˜æ•ˆ ðŸ¤–langchain-ChatGLM ](https://articles.zsxq.com/id_54vjwns5t6in.html)
   1. ä»‹ç»ï¼šlangchain-ChatGLMæ˜¯ä¸€ä¸ªåŸºäºŽæœ¬åœ°çŸ¥è¯†çš„é—®ç­”æœºå™¨äººï¼Œä½¿ç”¨è€…å¯ä»¥è‡ªç”±é…ç½®æœ¬åœ°çŸ¥è¯†ï¼Œç”¨æˆ·é—®é¢˜çš„ç­”æ¡ˆä¹Ÿæ˜¯åŸºäºŽæœ¬åœ°çŸ¥è¯†ç”Ÿæˆçš„ã€‚

### ç¬¬äºŒå±‚ Stanford Alpaca 7B 

- [ã€LLMs å…¥é—¨å®žæˆ˜ â€”â€” äº” ã€‘Stanford Alpaca 7B æ¨¡åž‹å­¦ä¹ ä¸Žå®žæˆ˜](https://articles.zsxq.com/id_xnt3fvp2wxz0.html)
  - ä»‹ç»ï¼šæœ¬æ•™ç¨‹æä¾›äº†å¯¹LLaMAæ¨¡åž‹è¿›è¡Œå¾®è°ƒçš„å»‰ä»·äº²æ°‘ LLMs å­¦ä¹ å’Œå¾®è°ƒ æ–¹å¼ï¼Œä¸»è¦ä»‹ç»å¯¹äºŽ Stanford Alpaca 7B æ¨¡åž‹åœ¨ç‰¹å®šä»»åŠ¡ä¸Š çš„ å¾®è°ƒå®žéªŒï¼Œæ‰€ç”¨çš„æ•°æ®ä¸ºOpenAIæä¾›çš„GPTæ¨¡åž‹APIç”Ÿæˆè´¨é‡è¾ƒé«˜çš„æŒ‡ä»¤æ•°æ®ï¼ˆä»…52kï¼‰ã€‚

### ç¬¬ä¸‰å±‚ Chinese-LLaMA-Alpaca 

- [ã€LLMs å…¥é—¨å®žæˆ˜ â€”â€” å…­ ã€‘Chinese-LLaMA-Alpaca æ¨¡åž‹å­¦ä¹ ä¸Žå®žæˆ˜](https://articles.zsxq.com/id_dqvusswrdg6c.html)
  - ä»‹ç»ï¼šæœ¬æ•™ç¨‹ä¸»è¦ä»‹ç»äº† Chinese-ChatLLaMA,æä¾›ä¸­æ–‡å¯¹è¯æ¨¡åž‹ ChatLLama ã€ä¸­æ–‡åŸºç¡€æ¨¡åž‹ LLaMA-zh åŠå…¶è®­ç»ƒæ•°æ®ã€‚ æ¨¡åž‹åŸºäºŽ TencentPretrain å¤šæ¨¡æ€é¢„è®­ç»ƒæ¡†æž¶æž„å»º

### ç¬¬å››å±‚ å°ç¾Šé©¼ Vicuna

- [ã€LLMs å…¥é—¨å®žæˆ˜ â€”â€” ä¸ƒ ã€‘å°ç¾Šé©¼ Vicunaæ¨¡åž‹å­¦ä¹ ä¸Žå®žæˆ˜](https://articles.zsxq.com/id_q9mx24q9fdab.html)
  - ä»‹ç»ï¼šUCä¼¯å…‹åˆ©å­¦è€…è”æ‰‹CMUã€æ–¯å¦ç¦ç­‰ï¼Œå†æ¬¡æŽ¨å‡ºä¸€ä¸ªå…¨æ–°æ¨¡åž‹70äº¿/130äº¿å‚æ•°çš„Vicunaï¼Œä¿—ç§°ã€Œå°ç¾Šé©¼ã€ï¼ˆéª†é©¬ï¼‰ã€‚å°ç¾Šé©¼å·ç§°èƒ½è¾¾åˆ°GPT-4çš„90%æ€§èƒ½

### ç¬¬äº”å±‚ MiniGPT-4 

- [ã€LLMs å…¥é—¨å®žæˆ˜ â€”â€” å…« ã€‘MiniGPT-4 æ¨¡åž‹å­¦ä¹ ä¸Žå®žæˆ˜](https://articles.zsxq.com/id_ff0w6czthq25.html)
  - ä»‹ç»ï¼š MiniGPT-4ï¼Œæ˜¯æ¥è‡ªé˜¿åœæœæ‹‰å›½çŽ‹ç§‘æŠ€å¤§å­¦çš„å‡ ä½åšå£«åšçš„ï¼Œå®ƒèƒ½æä¾›ç±»ä¼¼ GPT-4 çš„å›¾åƒç†è§£ä¸Žå¯¹è¯èƒ½åŠ›

### ç¬¬å…­å±‚ GPT4ALL

- [ã€LLMs å…¥é—¨å®žæˆ˜ â€”â€” å…« ã€‘GPT4ALL æ¨¡åž‹å­¦ä¹ ä¸Žå®žæˆ˜](https://articles.zsxq.com/id_ff0w6czthq25.html)
  - ä»‹ç»ï¼šä¸€ä¸ª å¯ä»¥åœ¨è‡ªå·±ç¬”è®°æœ¬ä¸Šé¢è·‘èµ·æ¥çš„  Nomic AI çš„åŠ©æ‰‹å¼èŠå¤©æœºå™¨äººï¼Œæˆä¸ºè´«æ°‘å®¶å­©å­çš„ ç¦éŸ³ï¼

### ç¬¬ä¸ƒå±‚ AutoGPT

- [AutoGPT ä½¿ç”¨å’Œéƒ¨ç½²](https://articles.zsxq.com/id_pli0z9916126.html)
  - ä»‹ç»ï¼šAuto-GPTæ˜¯ä¸€ä¸ªåŸºäºŽChatGPTçš„å·¥å…·ï¼Œä»–èƒ½å¸®ä½ è‡ªåŠ¨å®Œæˆå„ç§ä»»åŠ¡ï¼Œæ¯”å¦‚å†™ä»£ç ã€å†™æŠ¥å‘Šã€åšè°ƒç ”ç­‰ç­‰ã€‚ä½¿ç”¨å®ƒæ—¶ï¼Œä½ åªéœ€è¦å‘Šè¯‰ä»–è¦æ‰®æ¼”çš„è§’è‰²å’Œè¦å®žçŽ°çš„ç›®æ ‡ï¼Œç„¶åŽä»–å°±ä¼šåˆ©ç”¨ChatGPTå’Œè°·æ­Œæœç´¢ç­‰å·¥å…·ï¼Œä¸æ–­â€œæ€è€ƒâ€å¦‚ä½•æŽ¥è¿‘ç›®æ ‡å¹¶æ‰§è¡Œï¼Œä½ ç”šè‡³å¯ä»¥çœ‹åˆ°ä»–çš„æ€è€ƒè¿‡ç¨‹ã€‚

### ç¬¬å…«å±‚ MOSS

- [ã€LLMs å…¥é—¨å®žæˆ˜ â€”â€” åä¸‰ ã€‘MOSS æ¨¡åž‹å­¦ä¹ ä¸Žå®žæˆ˜](https://articles.zsxq.com/id_4vwpxod23zrc.html)
  - ä»‹ç»ï¼šMOSSæ˜¯ä¸€ä¸ªæ”¯æŒä¸­è‹±åŒè¯­å’Œå¤šç§æ’ä»¶çš„å¼€æºå¯¹è¯è¯­è¨€æ¨¡åž‹ï¼Œmoss-moonç³»åˆ—æ¨¡åž‹å…·æœ‰160äº¿å‚æ•°ï¼Œåœ¨FP16ç²¾åº¦ä¸‹å¯åœ¨å•å¼ A100/A800æˆ–ä¸¤å¼ 3090æ˜¾å¡è¿è¡Œï¼Œåœ¨INT4/8ç²¾åº¦ä¸‹å¯åœ¨å•å¼ 3090æ˜¾å¡è¿è¡Œã€‚MOSSåŸºåº§è¯­è¨€æ¨¡åž‹åœ¨çº¦ä¸ƒåƒäº¿ä¸­è‹±æ–‡ä»¥åŠä»£ç å•è¯ä¸Šé¢„è®­ç»ƒå¾—åˆ°ï¼ŒåŽç»­ç»è¿‡å¯¹è¯æŒ‡ä»¤å¾®è°ƒã€æ’ä»¶å¢žå¼ºå­¦ä¹ å’Œäººç±»åå¥½è®­ç»ƒå…·å¤‡å¤šè½®å¯¹è¯èƒ½åŠ›åŠä½¿ç”¨å¤šç§æ’ä»¶çš„èƒ½åŠ›ã€‚
  - å±€é™æ€§ï¼šç”±äºŽæ¨¡åž‹å‚æ•°é‡è¾ƒå°å’Œè‡ªå›žå½’ç”ŸæˆèŒƒå¼ï¼ŒMOSSä»ç„¶å¯èƒ½ç”ŸæˆåŒ…å«äº‹å®žæ€§é”™è¯¯çš„è¯¯å¯¼æ€§å›žå¤æˆ–åŒ…å«åè§/æ­§è§†çš„æœ‰å®³å†…å®¹ï¼Œè¯·è°¨æ…Žé‰´åˆ«å’Œä½¿ç”¨MOSSç”Ÿæˆçš„å†…å®¹ï¼Œè¯·å‹¿å°†MOSSç”Ÿæˆçš„æœ‰å®³å†…å®¹ä¼ æ’­è‡³äº’è”ç½‘ã€‚è‹¥äº§ç”Ÿä¸è‰¯åŽæžœï¼Œç”±ä¼ æ’­è€…è‡ªè´Ÿã€‚


## ä¸€ã€å‰è¨€

æœ¬æ•™ç¨‹ä¸»è¦ä»‹ç»å¯¹äºŽ ChatGLM-6B æ¨¡åž‹åŸºäºŽ [P-Tuning v2](https://github.com/THUDM/P-tuning-v2) çš„ç‰¹å®šä»»åŠ¡å¾®è°ƒå®žéªŒï¼Œå¾®è°ƒç›®æ ‡ä¸ºè‡ªåŠ¨ç”Ÿæˆçš„æ•´æ•°/å°æ•°åŠ å‡ä¹˜é™¤è¿ç®—ã€‚

æœ¬èŠ‚ ä»¥ æ•´æ•°/å°æ•°åŠ å‡ä¹˜é™¤è¿ æ•°æ®é›†ä¸ºä¾‹ä»‹ç»ä»£ç çš„ä½¿ç”¨æ–¹æ³•ï¼Œä»¥[yongzhuo/chatglm-maths](https://github.com/yongzhuo/chatglm-maths) ä¸ºä¾‹ã€‚

ç¡¬ä»¶éœ€æ±‚

| **é‡åŒ–ç­‰çº§**   | **æœ€ä½Ž GPU æ˜¾å­˜**ï¼ˆæŽ¨ç†ï¼‰ | **æœ€ä½Ž GPU æ˜¾å­˜**ï¼ˆé«˜æ•ˆå‚æ•°å¾®è°ƒï¼‰ |
| -------------- | ------------------------- | --------------------------------- |
| FP16ï¼ˆæ— é‡åŒ–ï¼‰ | 13 GB                     | 14 GB                             |
| INT8           | 8 GB                     | 9 GB                             |
| INT4           | 6 GB                      | 7 GB                              |

## äºŒã€çŽ¯å¢ƒæ­å»º

### 2.1 æž„å»ºçŽ¯å¢ƒ

```s
    $ conda create -n py310_chat python=3.10       # åˆ›å»ºæ–°çŽ¯å¢ƒ
    $ source activate py310_chat                   # æ¿€æ´»çŽ¯å¢ƒ
```

### 2.2 ä¸‹è½½ä»£ç 

```s
    $ git clone https://github.com/yongzhuo/chatglm-maths.git
    $ cd chatglm-maths
```

### 2.3 å®‰è£…ä¾èµ–

è¿è¡Œå¾®è°ƒéœ€è¦4.27.1ç‰ˆæœ¬çš„transformersã€‚é™¤ ChatGLM-6B çš„ä¾èµ–ä¹‹å¤–ï¼Œè¿˜éœ€è¦æŒ‰ç…§ä»¥ä¸‹ä¾èµ–

```s
    $ pip install -r requirements.txt
```

## ä¸‰ã€ä½¿ç”¨æ–¹æ³•

### 3.1 è®­ç»ƒæ•°æ®ä¸‹è½½

#### 3.1.1 æ•°æ®æ¥æº

1. [tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca)
2. [LianjiaTech/BELLE](https://github.com/LianjiaTech/BELLE)
3. [carbonz0/alpaca-chinese-dataset](https://github.com/carbonz0/alpaca-chinese-dataset)

#### 3.1.2 æ•°æ®ä»‹ç»

æœ¬ç« ä½¿ç”¨ [BelleGroup/school_math_0.25M](https://huggingface.co/datasets/BelleGroup/school_math_0.25M) ä½œä¸ºæœ¬æ¬¡ç‰¹å®šä»»åŠ¡å¾®è°ƒå®žéªŒæ•°æ®ï¼Œ è¯¥æ•°æ®é›† åŒ…å«çº¦25ä¸‡æ¡ç”±BELLEé¡¹ç›®ç”Ÿæˆçš„ä¸­æ–‡æ•°å­¦é¢˜æ•°æ®ï¼ŒåŒ…å«è§£é¢˜è¿‡ç¨‹ã€‚

æ³¨æ„ï¼šæ­¤æ•°æ®é›†æ˜¯ç”±ChatGPTäº§ç”Ÿçš„ï¼Œæœªç»è¿‡ä¸¥æ ¼æ ¡éªŒï¼Œé¢˜ç›®æˆ–è§£é¢˜è¿‡ç¨‹å¯èƒ½åŒ…å«é”™è¯¯ã€‚ä½¿ç”¨è¿‡ç¨‹ä¸­è¯·æ³¨æ„è¿™ä¸€ç‚¹ã€‚

> æ ·ä¾‹
```s
{
  "instruction": "é¢˜ç›®ï¼šå°åŽæ‰‹é‡Œæœ‰ä¸€ä¸ªè£…æ»¡ç³–æžœçš„è¢‹å­ï¼Œå…±æœ‰12ä¸ªï¼Œå°æ˜Žæƒ³çŸ¥é“é‡Œé¢æœ‰å¤šå°‘é¢—ç³–æžœï¼ŒäºŽæ˜¯ä»–é—®å°åŽï¼šâ€œä½ æ‰‹é‡Œçš„ç³–æžœè¢‹å­é‡Œæœ‰å¶æ•°ä¸ªç³–æžœå—ï¼Ÿâ€å°åŽå›žç­”ï¼šâ€œæœ‰ï¼Œè€Œä¸”å¤šäºŽ10é¢—ã€‚â€è¯·é—®å°åŽæ‰‹é‡Œçš„ç³–æžœè¢‹å­é‡Œæœ€å°‘æœ‰å‡ é¢—ç³–æžœï¼Ÿ",
  "input": "",
  "output": "\nç”±é¢˜ç›®å¯çŸ¥ï¼šå°åŽæ‰‹é‡Œçš„ç³–æžœè¢‹å­é‡Œæœ‰å¶æ•°ä¸ªç³–æžœï¼›\nåˆçŸ¥é“å°åŽæ‰‹é‡Œçš„ç³–æžœè¢‹å­é‡Œæœ‰å¤šäºŽ10é¢—ç³–æžœã€‚\nå› ä¸ºç³–æžœæ•°ä¸ºå¶æ•°ï¼Œå¤šäºŽ10é¢—ï¼Œæ‰€ä»¥ç³–æžœæ•°æœ€å°å¿…é¡»æ˜¯12ã€‚\næ‰€ä»¥å°åŽæ‰‹é‡Œçš„ç³–æžœè¢‹å­é‡Œæœ€å°‘æœ‰12é¢—ç³–æžœã€‚"
}
```

- å­—æ®µ
  - instruction: æŒ‡ä»¤
  - input: è¾“å…¥ï¼ˆæœ¬æ•°æ®é›†å‡ä¸ºç©ºï¼‰
  - output: è¾“å‡º

#### 3.1.3 æ•°æ®ä¸‹è½½æ–¹å¼

> æ–¹å¼ä¸€
```s
from datasets import load_dataset
dataset = load_dataset("BelleGroup/school_math_0.25M")
```

> æ–¹å¼äºŒã€æœ¬æ–‡æ‰€é‡‡ç”¨çš„æ–¹å¼ã€‘
```s
git lfs install
git clone https://huggingface.co/datasets/BelleGroup/school_math_0.25M
```

### 3.2 æ¨¡åž‹ä¸‹è½½

[Huggingface å¹³å°](https://huggingface.co/THUDM)ä¸‹è½½

```s
    $ git lfs install
    $ git clone https://huggingface.co/THUDM/chatglm-6b
```

### 3.3 æ¨¡åž‹å¾®è°ƒ

è¿è¡Œä»¥ä¸‹æŒ‡ä»¤è¿›è¡Œå¾®è°ƒï¼š

> lora æ–¹å¼ å¾®è°ƒ
```s
    $ python c00_toy_lora_train_6b.py
    >>>
    generator_calculate_line: ('13+75=', '13+75=88')
    tokenizer.vocab_size: 150344
    Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:10<00:00,  1.31s/it]
    transformer.word_embeddings.weight False
    ......
    transformer.layers.26.mlp.dense_4h_to_h.bias False
    transformer.layers.27.input_layernorm.weight True
    transformer.layers.27.input_layernorm.bias True
    transformer.layers.27.attention.query_key_value.weight True
    transformer.layers.27.attention.query_key_value.bias True
    transformer.layers.27.attention.dense.weight True
    transformer.layers.27.attention.dense.bias True
    transformer.layers.27.post_attention_layernorm.weight True
    transformer.layers.27.post_attention_layernorm.bias True
    transformer.layers.27.mlp.dense_h_to_4h.weight True
    transformer.layers.27.mlp.dense_h_to_4h.bias True
    transformer.layers.27.mlp.dense_4h_to_h.weight True
    transformer.layers.27.mlp.dense_4h_to_h.bias True
    transformer.final_layernorm.weight True
    transformer.final_layernorm.bias True
    model.chat start
    13+75=88, but that's not the correct answer. The correct answer is 13+75=88, which is 90.
    /anaconda3/envs/py371/lib/python3.7/site-packages/transformers/optimization.py:395: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
    FutureWarning,   
    epoch:   0%|                                 | 0/21 [00:00<?, ?it/s]epochs:
     batch_query: ['ç®€ä¾¿è¿ç®—: 98+83= å‰–æž: 98+83=181']       | 0/8 [00:00<?, ?it/s]
    epoch:   0%|                        | 0/21 [00:00<?, ?it/s]
    epochs:   batch_query: ['ç®€ä¾¿è¿ç®—: 98+83= å‰–æž: 98+83=181']    | 0/8 [00:00<?, ?it/s]
    epoch_global: 0, step_global: 1, step: 0, loss: 4.0625
    batch_query: ['å£ç®—: 57.84+13.64 è§£: 57.84+13.64=71.48']                               epoch_global: 0, step_global: 2, step: 1, loss: 2.5625â–ˆâ–ˆâ–ˆâ–Œ                 | 2/8 [00:17<00:51,  8.54s/it]
batch_query: ['è®¡ç®—é¢˜: 48+1 è§£ç­”: 48+1=49']                                                                                            epoch_global: 0, step_global: 3, step: 2, loss: 4.15625â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž        | 3/8 [00:38<01:09, 13.94s/it]
    batch_query: ['è®¡ç®—é¢˜: 61.65+33.05 è§£ç­”: 61.65+33.05=94.7']                           epoch_global: 0, step_global: 4, step: 3, loss: 2.40625â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [01:01<01:09, 17.43s/it]
batch_query: ['è®¡ç®—: 81+75 å›žç­”: 81+75=156']
    ...     
```

### 3.4 æ¨¡åž‹æŽ¨ç†

è¿è¡Œä»¥ä¸‹æŒ‡ä»¤è¿›è¡ŒæŽ¨ç†ï¼š

> lora æ–¹å¼ æŽ¨ç†
```s
    $ python p00_toy_lora_predict_6b.py
    >>>
    generator_calculate_line: ('13+75=', '13+75=88')
    tokenizer.vocab_size: 150344
    eval:   0%|                                                                                                                                                                      | 0/1 [00:00<?, ?it/s]batch_query: ['ç®€ä¾¿è¿ç®—: 98+83= å‰–æž: 98+83=181']
    batch_qtext_0: ç®€ä¾¿è¿ç®—: 98+83= å‰–æž:
    batch_qans_0: 98+83=181
    response_0: 98+83=171
    {'rouge-1': 0.0, 'rouge-2': 0.0, 'rouge-l': 0.0, 'bleu': 0.0}
    è¯·è¾“å…¥:
    25.31+86.35=
    è¯·ç¨ç­‰...
    25.31+86.35=101.66
    ...
```

## å››ã€ç»éªŒåˆ†äº«

è¿™é‡Œä¸»è¦æ¥ç€ [yongzhuo/chatglm-maths](https://github.com/yongzhuo/chatglm-maths) åšä¸»åœ¨å¤çŽ°è¿‡ç¨‹ä¸­ çš„ ç»éªŒåˆ†äº«ã€‚

1. eps=1e-5(ä¸è¦æ”¹å°), åŠç²¾åº¦float16, ä»¥åŠLNé‡‡ç”¨çš„æ˜¯Post-LN(æ³›åŒ–æ€§æ›´å¥½) + DeepNorm, ã€å®³, Attentionå‰ä¹Ÿæœ‰LNã€‘ç›®çš„æ˜¯å¤§æ¨¡åž‹ä¸ºäº†é˜²æ­¢æ¢¯åº¦æº¢å‡ºç­‰;
2. æ¨¡åž‹è¾“å…¥è¾“å‡º, é»˜è®¤çš„tokenization_chatglm.py/modeling_chatglm.pyä¸èƒ½ç”¨, å› ä¸ºé‚£æ˜¯å®Œå…¨ä¸ºç”Ÿæˆgenerateè®¾ç½®çš„, éœ€è¦è‡ªå·±å†™å¥½æ‰€æœ‰ç¼©å…¥å‚æ•°, æˆ–è€…æœºå­æ”¹æˆé€‚é…çš„;
   1. ChatGLMModelä¸­, get_masks()æ­£å¸¸, get_position_ids()å‡½æ•°ä¸­â€˜context_length = seq.index(150004) + 1â€™ æ”¹ä¸º â€˜context_length = len(seq)â€™;
   2. è®­ç»ƒè¾“å…¥input_idsæ ¼å¼æš‚å®šä¸º(è®­ç»ƒåŽpost-padding, æŽ¨ç†å‰pre-padding[tokenization_chatglm.pyé»˜è®¤pre-padding]) ï¼ˆeg: x: prompt_1 + "\n" + "_" + text_1 + "\n" + prompt_2 + [gMASK] + [BOS] + "_" + text_2 + [PAD]*Nï¼‰
   3. è®­ç»ƒè¾“å…¥label_idsæ ¼å¼æš‚å®šä¸º(CrossEntropyLossé»˜è®¤å¿½ç•¥-100ä¸å‚ä¸Žè®¡ç®—loss)  (egï¼šy = [-100]*len(text_1+1) + [BOS] + text_2 + [EOS] + [-100]*N)
   4. æ³¨æ„position/mask(è‡ªå¸¦çš„åªæ˜¯æŽ¨ç†ç”¨çš„batch_size=1, æ‰€ä»¥è®­ç»ƒè¾“å…¥è¿˜å¾—è‡ªå·±å†™), å¯å‚è€ƒGLM-130çš„README.md, huozhe æŸ¥çœ‹GLM-1æºç https://github.com/THUDM/GLM/blob/main/tasks/seq2seq/dataset.py
3. æ³¨æ„chatglm-6bæƒé‡æ˜¯float16çš„, ä¸è¿‡è®¡ç®—lossæ—¶å€™ä¼šè½¬æˆfloat32è®¡ç®—, æœ€åŽlosså†è½¬å›žfloat16æ›´æ–°æ¢¯åº¦;
4. ChatGLMTokenizeræœ‰æ—¶å€™ä¼šæŠ¥å¥‡å¥‡æ€ªæ€ªçš„é”™è¯¯, å»ºè®®ç”Ÿæˆæ—¶å€™è®¾ç½®max_new_tokens, æœ€å¤§{"max_new_tokens": 2048}; decodeæœ‰æ—¶å€™ä¼šå‡ºçŽ°ä¸å­˜åœ¨id;
5. ä½Žç§©è‡ªé€‚åº”LORA, RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!
   å°è¯• transformerså‡çº§åˆ°æœ€æ–°, get_peft_modelåŽå†.cuda(), device_map={'':torch.cuda.current_device()}

## å‚è€ƒ/æ„Ÿè°¢

1. [THUDM/ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B)
2. [å›½äº§å¼€æºç±»ChatGPTæ¨¡åž‹ï¼ŒChatGLM-6båˆæ­¥å¾®è°ƒå®žéªŒ](https://zhuanlan.zhihu.com/p/616013638)
3. [yongzhuo/chatglm-maths](https://github.com/yongzhuo/chatglm-maths) 
4. [THUDM/GLM](https://github.com/THUDM/GLM)
5. [tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca)
6. [LianjiaTech/BELLE](https://github.com/LianjiaTech/BELLE)
7. [carbonz0/alpaca-chinese-dataset](https://github.com/carbonz0/alpaca-chinese-dataset)
8. [huggingface/peft](https://github.com/huggingface/peft)
9. [mymusise/ChatGLM-Tuning](https://github.com/mymusise/ChatGLM-Tuning)
10. [å›½äº§å¼€æºç±»ChatGPTæ¨¡åž‹ï¼ŒChatGLM-6båˆæ­¥å¾®è°ƒå®žéªŒ](https://zhuanlan.zhihu.com/p/616013638) 
