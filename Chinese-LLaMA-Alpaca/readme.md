# ã€LLMs å…¥é—¨å®æˆ˜ â€”â€” å…­ ã€‘Chinese-LLaMA-Alpaca æ¨¡å‹å­¦ä¹ ä¸å®æˆ˜

- github åœ°å€: https://github.com/ydli-ai/Chinese-ChatLLaMA
- è¯•ç”¨åœ°å€ï¼šhttps://alpaca-ai-custom6.ngrok.io/

## ã€LLMs å…¥é—¨å®æˆ˜ç³»åˆ—ã€‘

### ç¬¬ä¸€å±‚ ChatGLM-6B

1. [ã€ChatGLM-6Bå…¥é—¨-ä¸€ã€‘æ¸…åå¤§å­¦å¼€æºä¸­æ–‡ç‰ˆChatGLM-6Bæ¨¡å‹å­¦ä¹ ä¸å®æˆ˜](ChatGLM-6B/induction.md)
   1. ä»‹ç»ï¼šChatGLM-6B ç¯å¢ƒé…ç½® å’Œ éƒ¨ç½²
2. [ã€ChatGLM-6Bå…¥é—¨-äºŒã€‘æ¸…åå¤§å­¦å¼€æºä¸­æ–‡ç‰ˆChatGLM-6Bæ¨¡å‹å¾®è°ƒå®æˆ˜](ChatGLM-6B/ptuning.md)
   1. ChatGLM-6B P-Tuning V2 å¾®è°ƒï¼šFine-tuning the prefix encoder of the model.
3. [ã€ChatGLM-6Bå…¥é—¨-ä¸‰ã€‘ChatGLM ç‰¹å®šä»»åŠ¡å¾®è°ƒå®æˆ˜](https://articles.zsxq.com/id_3b42ukjdkwpt.html)
4. [ã€ChatGLM-6Bå…¥é—¨-å››ã€‘ChatGLM + LoRA è¿›è¡Œfinetune](https://articles.zsxq.com/id_e2389qm0w0sx.html)
   1. ä»‹ç»ï¼šChatGLM-6B LoRA å¾®è°ƒï¼šFine-tuning the low-rank adapters of the model.
5. [ChatGLM-6B å°ç¼–å¡«å‘è®°](https://articles.zsxq.com/id_fw7vn0mhdsnq.html)
   1. ä»‹ç»ï¼šChatGLM-6B åœ¨ éƒ¨ç½²å’Œå¾®è°ƒ è¿‡ç¨‹ä¸­ ä¼šé‡åˆ°å¾ˆå¤šå‘ï¼Œå°ç¼–æ‰å‘äº†å¾ˆå¤šæ¬¡ï¼Œä¸ºé˜²æ­¢ åäººå’Œå°ç¼–ä¸€æ ·ç»§ç»­æ‰å‘ï¼Œå°ç¼–ç´¢æ€§æŠŠé‡åˆ°çš„å‘éƒ½å¡«äº†ã€‚
6. [ã€LLMså­¦ä¹ ã€‘å…³äºå¤§æ¨¡å‹å®è·µçš„ä¸€äº›æ€»ç»“](https://articles.zsxq.com/id_il58nxrs9jxr.html)
7. [ã€LLMs å…¥é—¨å®æˆ˜ â€”â€” åä¸€ ã€‘åŸºäº ğŸ¤—PEFT çš„é«˜æ•ˆ ğŸ¤–ChatGLM-6B å¾®è°ƒ](https://articles.zsxq.com/id_7rz5jtfguuc5.html)
   1. å¾®è°ƒæ–¹å¼ï¼š
      1. ChatGLM-6B Freeze å¾®è°ƒï¼šFine-tuning the MLPs in the last n blocks of the model.
      2. ChatGLM-6B P-Tuning V2 å¾®è°ƒï¼šFine-tuning the prefix encoder of the model.
      3. ChatGLM-6B LoRA å¾®è°ƒï¼šFine-tuning the low-rank adapters of the model.
8. [ã€LLMs å…¥é—¨å®æˆ˜ â€”â€” åäºŒ ã€‘åŸºäº æœ¬åœ°çŸ¥è¯†åº“ çš„é«˜æ•ˆ ğŸ¤–langchain-ChatGLM ](https://articles.zsxq.com/id_54vjwns5t6in.html)
   1. ä»‹ç»ï¼šlangchain-ChatGLMæ˜¯ä¸€ä¸ªåŸºäºæœ¬åœ°çŸ¥è¯†çš„é—®ç­”æœºå™¨äººï¼Œä½¿ç”¨è€…å¯ä»¥è‡ªç”±é…ç½®æœ¬åœ°çŸ¥è¯†ï¼Œç”¨æˆ·é—®é¢˜çš„ç­”æ¡ˆä¹Ÿæ˜¯åŸºäºæœ¬åœ°çŸ¥è¯†ç”Ÿæˆçš„ã€‚

### ç¬¬äºŒå±‚ Stanford Alpaca 7B 

- [ã€LLMs å…¥é—¨å®æˆ˜ â€”â€” äº” ã€‘Stanford Alpaca 7B æ¨¡å‹å­¦ä¹ ä¸å®æˆ˜](https://articles.zsxq.com/id_xnt3fvp2wxz0.html)
  - ä»‹ç»ï¼šæœ¬æ•™ç¨‹æä¾›äº†å¯¹LLaMAæ¨¡å‹è¿›è¡Œå¾®è°ƒçš„å»‰ä»·äº²æ°‘ LLMs å­¦ä¹ å’Œå¾®è°ƒ æ–¹å¼ï¼Œä¸»è¦ä»‹ç»å¯¹äº Stanford Alpaca 7B æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸Š çš„ å¾®è°ƒå®éªŒï¼Œæ‰€ç”¨çš„æ•°æ®ä¸ºOpenAIæä¾›çš„GPTæ¨¡å‹APIç”Ÿæˆè´¨é‡è¾ƒé«˜çš„æŒ‡ä»¤æ•°æ®ï¼ˆä»…52kï¼‰ã€‚

### ç¬¬ä¸‰å±‚ Chinese-LLaMA-Alpaca 

- [ã€LLMs å…¥é—¨å®æˆ˜ â€”â€” å…­ ã€‘Chinese-LLaMA-Alpaca æ¨¡å‹å­¦ä¹ ä¸å®æˆ˜](https://articles.zsxq.com/id_dqvusswrdg6c.html)
  - ä»‹ç»ï¼šæœ¬æ•™ç¨‹ä¸»è¦ä»‹ç»äº† Chinese-ChatLLaMA,æä¾›ä¸­æ–‡å¯¹è¯æ¨¡å‹ ChatLLama ã€ä¸­æ–‡åŸºç¡€æ¨¡å‹ LLaMA-zh åŠå…¶è®­ç»ƒæ•°æ®ã€‚ æ¨¡å‹åŸºäº TencentPretrain å¤šæ¨¡æ€é¢„è®­ç»ƒæ¡†æ¶æ„å»º

### ç¬¬å››å±‚ å°ç¾Šé©¼ Vicuna

- [ã€LLMs å…¥é—¨å®æˆ˜ â€”â€” ä¸ƒ ã€‘å°ç¾Šé©¼ Vicunaæ¨¡å‹å­¦ä¹ ä¸å®æˆ˜](https://articles.zsxq.com/id_q9mx24q9fdab.html)
  - ä»‹ç»ï¼šUCä¼¯å…‹åˆ©å­¦è€…è”æ‰‹CMUã€æ–¯å¦ç¦ç­‰ï¼Œå†æ¬¡æ¨å‡ºä¸€ä¸ªå…¨æ–°æ¨¡å‹70äº¿/130äº¿å‚æ•°çš„Vicunaï¼Œä¿—ç§°ã€Œå°ç¾Šé©¼ã€ï¼ˆéª†é©¬ï¼‰ã€‚å°ç¾Šé©¼å·ç§°èƒ½è¾¾åˆ°GPT-4çš„90%æ€§èƒ½

### ç¬¬äº”å±‚ MiniGPT-4 

- [ã€LLMs å…¥é—¨å®æˆ˜ â€”â€” å…« ã€‘MiniGPT-4 æ¨¡å‹å­¦ä¹ ä¸å®æˆ˜](https://articles.zsxq.com/id_ff0w6czthq25.html)
  - ä»‹ç»ï¼š MiniGPT-4ï¼Œæ˜¯æ¥è‡ªé˜¿åœæœæ‹‰å›½ç‹ç§‘æŠ€å¤§å­¦çš„å‡ ä½åšå£«åšçš„ï¼Œå®ƒèƒ½æä¾›ç±»ä¼¼ GPT-4 çš„å›¾åƒç†è§£ä¸å¯¹è¯èƒ½åŠ›

### ç¬¬å…­å±‚ GPT4ALL

- [ã€LLMs å…¥é—¨å®æˆ˜ â€”â€” å…« ã€‘GPT4ALL æ¨¡å‹å­¦ä¹ ä¸å®æˆ˜](https://articles.zsxq.com/id_ff0w6czthq25.html)
  - ä»‹ç»ï¼šä¸€ä¸ª å¯ä»¥åœ¨è‡ªå·±ç¬”è®°æœ¬ä¸Šé¢è·‘èµ·æ¥çš„  Nomic AI çš„åŠ©æ‰‹å¼èŠå¤©æœºå™¨äººï¼Œæˆä¸ºè´«æ°‘å®¶å­©å­çš„ ç¦éŸ³ï¼

### ç¬¬ä¸ƒå±‚ AutoGPT

- [AutoGPT ä½¿ç”¨å’Œéƒ¨ç½²](https://articles.zsxq.com/id_pli0z9916126.html)
  - ä»‹ç»ï¼šAuto-GPTæ˜¯ä¸€ä¸ªåŸºäºChatGPTçš„å·¥å…·ï¼Œä»–èƒ½å¸®ä½ è‡ªåŠ¨å®Œæˆå„ç§ä»»åŠ¡ï¼Œæ¯”å¦‚å†™ä»£ç ã€å†™æŠ¥å‘Šã€åšè°ƒç ”ç­‰ç­‰ã€‚ä½¿ç”¨å®ƒæ—¶ï¼Œä½ åªéœ€è¦å‘Šè¯‰ä»–è¦æ‰®æ¼”çš„è§’è‰²å’Œè¦å®ç°çš„ç›®æ ‡ï¼Œç„¶åä»–å°±ä¼šåˆ©ç”¨ChatGPTå’Œè°·æ­Œæœç´¢ç­‰å·¥å…·ï¼Œä¸æ–­â€œæ€è€ƒâ€å¦‚ä½•æ¥è¿‘ç›®æ ‡å¹¶æ‰§è¡Œï¼Œä½ ç”šè‡³å¯ä»¥çœ‹åˆ°ä»–çš„æ€è€ƒè¿‡ç¨‹ã€‚

### ç¬¬å…«å±‚ MOSS

- [ã€LLMs å…¥é—¨å®æˆ˜ â€”â€” åä¸‰ ã€‘MOSS æ¨¡å‹å­¦ä¹ ä¸å®æˆ˜](https://articles.zsxq.com/id_4vwpxod23zrc.html)
  - ä»‹ç»ï¼šMOSSæ˜¯ä¸€ä¸ªæ”¯æŒä¸­è‹±åŒè¯­å’Œå¤šç§æ’ä»¶çš„å¼€æºå¯¹è¯è¯­è¨€æ¨¡å‹ï¼Œmoss-moonç³»åˆ—æ¨¡å‹å…·æœ‰160äº¿å‚æ•°ï¼Œåœ¨FP16ç²¾åº¦ä¸‹å¯åœ¨å•å¼ A100/A800æˆ–ä¸¤å¼ 3090æ˜¾å¡è¿è¡Œï¼Œåœ¨INT4/8ç²¾åº¦ä¸‹å¯åœ¨å•å¼ 3090æ˜¾å¡è¿è¡Œã€‚MOSSåŸºåº§è¯­è¨€æ¨¡å‹åœ¨çº¦ä¸ƒåƒäº¿ä¸­è‹±æ–‡ä»¥åŠä»£ç å•è¯ä¸Šé¢„è®­ç»ƒå¾—åˆ°ï¼Œåç»­ç»è¿‡å¯¹è¯æŒ‡ä»¤å¾®è°ƒã€æ’ä»¶å¢å¼ºå­¦ä¹ å’Œäººç±»åå¥½è®­ç»ƒå…·å¤‡å¤šè½®å¯¹è¯èƒ½åŠ›åŠä½¿ç”¨å¤šç§æ’ä»¶çš„èƒ½åŠ›ã€‚
  - å±€é™æ€§ï¼šç”±äºæ¨¡å‹å‚æ•°é‡è¾ƒå°å’Œè‡ªå›å½’ç”ŸæˆèŒƒå¼ï¼ŒMOSSä»ç„¶å¯èƒ½ç”ŸæˆåŒ…å«äº‹å®æ€§é”™è¯¯çš„è¯¯å¯¼æ€§å›å¤æˆ–åŒ…å«åè§/æ­§è§†çš„æœ‰å®³å†…å®¹ï¼Œè¯·è°¨æ…é‰´åˆ«å’Œä½¿ç”¨MOSSç”Ÿæˆçš„å†…å®¹ï¼Œè¯·å‹¿å°†MOSSç”Ÿæˆçš„æœ‰å®³å†…å®¹ä¼ æ’­è‡³äº’è”ç½‘ã€‚è‹¥äº§ç”Ÿä¸è‰¯åæœï¼Œç”±ä¼ æ’­è€…è‡ªè´Ÿã€‚



## ä¸€ã€å‰è¨€

æœ¬æ•™ç¨‹ä¸»è¦ä»‹ç»äº† [Chinese-ChatLLaMA](https://github.com/ydli-ai/Chinese-ChatLLaMA),æä¾›ä¸­æ–‡å¯¹è¯æ¨¡å‹ ChatLLama ã€ä¸­æ–‡åŸºç¡€æ¨¡å‹ LLaMA-zh åŠå…¶è®­ç»ƒæ•°æ®ã€‚ æ¨¡å‹åŸºäº [TencentPretrain](https://github.com/Tencent/TencentPretrain) å¤šæ¨¡æ€é¢„è®­ç»ƒæ¡†æ¶æ„å»º

## äºŒã€æ•´ä½“æ–¹æ³•ä»‹ç»

ChatLLaMA æ”¯æŒç®€ç¹ä½“ä¸­æ–‡ã€è‹±æ–‡ã€æ—¥æ–‡ç­‰å¤šè¯­è¨€ã€‚ LLaMA åœ¨é¢„è®­ç»ƒé˜¶æ®µä¸»è¦ä½¿ç”¨è‹±æ–‡ï¼Œä¸ºäº†å°†å…¶è¯­è¨€èƒ½åŠ›è¿ç§»åˆ°ä¸­æ–‡ä¸Šï¼Œé¦–å…ˆè¿›è¡Œä¸­æ–‡å¢é‡é¢„è®­ç»ƒï¼Œ ä½¿ç”¨çš„è¯­æ–™åŒ…æ‹¬ä¸­è‹±å¹³è¡Œè¯­æ–™ã€ä¸­æ–‡ç»´åŸºã€ç¤¾åŒºäº’åŠ¨ã€æ–°é—»æ•°æ®ã€ç§‘å­¦æ–‡çŒ®ç­‰ã€‚å†é€šè¿‡ Alpaca æŒ‡ä»¤å¾®è°ƒå¾—åˆ° Chinese-ChatLLaMAã€‚

- **é¡¹ç›®ç‰¹ç‚¹**
  - é€šè¿‡ Full-tuning ï¼ˆå…¨å‚æ•°è®­ç»ƒï¼‰è·å¾—ä¸­æ–‡æ¨¡å‹æƒé‡ï¼Œæä¾› TencentPretrain ä¸ HuggingFace ç‰ˆæœ¬
  - æ¨¡å‹ç»†èŠ‚å…¬å¼€å¯å¤ç°ï¼Œæä¾›æ•°æ®å‡†å¤‡ã€æ¨¡å‹è®­ç»ƒå’Œæ¨¡å‹è¯„ä¼°å®Œæ•´æµç¨‹ä»£ç 
  - æä¾›ç›®å‰æœ€å¤§çš„ä¸­æ–‡ LLaMA æ¨¡å‹
  - å¤šç§é‡åŒ–æ–¹æ¡ˆï¼Œæ”¯æŒ CUDA å’Œè¾¹ç¼˜è®¾å¤‡éƒ¨ç½²æ¨ç†

## ä¸‰ã€ç¯å¢ƒæ­å»º

### 3.1 åŸºç¡€ç¯å¢ƒé…ç½®è¦æ±‚

1. æ“ä½œç³»ç»Ÿï¼šLinux
2. CPUs: å•ä¸ªèŠ‚ç‚¹å…·æœ‰ 1TB å†…å­˜çš„ Intel CPUï¼Œç‰©ç†CPUä¸ªæ•°ä¸º64ï¼Œæ¯é¢—CPUæ ¸æ•°ä¸º16
3. GPUs: 8 å¡ A800 80GB GPUs
4. Python: 3.10 

### 3.2 æ„å»ºç¯å¢ƒ

```s
    $ conda create -n py310_chat python=3.10       # åˆ›å»ºæ–°ç¯å¢ƒ
    $ source activate py310_chat                   # æ¿€æ´»ç¯å¢ƒ
```

### 3.3 ç¦»çº¿å®‰è£… pytorch

ç¦»çº¿å®‰è£…PyTorchï¼Œç‚¹å‡»ä¸‹è½½å¯¹åº” [cudaç‰ˆæœ¬](https://download.pytorch.org/whl/torch_stable.html)çš„torchå’Œtorchvisionå³å¯ã€‚

```s
    $ pip  install -U torch==1.13.1
    $ pip install torchvision-0.14.1
```

### 3.4 å®‰è£… TencentPretrain

å®‰è£… TencentPretrain ï¼Œåé¢éœ€è¦ç”¨ä»– è½¬åŒ– é¢„è®­ç»ƒæ¨¡å‹ 

```s
    $ https://github.com/Tencent/TencentPretrain.git
```

### 3.5 å®‰è£… apex

```s
    $ git clone https://github.com/NVIDIA/apex.git
    $ cd apex
    $ git checkout 22.04-dev
    $ pip install -v --disable-pip-version-check --no-cache-dir --global-option="--cpp_ext" --global-option="--cuda_ext" ./
```

### 3.6 å®‰è£…å…¶ä»–ä¾èµ–åŒ…

ä½¿ç”¨ ä»¥ä¸‹å‘½ä»¤ å®‰è£…å…¶ä»–ä¾èµ–åŒ…

```s
    $ pip install -r requirements.txt
```

> æ³¨ï¼š**å¦‚æœåˆ°è¿™é‡Œä¸€è·¯ç•…é€šï¼Œé‚£å°±è¯´æ˜ ä½ å·²ç» å®Œæˆä¸€å¤§æ­¥å·¥ä½œäº†ï¼**

## å››ã€LLaMAåŸå§‹æƒé‡æ–‡ä»¶ ä¸‹è½½

```s
    $ git lfs install
    $ git clone https://huggingface.co/P01son/ChatLLaMA-zh-7B
    >>>
    Cloning into 'ChatLLaMA-zh-7B'...
    remote: Enumerating objects: 12, done.
    remote: Counting objects: 100% (12/12), done.
    remote: Compressing objects: 100% (11/11), done.
    remote: Total 12 (delta 2), reused 0 (delta 0), pack-reused 0
    Unpacking objects: 100% (12/12), 1.44 KiB | 369.00 KiB/s, done.
    ...
```

## äº”ã€è®­ç»ƒæ•°æ®ä»‹ç»

ä½¿ç”¨çš„è¯­æ–™åŒ…æ‹¬[ä¸­è‹±å¹³è¡Œè¯­æ–™](https://statmt.org/wmt18/translation-task.html#download)ã€[ä¸­æ–‡ç»´åŸº](https://github.com/CLUEbenchmark/CLUECorpus2020)ã€[ç¤¾åŒºäº’åŠ¨](https://github.com/CLUEbenchmark/CLUECorpus2020)ã€[æ–°é—»æ•°æ®](https://github.com/CLUEbenchmark/CLUECorpus2020)ã€[ç§‘å­¦æ–‡çŒ®](https://github.com/ydli-ai/CSL)ç­‰

ä¸‹è½½è¯­æ–™åï¼Œåˆå¹¶åˆ°ä¸€ä¸ª .txt æ–‡ä»¶å¹¶æŒ‰è¡Œéšæœºæ‰“ä¹±ï¼Œè¯­æ–™æ ¼å¼å¦‚ä¸‹ï¼š

doc1
doc2
doc3

## å…­ã€Chinese-ChatLLaMA å¿«é€Ÿä½¿ç”¨

### 6.1 clone fengyh3/llama_inference æ¨ç†

```s
    $ git clone https://github.com/fengyh3/llama_inference.git
    Cloning into 'llama_inference'...
    remote: Enumerating objects: 145, done.
    remote: Counting objects: 100% (145/145), done.
    remote: Compressing objects: 100% (102/102), done.
    remote: Total 145 (delta 68), reused 105 (delta 35), pack-reused 0
    Receiving objects: 100% (145/145), 47.15 KiB | 41.00 KiB/s, done.
    Resolving deltas: 100% (68/68), done.
```

### 6.2 fengyh3/llama_inference é—®ç­”

```s
    $ cd llama_inference 
    $ vi prompts.txt  #ç¼–è¾‘ç”¨æˆ·è¾“å…¥ï¼Œä¾‹å¦‚"ä¸Šæµ·æœ‰ä»€ä¹ˆå¥½ç©çš„åœ°æ–¹ï¼Ÿ"
    $ python llama_infer.py --test_path prompts.txt --prediction_path result.txt  --load_model_path /mnt/kaimo/data/chat/ChatLLaMA-zh-7B/chatllama_7b.bin   --config_path config/llama_7b_config.json  --spm_model_path /mnt/kaimo/data/chat/ChatLLaMA-zh-7B/tokenizer.model --seq_length 512
```

- å‚æ•°ä»‹ç»ï¼š
  - test_pathï¼šæµ‹è¯•æ–‡ä»¶ è·¯å¾„
  - prediction_pathï¼šé¢„æµ‹ç»“æœæ–‡ä»¶ è·¯å¾„
  - load_model_pathï¼šChatLLaMA-zh-7B æ¨¡å‹ è·¯å¾„
  - config_pathï¼šChatLLaMA-zh-7B æ¨¡å‹ é…ç½®é¡¹ è·¯å¾„
  - spm_model_pathï¼šChatLLaMA-zh-7B æ¨¡å‹ tokenizer è·¯å¾„
  - seq_lengthï¼šåºåˆ—é•¿åº¦

### 6.3 fengyh3/llama_inference Int8 æ¨ç†åŠ é€Ÿ

å¦‚æœ æ˜¾å­˜ä¸å¤Ÿï¼Œå¯ä»¥ ä½¿ç”¨  Int8 æ¨ç†åŠ é€Ÿï¼Œåªéœ€è¦åœ¨ åé¢åŠ ä¸Š -use_int8  å³å¯

```s
    $ python llama_infer.py --test_path beginning.txt --prediction_path result.txt  \
        --load_model_path ../ChatLLaMA-zh-7B/chatllama_7b.bin  \
        --config_path config/llama_7b_config.json \
        --spm_model_path ../ChatLLaMA-zh-7B/tokenizer.model --seq_length 512 --use_int8 
```

## ä¸ƒã€Chinese-ChatLLaMA æ¨¡å‹è®­ç»ƒ

### 7.1 clone TencentPretrain 

```s
    $ git clone https://github.com/Tencent/TencentPretrain.git
    $ cd TencentPretrain
```

### 7.2 ä¸‹è½½[é¢„è®­ç»ƒLLaMAæƒé‡](https://huggingface.co/decapoda-research/llama-7b-hf)

```s
    $ git lfs install
    $ git clone https://huggingface.co/decapoda-research/llama-7b-hf
```

### 7.3 [é¢„è®­ç»ƒLLaMAæƒé‡](https://huggingface.co/decapoda-research/llama-7b-hf)è½¬åŒ–ä¸ºTencentPretrainæ ¼å¼

```s
    $ python scripts/convert_llama_from_huggingface_to_tencentpretrain.py --input_model_path $LLaMA_HF_PATH --output_model_path  models/llama-7b.bin --type 7B
```

> æ³¨ï¼šè¿™é‡Œçš„ $LLaMA_HF_PATH ä¸º [é¢„è®­ç»ƒLLaMAæƒé‡](https://huggingface.co/decapoda-research/llama-7b-hf) ä¸‹è½½åœ°å€

### 7.4 ä¸‹è½½[ä¸­æ–‡é¢„è®­ç»ƒè¯­æ–™](https://github.com/ydli-ai/Chinese-ChatLLaMA/blob/main/corpus/README.md)ï¼Œ é¢„å¤„ç†

```s
    $ python preprocess.py --corpus_path $CORPUS_PATH --spm_model_path $LLaMA_PATH/tokenizer.model  --dataset_path $OUTPUT_DATASET_PATH --data_processor lm --seq_length 512
```

## å…«ã€æ€»ç»“


## è¸©å‘æ‰‹å†Œ

### ModuleNotFoundError: No module named 'bitsandbytes'

- é—®é¢˜æè¿°ï¼š

```s
    Traceback (most recent call last):
    File "llama_infer.py", line 4, in <module>
        from model.llama import *
    File "/mnt/kaimo/project/chat_wp/llama_inference/model/llama.py", line 6, in <module>
        import bitsandbytes as bnb
    ModuleNotFoundError: No module named 'bitsandbytes'
```

- è§£å†³æ–¹æ³•ï¼š

```s
    $ pip install bitsandbytes
```

## å‚è€ƒ

1. [ymcui/Chinese-LLaMA-Alpaca](https://github.com/ymcui/Chinese-LLaMA-Alpaca)
2. [facebookresearch/llama](https://github.com/facebookresearch/llama)
3. [stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca)
4. [alpaca-lora](https://github.com/tloen/alpaca-lora)
5. [Alpaca: A Strong, Replicable Instruction-Following Model](https://crfm.stanford.edu/2023/03/13/alpaca.html)
6.  [llama-7b-hf/tree/main](https://huggingface.co/decapoda-research/llama-7b-hf/tree/main)
