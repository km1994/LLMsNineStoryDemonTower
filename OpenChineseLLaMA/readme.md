# ã€LLMs å…¥é—¨å®æˆ˜ã€‘ OpenChineseLLaMA æ¨¡å‹å­¦ä¹ ä¸å®æˆ˜
 
- Github ä»£ç ï¼šhttps://github.com/OpenLMLab/OpenChineseLLaMA
- æ¨¡å‹ï¼šhttps://huggingface.co/openlmlab/open-chinese-llama-7b-patch

## ä¸€ã€å‰è¨€

### 1.1 ä»‹ç»

åŸºäº LLaMA-7B ç»è¿‡ä¸­æ–‡æ•°æ®é›†å¢é‡é¢„è®­ç»ƒäº§ç”Ÿçš„ä¸­æ–‡å¤§è¯­è¨€æ¨¡å‹åŸºåº§ï¼Œå¯¹æ¯”åŸç‰ˆ LLaMAï¼Œè¯¥æ¨¡å‹åœ¨ä¸­æ–‡ç†è§£èƒ½åŠ›å’Œç”Ÿæˆèƒ½åŠ›æ–¹é¢å‡è·å¾—è¾ƒå¤§æå‡ï¼Œåœ¨ä¼—å¤šä¸‹æ¸¸ä»»åŠ¡ä¸­å‡å–å¾—äº†çªå‡ºçš„æˆç»©ã€‚

### 1.2 è½¯ä»¶èµ„æº

- CUDA 11.7
- Python 3.10
- pytorch 1.13.1+cu117

## äºŒã€ç¯å¢ƒæ­å»º

### 2.1 ä¸‹è½½ä»£ç  

```s
    $ git clone https://github.com/OpenLMLab/OpenChineseLLaMA
```

### 2.2 æ„å»ºç¯å¢ƒ

```s
    $ conda create -n py310_chat python=3.10       # åˆ›å»ºæ–°ç¯å¢ƒ
    $ source activate py310_chat                   # æ¿€æ´»ç¯å¢ƒ
```

### 2.3 å®‰è£…ä¾èµ– 

```s
    $ cd OpenChineseLLaMA
    $ pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple
```

### 2.4 ä¸‹è½½æ¨¡å‹

- æ¨¡å‹åç§°ï¼šOpen-Chinese-LLaMA-7B-Patch
- æƒé‡ç±»å‹ï¼šPatch
- ä¸‹è½½åœ°å€ï¼š
  - [ğŸ¤—Huggingface]ï¼šhttps://huggingface.co/openlmlab/open-chinese-llama-7b-patch
  - [ç™¾åº¦ç½‘ç›˜]ï¼šhttps://pan.baidu.com/s/14E7iZKcH-5SHMDu97k70cg?pwd=gk34#list/path=%2F
  - [Google Driver]ï¼šhttps://drive.google.com/drive/folders/1THvuFzq_wojVfMLYV1qsSE_ddSjG0Ypv?usp=sharing
- SHA256ï¼šhttps://github.com/OpenLMLab/OpenChineseLLaMA/blob/main/SHA256.txt

### 2.5 æ¨¡å‹åˆå¹¶

Meta å®˜æ–¹å‘å¸ƒçš„ [LLaMA](https://github.com/facebookresearch/llama) æœªå¼€æºæƒé‡ï¼Œä¸ºäº†éµå®ˆç›¸å…³è®¸å¯ï¼Œæœ¬æ¬¡å‘å¸ƒçš„æ¨¡å‹ä¸º è¡¥ä¸ï¼ˆPatchï¼‰ ç±»å‹ï¼Œé¡»é…åˆåŸå§‹å®˜æ–¹æƒé‡æ‰å¯ä»¥ä½¿ç”¨ã€‚

å®˜æ–¹ æä¾›äº† è¡¥ä¸ï¼ˆPatchï¼‰ çš„å®‰è£…è„šæœ¬ï¼Œåœ¨é€šè¿‡æ­£è§„æ¸ é“è·å¾—å®˜æ–¹æƒé‡åï¼Œå¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼å®‰è£…è¡¥ä¸ï¼š

```s
  python tools/patch_model.py --base_model <path_or_name_to_original_model>
                              --patch_model openlmlab/open-chinese-llama-7b-patch
                              --base_model_format <hf_or_raw>
```

> æç¤ºï¼šæœ¬è¡¥ä¸çš„å®‰è£…æ–¹å¼ä¸ºåŸåœ°å®‰è£…ï¼Œå³å®‰è£…åçš„è¡¥ä¸å³ä¸ºå®Œæ•´ç‰ˆ huggingface ç‰ˆæœ¬çš„æœ¬æ¨¡å‹æƒé‡ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ transformersåŠ è½½æ¨¡å‹ã€‚

## ä¸‰ã€æ¨¡å‹æ¨ç†

å¯ä»¥ä½¿ç”¨è„šæœ¬å¯åŠ¨äº¤äº’å¼ç•Œé¢ï¼š

```s
  python cli_demo.py --model openlmlab/open-chinese-llama-7b-patch
                    --devices 0
                    --max_length 1024
                    --do_sample true
                    --top_k 40
                    --top_p 0.8
                    --temperature 0.7
                    --penalty 1.02
```

## å››ã€æ¨¡å‹å¾®è°ƒ



## å¡«å‘ç¬”è®°


## å‚è€ƒ

1. [Panda LLM: Training Data and Evaluation for Open-Sourced Chinese Instruction-Following Large Language Models](https://arxiv.org/pdf/2305.03025v1.pdf)
2. [dandelionsllm/pandallm](https://github.com/dandelionsllm/pandallm)
